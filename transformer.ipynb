{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Transformer from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is based off of the following repos/blog posts:\n",
    "\n",
    "- [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) \n",
    "\n",
    "Thanks so much to their authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=[{cls.__name__}, {name}, {tsr.shape}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "\n",
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention # Logging level: \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # Compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
    "\n",
    "        attn = attn / math.sqrt(d_k)\n",
    "\n",
    "        attn = torch.exp(attn)\n",
    "        log_size(attn, \"attention weight\") # (Batch, Seq, Seq)\n",
    "        \n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3121, 0.3792, 0.3698, 0.3719, 0.4281, 0.4989, 0.3800, 0.4678,\n",
       "          0.5751, 0.5017, 0.2173, 0.3099, 0.4850, 0.3350, 0.3449, 0.3195,\n",
       "          0.3780, 0.6554, 0.6096, 0.6082],\n",
       "         [0.3093, 0.3974, 0.3839, 0.3750, 0.4092, 0.4910, 0.4202, 0.4002,\n",
       "          0.5377, 0.4995, 0.2539, 0.3104, 0.5134, 0.3586, 0.3292, 0.2958,\n",
       "          0.4035, 0.5870, 0.6650, 0.5289],\n",
       "         [0.3243, 0.5410, 0.4627, 0.5091, 0.5296, 0.6313, 0.5033, 0.5384,\n",
       "          0.6450, 0.6944, 0.3142, 0.3761, 0.5923, 0.5035, 0.4528, 0.3683,\n",
       "          0.5243, 0.6753, 0.7097, 0.6359],\n",
       "         [0.3351, 0.5295, 0.4725, 0.5102, 0.5204, 0.6178, 0.5099, 0.5177,\n",
       "          0.6504, 0.6965, 0.3239, 0.3728, 0.5767, 0.5037, 0.4415, 0.3596,\n",
       "          0.5163, 0.6683, 0.6948, 0.6305],\n",
       "         [0.2234, 0.4590, 0.3810, 0.4069, 0.4264, 0.4647, 0.4199, 0.3563,\n",
       "          0.5414, 0.5144, 0.2527, 0.2695, 0.5280, 0.3586, 0.3808, 0.3276,\n",
       "          0.3982, 0.5265, 0.5716, 0.4751],\n",
       "         [0.2334, 0.5043, 0.3602, 0.3543, 0.4779, 0.4595, 0.4032, 0.4037,\n",
       "          0.5731, 0.5347, 0.2518, 0.2823, 0.4450, 0.3293, 0.3351, 0.3196,\n",
       "          0.3496, 0.4820, 0.5365, 0.4867],\n",
       "         [0.2796, 0.4114, 0.4274, 0.3951, 0.4504, 0.5235, 0.4431, 0.4187,\n",
       "          0.4743, 0.5624, 0.3114, 0.2965, 0.4317, 0.3650, 0.3281, 0.2898,\n",
       "          0.3384, 0.5640, 0.5702, 0.5711],\n",
       "         [0.3183, 0.4240, 0.3824, 0.4491, 0.4503, 0.5382, 0.4039, 0.4854,\n",
       "          0.6229, 0.5750, 0.2256, 0.3382, 0.5107, 0.4138, 0.4190, 0.3456,\n",
       "          0.4511, 0.6484, 0.6112, 0.6038],\n",
       "         [0.3325, 0.5278, 0.4600, 0.5083, 0.5171, 0.6243, 0.4894, 0.5345,\n",
       "          0.6421, 0.6987, 0.3042, 0.3700, 0.5816, 0.5046, 0.4406, 0.3621,\n",
       "          0.5159, 0.6881, 0.7029, 0.6401],\n",
       "         [0.2222, 0.4219, 0.3541, 0.3625, 0.4340, 0.4333, 0.3726, 0.3937,\n",
       "          0.5836, 0.4598, 0.2145, 0.2646, 0.4754, 0.2902, 0.3682, 0.3431,\n",
       "          0.3491, 0.5527, 0.4936, 0.5274]],\n",
       "\n",
       "        [[0.4002, 0.4166, 0.5787, 0.5708, 0.3365, 0.4057, 0.4191, 0.3543,\n",
       "          0.4187, 0.2967, 0.4620, 0.4977, 0.5796, 0.4932, 0.5912, 0.5531,\n",
       "          0.5760, 0.4376, 0.5927, 0.4069],\n",
       "         [0.4800, 0.4943, 0.6146, 0.5790, 0.3448, 0.4578, 0.5242, 0.3972,\n",
       "          0.4282, 0.3179, 0.5518, 0.5024, 0.6059, 0.5140, 0.6498, 0.5820,\n",
       "          0.6614, 0.4785, 0.6034, 0.4808],\n",
       "         [0.3940, 0.4463, 0.5376, 0.5262, 0.3298, 0.3554, 0.5160, 0.2980,\n",
       "          0.4252, 0.2804, 0.4584, 0.4270, 0.5211, 0.4745, 0.6267, 0.4790,\n",
       "          0.5692, 0.4639, 0.5183, 0.4584],\n",
       "         [0.4061, 0.3788, 0.5388, 0.4865, 0.3129, 0.4128, 0.4143, 0.2756,\n",
       "          0.3470, 0.2454, 0.4955, 0.4643, 0.5498, 0.4701, 0.6041, 0.4623,\n",
       "          0.5289, 0.4055, 0.5399, 0.3982],\n",
       "         [0.4125, 0.4265, 0.4789, 0.4762, 0.3342, 0.3489, 0.4515, 0.2229,\n",
       "          0.4208, 0.2375, 0.4282, 0.3699, 0.4752, 0.3645, 0.5282, 0.3891,\n",
       "          0.4694, 0.4017, 0.4782, 0.3549],\n",
       "         [0.4829, 0.5119, 0.6255, 0.5708, 0.3554, 0.4329, 0.5326, 0.3959,\n",
       "          0.4395, 0.3242, 0.5470, 0.4975, 0.5956, 0.5119, 0.6498, 0.5853,\n",
       "          0.6606, 0.4727, 0.6056, 0.4779],\n",
       "         [0.4033, 0.4568, 0.5447, 0.5277, 0.3437, 0.3708, 0.5128, 0.2935,\n",
       "          0.4334, 0.2780, 0.4533, 0.4389, 0.5231, 0.4643, 0.6220, 0.4708,\n",
       "          0.5563, 0.4559, 0.5392, 0.4482],\n",
       "         [0.3552, 0.4239, 0.5024, 0.4249, 0.2841, 0.3627, 0.4070, 0.2589,\n",
       "          0.4098, 0.2481, 0.3864, 0.3928, 0.4623, 0.3853, 0.5973, 0.4149,\n",
       "          0.4563, 0.4092, 0.4996, 0.4532],\n",
       "         [0.4138, 0.3892, 0.5480, 0.4923, 0.3058, 0.4192, 0.4160, 0.3157,\n",
       "          0.3265, 0.2477, 0.5078, 0.4797, 0.5537, 0.4810, 0.5930, 0.4914,\n",
       "          0.5661, 0.3958, 0.5510, 0.3970],\n",
       "         [0.4789, 0.4937, 0.6019, 0.5848, 0.3553, 0.4717, 0.5199, 0.3993,\n",
       "          0.4246, 0.3054, 0.5392, 0.5248, 0.6073, 0.5019, 0.6411, 0.5737,\n",
       "          0.6510, 0.4716, 0.6234, 0.4750]],\n",
       "\n",
       "        [[0.5145, 0.4584, 0.5899, 0.4947, 0.5037, 0.4608, 0.3557, 0.3843,\n",
       "          0.3709, 0.4891, 0.3066, 0.2881, 0.3462, 0.4416, 0.4924, 0.3745,\n",
       "          0.3001, 0.6041, 0.4062, 0.4569],\n",
       "         [0.5906, 0.6227, 0.5947, 0.5959, 0.6152, 0.6084, 0.5099, 0.5430,\n",
       "          0.4667, 0.5304, 0.3224, 0.4649, 0.4616, 0.5408, 0.4955, 0.4451,\n",
       "          0.3287, 0.6613, 0.4214, 0.6056],\n",
       "         [0.5490, 0.5458, 0.5586, 0.5226, 0.6025, 0.5101, 0.4868, 0.4515,\n",
       "          0.4329, 0.4922, 0.2932, 0.3998, 0.4211, 0.5317, 0.4423, 0.3596,\n",
       "          0.3032, 0.5890, 0.3523, 0.5683],\n",
       "         [0.4194, 0.4315, 0.4804, 0.4270, 0.4611, 0.4655, 0.3226, 0.3788,\n",
       "          0.2901, 0.3713, 0.2065, 0.2869, 0.2628, 0.3666, 0.3935, 0.3604,\n",
       "          0.2079, 0.4814, 0.2445, 0.4933],\n",
       "         [0.6016, 0.6333, 0.6006, 0.5704, 0.6313, 0.6161, 0.5119, 0.5381,\n",
       "          0.4667, 0.5337, 0.3394, 0.4717, 0.4637, 0.5435, 0.5034, 0.4587,\n",
       "          0.3259, 0.6613, 0.4115, 0.6052],\n",
       "         [0.5809, 0.6179, 0.5904, 0.6006, 0.5976, 0.6001, 0.5385, 0.5303,\n",
       "          0.4688, 0.5229, 0.3221, 0.4639, 0.4765, 0.5473, 0.4935, 0.4465,\n",
       "          0.3235, 0.6557, 0.4429, 0.5904],\n",
       "         [0.5673, 0.5753, 0.4957, 0.4725, 0.5356, 0.5434, 0.4055, 0.5145,\n",
       "          0.4628, 0.5191, 0.3403, 0.4575, 0.4661, 0.5221, 0.4833, 0.4073,\n",
       "          0.2937, 0.5925, 0.3631, 0.5532],\n",
       "         [0.3693, 0.4526, 0.4404, 0.4179, 0.3784, 0.4638, 0.3639, 0.3342,\n",
       "          0.3056, 0.3309, 0.2813, 0.2615, 0.3344, 0.3565, 0.3995, 0.3983,\n",
       "          0.2167, 0.4227, 0.3601, 0.4336],\n",
       "         [0.6052, 0.6287, 0.6041, 0.5741, 0.6365, 0.6024, 0.5288, 0.5292,\n",
       "          0.4731, 0.5379, 0.3414, 0.4716, 0.4706, 0.5501, 0.4951, 0.4439,\n",
       "          0.3351, 0.6662, 0.4235, 0.5891],\n",
       "         [0.6000, 0.6196, 0.6083, 0.5885, 0.6241, 0.6023, 0.5281, 0.5314,\n",
       "          0.4640, 0.5361, 0.3279, 0.4679, 0.4600, 0.5446, 0.4980, 0.4468,\n",
       "          0.3297, 0.6740, 0.4283, 0.5857]],\n",
       "\n",
       "        [[0.4443, 0.6668, 0.5040, 0.5261, 0.4380, 0.6096, 0.3781, 0.4284,\n",
       "          0.6150, 0.5727, 0.5254, 0.5782, 0.4870, 0.3183, 0.5648, 0.6863,\n",
       "          0.3382, 0.6524, 0.3849, 0.6667],\n",
       "         [0.5181, 0.7294, 0.5505, 0.6398, 0.5330, 0.6576, 0.4360, 0.4562,\n",
       "          0.6933, 0.6359, 0.5773, 0.6722, 0.5159, 0.3818, 0.6691, 0.7523,\n",
       "          0.4047, 0.7072, 0.3967, 0.7189],\n",
       "         [0.5069, 0.7196, 0.5527, 0.6484, 0.5685, 0.6288, 0.4243, 0.4561,\n",
       "          0.6925, 0.6204, 0.5478, 0.6944, 0.5130, 0.3864, 0.6889, 0.7817,\n",
       "          0.4210, 0.6908, 0.3788, 0.7240],\n",
       "         [0.3048, 0.4664, 0.3185, 0.5090, 0.4249, 0.4080, 0.3114, 0.2613,\n",
       "          0.4740, 0.4295, 0.3181, 0.4660, 0.3567, 0.2958, 0.5406, 0.6454,\n",
       "          0.3030, 0.4696, 0.2115, 0.5178],\n",
       "         [0.5002, 0.6514, 0.4525, 0.5941, 0.4820, 0.5433, 0.4018, 0.4417,\n",
       "          0.6187, 0.5307, 0.4801, 0.5687, 0.4348, 0.3040, 0.6509, 0.7382,\n",
       "          0.3268, 0.6747, 0.3630, 0.6912],\n",
       "         [0.5157, 0.7198, 0.5512, 0.6458, 0.5356, 0.6453, 0.4325, 0.4510,\n",
       "          0.6879, 0.6275, 0.5725, 0.6810, 0.5136, 0.3846, 0.6712, 0.7554,\n",
       "          0.4103, 0.7046, 0.3864, 0.7241],\n",
       "         [0.5111, 0.7227, 0.5529, 0.6380, 0.5571, 0.6365, 0.4125, 0.4585,\n",
       "          0.6865, 0.6148, 0.5658, 0.6869, 0.5245, 0.3867, 0.6791, 0.7827,\n",
       "          0.4025, 0.7028, 0.4006, 0.7237],\n",
       "         [0.5064, 0.7205, 0.5593, 0.6325, 0.5501, 0.6416, 0.4163, 0.4431,\n",
       "          0.6881, 0.6254, 0.5774, 0.6881, 0.5143, 0.3979, 0.6595, 0.7612,\n",
       "          0.4200, 0.6942, 0.3897, 0.7232],\n",
       "         [0.5099, 0.7195, 0.5537, 0.6473, 0.5433, 0.6454, 0.4285, 0.4469,\n",
       "          0.6917, 0.6279, 0.5676, 0.6863, 0.5144, 0.3897, 0.6759, 0.7575,\n",
       "          0.4146, 0.6977, 0.3848, 0.7206],\n",
       "         [0.3955, 0.6217, 0.5106, 0.6070, 0.5355, 0.5632, 0.3397, 0.3671,\n",
       "          0.6270, 0.5435, 0.4571, 0.6707, 0.4993, 0.3802, 0.6540, 0.7322,\n",
       "          0.3892, 0.5831, 0.3275, 0.6366]],\n",
       "\n",
       "        [[0.6046, 0.5495, 0.6537, 0.4573, 0.6700, 0.6343, 0.5797, 0.4739,\n",
       "          0.7337, 0.4324, 0.4803, 0.4907, 0.5542, 0.5705, 0.4344, 0.6216,\n",
       "          0.3478, 0.5848, 0.5151, 0.6640],\n",
       "         [0.6360, 0.5367, 0.6590, 0.4957, 0.6518, 0.6249, 0.5733, 0.4718,\n",
       "          0.7345, 0.4390, 0.4707, 0.4834, 0.5745, 0.5325, 0.4570, 0.6293,\n",
       "          0.3317, 0.5804, 0.4884, 0.6589],\n",
       "         [0.5506, 0.4192, 0.5728, 0.4714, 0.5036, 0.4682, 0.4315, 0.2942,\n",
       "          0.5578, 0.3357, 0.3511, 0.4204, 0.5025, 0.4016, 0.4076, 0.5441,\n",
       "          0.2058, 0.4744, 0.4388, 0.5386],\n",
       "         [0.5948, 0.4838, 0.5875, 0.4467, 0.5829, 0.5150, 0.4729, 0.3565,\n",
       "          0.6341, 0.4134, 0.3840, 0.4461, 0.5293, 0.4418, 0.4041, 0.5857,\n",
       "          0.2431, 0.5362, 0.4512, 0.6199],\n",
       "         [0.5224, 0.4927, 0.5620, 0.3733, 0.6015, 0.5680, 0.4659, 0.4152,\n",
       "          0.6368, 0.4033, 0.4405, 0.4603, 0.5028, 0.5300, 0.3426, 0.5508,\n",
       "          0.3050, 0.5099, 0.4896, 0.6390],\n",
       "         [0.6023, 0.5669, 0.6504, 0.4740, 0.6611, 0.6121, 0.5748, 0.4643,\n",
       "          0.7257, 0.4244, 0.4793, 0.4825, 0.5608, 0.5531, 0.4487, 0.6196,\n",
       "          0.3382, 0.5795, 0.5286, 0.6630],\n",
       "         [0.5254, 0.5358, 0.6051, 0.4091, 0.6305, 0.5549, 0.5724, 0.3954,\n",
       "          0.6622, 0.3929, 0.4395, 0.4357, 0.4877, 0.5251, 0.3906, 0.5528,\n",
       "          0.3135, 0.5677, 0.4945, 0.5875],\n",
       "         [0.5902, 0.5622, 0.6574, 0.4679, 0.6699, 0.6206, 0.5851, 0.4667,\n",
       "          0.7378, 0.4248, 0.4854, 0.4782, 0.5571, 0.5613, 0.4363, 0.6097,\n",
       "          0.3434, 0.5871, 0.5262, 0.6593],\n",
       "         [0.4695, 0.3176, 0.4735, 0.4064, 0.4376, 0.5248, 0.5105, 0.3731,\n",
       "          0.5138, 0.3549, 0.3775, 0.3253, 0.4070, 0.3798, 0.3335, 0.4271,\n",
       "          0.2796, 0.4219, 0.2102, 0.3736],\n",
       "         [0.5537, 0.4374, 0.6360, 0.4044, 0.6123, 0.5977, 0.5543, 0.4526,\n",
       "          0.7100, 0.3980, 0.4354, 0.4179, 0.4941, 0.5000, 0.3414, 0.5095,\n",
       "          0.3291, 0.5559, 0.4110, 0.5764]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) \n",
    "        V = self.value_tfm(values) \n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0866, -0.3308, -0.3227, -0.3983,  0.1949, -0.0884, -0.4402,\n",
       "           0.0801, -0.1528, -0.2324, -0.0808, -0.1215, -0.0841, -0.1485,\n",
       "          -0.2116,  0.0134, -0.0590, -0.3217,  0.0231,  0.1431],\n",
       "         [ 0.1099, -0.3605, -0.4230, -0.4906,  0.2339, -0.1566, -0.5526,\n",
       "           0.0826, -0.1851, -0.2680, -0.1532, -0.1891, -0.1318, -0.1841,\n",
       "          -0.3186,  0.0210, -0.1151, -0.3843,  0.0413,  0.1580],\n",
       "         [ 0.0835, -0.3177, -0.3744, -0.4102,  0.2207, -0.1657, -0.5114,\n",
       "           0.0427, -0.1680, -0.2053, -0.1135, -0.1667, -0.1162, -0.1649,\n",
       "          -0.2923,  0.0527, -0.0991, -0.3731,  0.0502,  0.1184],\n",
       "         [ 0.1117, -0.3623, -0.4228, -0.4892,  0.2366, -0.1585, -0.5556,\n",
       "           0.0846, -0.1889, -0.2669, -0.1544, -0.1855, -0.1311, -0.1830,\n",
       "          -0.3197,  0.0170, -0.1143, -0.3846,  0.0391,  0.1608],\n",
       "         [ 0.1228, -0.3175, -0.3637, -0.4327,  0.2037, -0.1589, -0.5255,\n",
       "           0.1018, -0.1696, -0.2445, -0.1563, -0.1678, -0.1173, -0.1726,\n",
       "          -0.2838,  0.0061, -0.1193, -0.3669,  0.0426,  0.1464],\n",
       "         [ 0.0598, -0.2542, -0.3377, -0.4038,  0.1914, -0.1298, -0.4303,\n",
       "           0.0331, -0.1294, -0.2038, -0.0987, -0.1739, -0.0919, -0.1469,\n",
       "          -0.2512,  0.0551, -0.1007, -0.3095,  0.0601,  0.0814],\n",
       "         [ 0.0932, -0.2743, -0.3119, -0.3475,  0.1876, -0.1665, -0.4774,\n",
       "           0.0554, -0.1487, -0.1800, -0.1088, -0.1452, -0.1042, -0.1540,\n",
       "          -0.2568,  0.0399, -0.1010, -0.3512,  0.0510,  0.1012],\n",
       "         [ 0.1032, -0.3429, -0.3809, -0.4579,  0.2091, -0.1133, -0.4912,\n",
       "           0.0889, -0.1715, -0.2636, -0.1354, -0.1887, -0.1088, -0.1878,\n",
       "          -0.2585,  0.0204, -0.0955, -0.3500,  0.0425,  0.1443],\n",
       "         [ 0.1144, -0.3367, -0.3729, -0.4500,  0.2301, -0.1251, -0.4984,\n",
       "           0.0798, -0.1866, -0.2615, -0.1400, -0.1856, -0.1260, -0.2085,\n",
       "          -0.2995, -0.0380, -0.0959, -0.3164,  0.0231,  0.1410],\n",
       "         [ 0.1220, -0.3176, -0.3625, -0.4295,  0.2048, -0.1602, -0.5267,\n",
       "           0.1002, -0.1699, -0.2415, -0.1540, -0.1665, -0.1168, -0.1730,\n",
       "          -0.2842,  0.0083, -0.1183, -0.3693,  0.0430,  0.1454]],\n",
       "\n",
       "        [[ 0.2016, -0.2539, -0.3915, -0.2584,  0.1525, -0.2214, -0.7195,\n",
       "           0.2247, -0.1978, -0.1789, -0.3203, -0.2577, -0.1586, -0.2345,\n",
       "          -0.2564,  0.1332, -0.1964, -0.5851,  0.1671,  0.3191],\n",
       "         [ 0.2035, -0.2537, -0.3905, -0.2585,  0.1519, -0.2190, -0.7194,\n",
       "           0.2271, -0.1965, -0.1803, -0.3210, -0.2560, -0.1593, -0.2335,\n",
       "          -0.2568,  0.1307, -0.1963, -0.5849,  0.1661,  0.3195],\n",
       "         [ 0.2040, -0.2554, -0.3921, -0.2518,  0.1522, -0.2194, -0.7226,\n",
       "           0.2276, -0.1993, -0.1794, -0.3211, -0.2555, -0.1611, -0.2349,\n",
       "          -0.2573,  0.1310, -0.1956, -0.5880,  0.1662,  0.3217],\n",
       "         [ 0.1574, -0.1545, -0.2525, -0.2566,  0.1185, -0.1511, -0.5479,\n",
       "           0.2020, -0.1244, -0.1460, -0.2686, -0.2108, -0.0965, -0.1954,\n",
       "          -0.2051,  0.0932, -0.1694, -0.4272,  0.1325,  0.2497],\n",
       "         [ 0.1743, -0.2137, -0.3913, -0.2284,  0.1173, -0.1838, -0.5498,\n",
       "           0.1698, -0.1467, -0.1629, -0.2353, -0.1899, -0.1320, -0.1416,\n",
       "          -0.2188,  0.1179, -0.1635, -0.4601,  0.1171,  0.2419],\n",
       "         [ 0.1843, -0.2187, -0.3901, -0.2382,  0.1285, -0.2129, -0.6140,\n",
       "           0.1926, -0.1809, -0.1492, -0.2740, -0.2426, -0.1489, -0.2005,\n",
       "          -0.2302,  0.1325, -0.1806, -0.5308,  0.1485,  0.2808],\n",
       "         [ 0.2057, -0.2542, -0.3917, -0.2542,  0.1503, -0.2168, -0.7222,\n",
       "           0.2294, -0.1952, -0.1818, -0.3209, -0.2533, -0.1615, -0.2310,\n",
       "          -0.2587,  0.1304, -0.1965, -0.5878,  0.1653,  0.3205],\n",
       "         [ 0.2028, -0.2558, -0.3912, -0.2578,  0.1525, -0.2200, -0.7213,\n",
       "           0.2255, -0.1972, -0.1808, -0.3215, -0.2563, -0.1600, -0.2338,\n",
       "          -0.2567,  0.1311, -0.1955, -0.5854,  0.1668,  0.3195],\n",
       "         [ 0.2047, -0.2544, -0.3912, -0.2542,  0.1507, -0.2186, -0.7217,\n",
       "           0.2282, -0.1967, -0.1803, -0.3212, -0.2553, -0.1614, -0.2330,\n",
       "          -0.2578,  0.1310, -0.1963, -0.5879,  0.1662,  0.3206],\n",
       "         [ 0.1853, -0.2228, -0.3910, -0.2379,  0.1285, -0.2115, -0.6119,\n",
       "           0.1917, -0.1810, -0.1502, -0.2748, -0.2412, -0.1514, -0.2003,\n",
       "          -0.2311,  0.1273, -0.1787, -0.5276,  0.1464,  0.2811]],\n",
       "\n",
       "        [[ 0.0901, -0.2256, -0.3797, -0.3533,  0.1650, -0.0868, -0.5020,\n",
       "           0.1342, -0.1501, -0.2863, -0.0851, -0.1556, -0.1454, -0.1330,\n",
       "          -0.1622,  0.1142, -0.1171, -0.3739,  0.0943,  0.2311],\n",
       "         [ 0.1132, -0.3048, -0.3776, -0.4823,  0.1816, -0.0814, -0.6138,\n",
       "           0.1720, -0.1518, -0.3338, -0.1296, -0.2023, -0.1893, -0.1701,\n",
       "          -0.1656,  0.1280, -0.1407, -0.4846,  0.1288,  0.2700],\n",
       "         [ 0.1031, -0.2773, -0.3204, -0.4827,  0.1749, -0.0563, -0.5536,\n",
       "           0.1626, -0.1221, -0.3306, -0.1209, -0.1884, -0.1570, -0.1680,\n",
       "          -0.1454,  0.1046, -0.1127, -0.4093,  0.1175,  0.2561],\n",
       "         [ 0.1007, -0.2941, -0.3274, -0.4131,  0.1479, -0.0717, -0.5719,\n",
       "           0.1827, -0.1561, -0.2981, -0.1420, -0.1975, -0.1909, -0.1658,\n",
       "          -0.1467,  0.1104, -0.1171, -0.4781,  0.1304,  0.2612],\n",
       "         [ 0.0912, -0.2537, -0.3188, -0.3694,  0.1758, -0.0943, -0.5172,\n",
       "           0.1132, -0.1767, -0.2430, -0.0931, -0.1710, -0.1240, -0.1489,\n",
       "          -0.1261,  0.0995, -0.1235, -0.3790,  0.0964,  0.2166],\n",
       "         [ 0.1128, -0.3024, -0.3861, -0.4801,  0.1843, -0.0797, -0.6148,\n",
       "           0.1736, -0.1552, -0.3395, -0.1286, -0.2060, -0.1882, -0.1739,\n",
       "          -0.1687,  0.1309, -0.1375, -0.4832,  0.1293,  0.2723],\n",
       "         [ 0.0868, -0.2061, -0.3291, -0.4058,  0.1281, -0.0261, -0.3626,\n",
       "           0.0995, -0.0595, -0.2830, -0.0648, -0.1516, -0.1428, -0.1134,\n",
       "          -0.1200,  0.0691, -0.0921, -0.2667,  0.0844,  0.1979],\n",
       "         [ 0.1157, -0.3061, -0.3729, -0.4839,  0.1812, -0.0832, -0.6138,\n",
       "           0.1702, -0.1507, -0.3296, -0.1318, -0.2027, -0.1886, -0.1697,\n",
       "          -0.1644,  0.1239, -0.1408, -0.4846,  0.1294,  0.2688],\n",
       "         [ 0.1216, -0.2422, -0.2923, -0.3834,  0.1236, -0.0891, -0.4668,\n",
       "           0.1325, -0.0885, -0.2406, -0.1228, -0.1433, -0.1582, -0.1090,\n",
       "          -0.1389,  0.0510, -0.1182, -0.3708,  0.1040,  0.2234],\n",
       "         [ 0.1016, -0.2936, -0.3307, -0.4113,  0.1472, -0.0726, -0.5705,\n",
       "           0.1824, -0.1560, -0.2972, -0.1419, -0.1974, -0.1912, -0.1647,\n",
       "          -0.1468,  0.1100, -0.1185, -0.4780,  0.1306,  0.2612]],\n",
       "\n",
       "        [[ 0.0524, -0.3238, -0.6407, -0.3789,  0.1500, -0.1767, -0.5889,\n",
       "           0.0741, -0.0948, -0.3078, -0.1565, -0.2778, -0.2212, -0.1807,\n",
       "          -0.2810,  0.3142, -0.1388, -0.5618,  0.1853,  0.2400],\n",
       "         [ 0.0612, -0.2477, -0.4923, -0.3286,  0.1000, -0.1617, -0.4533,\n",
       "           0.1283, -0.0690, -0.2659, -0.0950, -0.1838, -0.1958, -0.1335,\n",
       "          -0.2473,  0.2419, -0.1461, -0.4627,  0.1246,  0.2074],\n",
       "         [ 0.0531, -0.3227, -0.6392, -0.3791,  0.1495, -0.1756, -0.5889,\n",
       "           0.0753, -0.0940, -0.3084, -0.1569, -0.2775, -0.2195, -0.1804,\n",
       "          -0.2785,  0.3140, -0.1382, -0.5633,  0.1858,  0.2386],\n",
       "         [ 0.0446, -0.2849, -0.5610, -0.3244,  0.1441, -0.1473, -0.5349,\n",
       "           0.0420, -0.0675, -0.2841, -0.1222, -0.2222, -0.1789, -0.1626,\n",
       "          -0.2585,  0.2884, -0.0978, -0.4714,  0.1466,  0.2106],\n",
       "         [ 0.0519, -0.2768, -0.5572, -0.3643,  0.1202, -0.1632, -0.5130,\n",
       "           0.0881, -0.0850, -0.2881, -0.1255, -0.2572, -0.1951, -0.1694,\n",
       "          -0.2466,  0.2857, -0.1413, -0.5147,  0.1612,  0.1873],\n",
       "         [ 0.0538, -0.3247, -0.6397, -0.3784,  0.1495, -0.1756, -0.5894,\n",
       "           0.0756, -0.0952, -0.3079, -0.1580, -0.2781, -0.2204, -0.1809,\n",
       "          -0.2772,  0.3120, -0.1382, -0.5650,  0.1865,  0.2384],\n",
       "         [ 0.0590, -0.2898, -0.5725, -0.3389,  0.1152, -0.1677, -0.5014,\n",
       "           0.0437, -0.0724, -0.2404, -0.1528, -0.2686, -0.1761, -0.1594,\n",
       "          -0.2238,  0.2945, -0.1094, -0.5233,  0.1692,  0.1965],\n",
       "         [ 0.0415, -0.2770, -0.5123, -0.2860,  0.1093, -0.1542, -0.4421,\n",
       "          -0.0009, -0.0837, -0.1961, -0.1433, -0.2799, -0.1632, -0.1931,\n",
       "          -0.2029,  0.2685, -0.0849, -0.4522,  0.1506,  0.1697],\n",
       "         [ 0.0575, -0.3061, -0.6089, -0.3535,  0.1422, -0.1740, -0.5256,\n",
       "           0.0830, -0.0930, -0.2798, -0.1238, -0.2250, -0.2174, -0.1391,\n",
       "          -0.2787,  0.2641, -0.1364, -0.5170,  0.1554,  0.2255],\n",
       "         [ 0.0554, -0.3243, -0.6401, -0.3793,  0.1476, -0.1766, -0.5882,\n",
       "           0.0754, -0.0926, -0.3064, -0.1581, -0.2766, -0.2187, -0.1772,\n",
       "          -0.2762,  0.3132, -0.1379, -0.5666,  0.1860,  0.2378]],\n",
       "\n",
       "        [[ 0.0777, -0.2851, -0.4307, -0.2797,  0.1071, -0.1186, -0.4913,\n",
       "           0.0237, -0.1259, -0.1897, -0.0887, -0.1539, -0.2104, -0.0409,\n",
       "          -0.1880,  0.2358, -0.1250, -0.4352,  0.0837,  0.1795],\n",
       "         [ 0.0758, -0.2883, -0.4427, -0.2919,  0.1117, -0.1186, -0.4959,\n",
       "           0.0212, -0.1257, -0.1955, -0.0879, -0.1574, -0.2120, -0.0421,\n",
       "          -0.1982,  0.2405, -0.1294, -0.4354,  0.0818,  0.1807],\n",
       "         [ 0.0911, -0.3113, -0.5001, -0.3424,  0.1409, -0.1298, -0.5614,\n",
       "           0.0189, -0.1489, -0.2094, -0.1334, -0.2289, -0.2069, -0.0826,\n",
       "          -0.1944,  0.2524, -0.1489, -0.4818,  0.1275,  0.2069],\n",
       "         [ 0.1135, -0.3370, -0.5144, -0.4048,  0.1480, -0.1469, -0.6241,\n",
       "           0.0794, -0.1631, -0.2389, -0.1738, -0.2625, -0.2529, -0.1080,\n",
       "          -0.2398,  0.2488, -0.2000, -0.5509,  0.1526,  0.2292],\n",
       "         [ 0.1105, -0.3388, -0.5199, -0.4048,  0.1503, -0.1445, -0.6213,\n",
       "           0.0751, -0.1632, -0.2401, -0.1709, -0.2595, -0.2520, -0.1063,\n",
       "          -0.2427,  0.2511, -0.1973, -0.5460,  0.1479,  0.2295],\n",
       "         [ 0.0627, -0.2308, -0.3663, -0.3263,  0.1186, -0.0905, -0.3871,\n",
       "           0.0548, -0.1350, -0.1416, -0.1748, -0.2574, -0.1866, -0.1288,\n",
       "          -0.2072,  0.1470, -0.1707, -0.3307,  0.1144,  0.1768],\n",
       "         [ 0.1116, -0.3362, -0.5187, -0.4070,  0.1498, -0.1460, -0.6213,\n",
       "           0.0750, -0.1635, -0.2379, -0.1742, -0.2655, -0.2503, -0.1090,\n",
       "          -0.2410,  0.2505, -0.1997, -0.5456,  0.1515,  0.2301],\n",
       "         [ 0.1043, -0.2684, -0.3884, -0.3061,  0.0888, -0.1469, -0.5132,\n",
       "           0.0848, -0.1169, -0.1934, -0.0935, -0.1346, -0.2282, -0.0317,\n",
       "          -0.2172,  0.2312, -0.1598, -0.4652,  0.0802,  0.2016],\n",
       "         [ 0.1209, -0.2967, -0.4649, -0.3446,  0.1288, -0.1207, -0.5606,\n",
       "           0.0663, -0.1309, -0.2244, -0.1381, -0.2202, -0.2134, -0.0925,\n",
       "          -0.1939,  0.2069, -0.1657, -0.4900,  0.1357,  0.2000],\n",
       "         [ 0.0996, -0.3124, -0.4578, -0.3527,  0.1165, -0.1335, -0.5523,\n",
       "           0.0820, -0.1399, -0.2231, -0.1320, -0.1941, -0.2591, -0.0682,\n",
       "          -0.2438,  0.2344, -0.1815, -0.4989,  0.1061,  0.2046]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi head attention block applies multiple attention heads as can be seen in the paper \"Attention is all you need\", then concatenates the output and applies single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Sequence, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"output of single head\")\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Sequence, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Sequence, D_Model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0021, -0.2063, -0.3810,  ..., -0.1469,  0.3506,  0.0006],\n",
       "         [-0.0201, -0.1439, -0.4275,  ..., -0.1392,  0.3123,  0.0139],\n",
       "         [-0.0567, -0.2081, -0.3843,  ..., -0.1240,  0.3090,  0.0697],\n",
       "         ...,\n",
       "         [-0.0316, -0.2066, -0.4124,  ..., -0.1247,  0.3343,  0.0162],\n",
       "         [-0.1181, -0.2361, -0.3448,  ..., -0.0739,  0.3246,  0.0562],\n",
       "         [-0.0622, -0.2448, -0.4236,  ..., -0.1333,  0.3385,  0.0313]],\n",
       "\n",
       "        [[-0.0337, -0.2003, -0.3458,  ..., -0.0626,  0.2728,  0.0326],\n",
       "         [-0.0628, -0.1707, -0.3192,  ..., -0.0929,  0.2667,  0.0458],\n",
       "         [-0.0858, -0.1638, -0.3324,  ..., -0.1065,  0.2934,  0.0323],\n",
       "         ...,\n",
       "         [-0.0594, -0.1762, -0.3399,  ..., -0.1028,  0.2747,  0.0340],\n",
       "         [-0.0665, -0.1368, -0.2944,  ..., -0.0376,  0.2798, -0.0018],\n",
       "         [-0.0681, -0.1694, -0.3483,  ..., -0.0939,  0.2874,  0.0674]],\n",
       "\n",
       "        [[-0.0193, -0.1591, -0.2577,  ..., -0.0960,  0.3348,  0.0142],\n",
       "         [-0.0557, -0.1801, -0.2885,  ..., -0.1621,  0.3667,  0.0510],\n",
       "         [-0.0435, -0.1467, -0.2591,  ..., -0.0930,  0.3442,  0.0614],\n",
       "         ...,\n",
       "         [ 0.0084, -0.1778, -0.3092,  ..., -0.1029,  0.3463, -0.0046],\n",
       "         [-0.0073, -0.1740, -0.3018,  ..., -0.1224,  0.3866,  0.0019],\n",
       "         [-0.0354, -0.1529, -0.3026,  ..., -0.1453,  0.3716,  0.0789]],\n",
       "\n",
       "        [[-0.0290, -0.2062, -0.3710,  ..., -0.1973,  0.3333,  0.0802],\n",
       "         [-0.0263, -0.2324, -0.3655,  ..., -0.1771,  0.3897,  0.0642],\n",
       "         [-0.0433, -0.2015, -0.3261,  ..., -0.1844,  0.3812,  0.1227],\n",
       "         ...,\n",
       "         [-0.0629, -0.2121, -0.3654,  ..., -0.1794,  0.3861,  0.0691],\n",
       "         [-0.0361, -0.2145, -0.3770,  ..., -0.1743,  0.3723,  0.0792],\n",
       "         [-0.0484, -0.1781, -0.3829,  ..., -0.2044,  0.3732,  0.0516]],\n",
       "\n",
       "        [[-0.0770, -0.2120, -0.3573,  ..., -0.1490,  0.3811,  0.0211],\n",
       "         [-0.0637, -0.1713, -0.3456,  ..., -0.1501,  0.3807, -0.0088],\n",
       "         [-0.0960, -0.1667, -0.3540,  ..., -0.2026,  0.3794,  0.0647],\n",
       "         ...,\n",
       "         [-0.0509, -0.1780, -0.3796,  ..., -0.1443,  0.3395, -0.0193],\n",
       "         [-0.0739, -0.1848, -0.3406,  ..., -0.1404,  0.3671, -0.0102],\n",
       "         [-0.0595, -0.1557, -0.3053,  ..., -0.1203,  0.3545,  0.0335]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is made up of the following components:\n",
    "- multi-head attention block\n",
    "- simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention output\")\n",
    "        # Applying normalization and residual connection.\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Applying position-wise feedforward network.\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Applying normalization and residual connection.\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1095,  1.7434, -0.1895,  ..., -1.5398,  3.6488,  0.7854],\n",
       "         [-0.7645,  1.2969, -1.0531,  ..., -2.5017,  1.5297,  0.8551],\n",
       "         [-0.8297,  2.7754, -1.2093,  ..., -2.4729,  2.8615,  0.6606],\n",
       "         ...,\n",
       "         [-1.4022,  2.5721, -0.4436,  ..., -1.5164,  2.7852,  0.5435],\n",
       "         [-1.2438,  0.8830, -0.5776,  ..., -2.4402,  2.8595,  0.4045],\n",
       "         [-1.2411,  2.2304, -0.9106,  ..., -2.0922,  2.6895,  1.0805]],\n",
       "\n",
       "        [[-0.7219,  0.2134, -0.4994,  ..., -1.6049,  3.0083,  0.1484],\n",
       "         [-1.9537,  1.4357, -2.1847,  ..., -1.4964,  2.5868,  0.4772],\n",
       "         [-1.3738,  0.5723, -0.1929,  ..., -1.6825,  3.5363,  1.1446],\n",
       "         ...,\n",
       "         [-0.8332,  1.4404,  0.7170,  ..., -2.0522,  3.0696,  0.1300],\n",
       "         [-1.6958,  1.3214, -1.4585,  ..., -1.7573,  3.0327, -0.4057],\n",
       "         [-1.5803,  1.3241, -1.9630,  ..., -2.1887,  3.4233,  0.4559]],\n",
       "\n",
       "        [[-1.3827,  0.2911, -1.6660,  ..., -1.0388,  1.6009,  0.4921],\n",
       "         [ 0.4263,  0.2747, -0.9540,  ..., -1.2878,  2.0736, -0.4615],\n",
       "         [-2.0664,  0.6521, -1.1129,  ..., -2.2972,  2.3177,  2.7127],\n",
       "         ...,\n",
       "         [-0.5104,  1.3721, -0.5729,  ..., -1.2426,  2.9144,  0.2924],\n",
       "         [-1.5288,  1.8175, -0.5089,  ..., -1.6969,  2.9975,  0.7583],\n",
       "         [ 0.4150,  0.8497, -0.4416,  ..., -1.6209,  3.2036,  1.0231]],\n",
       "\n",
       "        [[-2.8697,  0.3947, -0.2473,  ..., -0.2581,  2.0730, -0.8049],\n",
       "         [ 0.0041,  1.1800, -0.0363,  ..., -1.0857,  2.2846,  0.2027],\n",
       "         [-1.9346, -0.1887, -0.4206,  ..., -1.0655,  1.1849,  0.8468],\n",
       "         ...,\n",
       "         [-1.4617,  1.3832, -0.0635,  ..., -2.1276,  2.0823,  0.2829],\n",
       "         [-1.4649,  0.1917, -0.8981,  ..., -1.1176,  3.5264,  1.6238],\n",
       "         [-2.1023,  0.6950, -0.2105,  ..., -0.6618,  2.7170, -1.0069]],\n",
       "\n",
       "        [[-0.4045,  1.3239, -1.1346,  ..., -1.2026,  2.7921,  0.9051],\n",
       "         [-0.4032,  1.6926, -0.6392,  ..., -0.8870,  3.0183,  0.9726],\n",
       "         [-1.8335,  0.9395, -1.2782,  ..., -0.4892,  2.7781,  0.9998],\n",
       "         ...,\n",
       "         [-0.8766,  1.2650, -0.3665,  ..., -2.1289,  2.5621,  0.8116],\n",
       "         [ 0.5160,  0.6391, -1.1104,  ..., -1.4679,  2.1763,  2.2421],\n",
       "         [-1.0462,  1.8211,  0.3407,  ..., -1.2942,  1.7656,  1.6957]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder is having six consecutive encoder blocks, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is same in structure as the encoder with just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Applying attention to inputs.\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Applying attention to the encoder outputs and outputs of the previous layer.\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Applying position-wise feedforward network.\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6062,  1.8064,  2.8349,  ...,  3.1780, -1.5658, -0.0221],\n",
       "         [-2.1802,  2.5221,  4.1545,  ...,  2.8810, -1.4469,  0.0577],\n",
       "         [-2.1807, -0.6327,  2.5743,  ...,  3.3030, -1.2394, -0.3203],\n",
       "         ...,\n",
       "         [-1.0182,  2.6559,  3.1723,  ...,  2.5830,  0.2688, -0.1503],\n",
       "         [-2.8523,  1.6591,  3.2992,  ...,  1.8542, -2.3638, -0.0734],\n",
       "         [-0.7815,  1.4557,  3.2753,  ...,  1.4924, -1.7132, -0.6898]],\n",
       "\n",
       "        [[-1.1719,  1.5686,  1.8237,  ...,  0.0991, -0.8198,  0.0669],\n",
       "         [ 0.6782,  2.6044,  3.5376,  ...,  0.1770, -2.7176,  0.2472],\n",
       "         [-0.1053,  1.9548,  3.9121,  ...,  1.8951, -2.1748,  0.4391],\n",
       "         ...,\n",
       "         [-1.5224,  3.0837,  2.7690,  ...,  2.3316, -2.3260, -0.3484],\n",
       "         [-1.3200,  2.7619,  3.1300,  ...,  2.4136, -3.4754, -0.5888],\n",
       "         [-0.7242,  1.7753,  3.5148,  ...,  2.4680, -3.4274, -0.0919]],\n",
       "\n",
       "        [[-0.2524,  3.2600,  3.9048,  ...,  2.2965, -0.8458, -0.0596],\n",
       "         [-0.5028,  1.8695,  3.0891,  ...,  1.3725, -0.1023, -0.4678],\n",
       "         [-1.3146,  3.0215,  3.7337,  ...,  2.9671, -1.9099,  0.2621],\n",
       "         ...,\n",
       "         [-0.7738,  2.2407,  3.3556,  ...,  1.2264, -2.3703,  0.0161],\n",
       "         [-0.1538,  2.5924,  3.8209,  ...,  2.6051, -1.6297, -0.3616],\n",
       "         [-1.4644,  1.9406,  0.9729,  ...,  2.6080, -1.1310, -0.3320]],\n",
       "\n",
       "        [[-1.6381,  2.4073,  2.9071,  ...,  2.8752,  0.8644, -0.7351],\n",
       "         [ 0.1383,  2.3699,  1.1990,  ...,  2.4707, -2.7074, -0.7845],\n",
       "         [-0.8837,  2.5311,  3.0683,  ...,  2.7022, -1.3903, -0.3311],\n",
       "         ...,\n",
       "         [-0.3626,  2.9435,  3.3861,  ...,  2.6728, -2.6044,  1.7059],\n",
       "         [-0.0887,  2.6205,  3.2545,  ...,  1.9877, -1.8076,  0.0586],\n",
       "         [-0.9009,  3.1610,  2.6281,  ...,  3.4722, -3.1484, -1.6272]],\n",
       "\n",
       "        [[-2.5613,  1.0670,  3.8378,  ...,  2.3061, -2.7524, -1.1354],\n",
       "         [-1.2835,  2.5082,  4.6196,  ...,  3.9098, -1.0951, -0.6298],\n",
       "         [-2.6911,  2.3469,  4.7609,  ...,  2.0611, -2.3652, -0.3477],\n",
       "         ...,\n",
       "         [-2.0136,  1.2363,  3.6225,  ...,  0.3316, -2.0255, -1.3708],\n",
       "         [-1.6573,  1.8548,  4.1828,  ...,  2.3922, -0.8017,  0.4843],\n",
       "         [-1.8833,  2.9921,  4.6620,  ...,  1.5668, -2.6841, -0.1885]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks don't have any notion of word order in a sentence. The Transformer explicitly adds the positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.4139,  4.9157,  4.4711,  ..., -3.1143, -5.0614,  7.0412],\n",
       "         [-2.3831,  0.3104,  6.8529,  ..., -1.1319, -3.3014,  6.2591],\n",
       "         [ 1.6324,  2.3121,  9.1768,  ..., -2.5963, -4.7169,  7.1547],\n",
       "         ...,\n",
       "         [-1.7021,  1.8205,  6.6961,  ..., -2.6435, -4.6028,  7.7010],\n",
       "         [-4.9978, -3.5000,  8.0692,  ..., -3.4289, -8.1106,  4.9312],\n",
       "         [-9.5400,  1.2028,  8.3823,  ..., -2.2996, -7.6988,  9.1668]],\n",
       "\n",
       "        [[-2.6065,  1.6870,  4.9403,  ..., -4.1684, -5.1463,  3.3897],\n",
       "         [-2.5092,  0.5683,  6.6001,  ..., -2.7813, -3.7902,  8.1763],\n",
       "         [-2.5709, -3.2853,  3.1206,  ..., -5.2751, -4.6666,  2.3572],\n",
       "         ...,\n",
       "         [-4.7743, -0.4509,  1.2315,  ..., -5.9751, -0.1397,  6.0353],\n",
       "         [-4.1172, -2.2306,  4.7744,  ..., -3.9229, -7.0780,  6.6143],\n",
       "         [-3.3333, -1.5585,  6.0781,  ..., -4.8825, -5.1019,  7.2790]],\n",
       "\n",
       "        [[-1.2495,  1.2900,  3.9429,  ..., -0.5574, -3.3021,  6.8422],\n",
       "         [-0.5495,  2.4290,  5.1255,  ..., -8.8799, -2.8951,  5.7421],\n",
       "         [-5.1043,  0.7440,  6.6147,  ..., -1.8546, -2.3592,  4.4629],\n",
       "         ...,\n",
       "         [-5.5811, -1.0355,  1.5376,  ..., -5.8403, -4.4981,  5.8315],\n",
       "         [-5.7936, -0.3082,  4.7754,  ..., -4.8886, -1.1721,  6.2788],\n",
       "         [-5.4751, -4.6799,  3.6703,  ..., -4.1921, -2.6944,  7.2196]],\n",
       "\n",
       "        [[-6.5478, -0.5478,  8.9452,  ..., -6.3775, -0.6220,  5.7300],\n",
       "         [-2.8628,  0.1125,  7.2112,  ..., -6.6190, -2.8813,  3.4587],\n",
       "         [-3.9381, -0.1995,  9.0015,  ..., -3.1852, -1.2017,  8.2019],\n",
       "         ...,\n",
       "         [-6.9671, -4.6106,  4.2121,  ..., -4.8730, -0.6864,  9.1654],\n",
       "         [-4.9845,  4.2786,  4.9264,  ..., -3.2192, -5.8974,  7.4058],\n",
       "         [-6.9895, -1.2504,  7.2676,  ..., -7.4972, -1.9588,  5.6111]],\n",
       "\n",
       "        [[-4.1257, -0.4628,  6.0278,  ..., -3.6313, -3.0909,  9.9173],\n",
       "         [-0.2975,  5.7263,  9.8412,  ..., -1.6050, -0.9424, 10.3332],\n",
       "         [-1.8784, -2.0677,  4.8761,  ..., -2.3732, -1.9149,  9.3347],\n",
       "         ...,\n",
       "         [-1.9414, -2.4877,  3.5404,  ..., -5.4948, -0.8323,  8.8133],\n",
       "         [-5.0910, -1.3568,  3.3552,  ..., -1.3693, -1.9791, 11.0517],\n",
       "         [-8.8961, -1.9219,  5.5123,  ..., -4.7265, -5.0320, 10.9889]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.enc_dec_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -0.6403,  -4.5193,  -0.1966,  ...,   1.8402,   6.1618,   1.1170],\n",
       "         [  1.9767,  -3.2395,   2.5544,  ...,   0.9177,   2.2143,   2.5810],\n",
       "         [  2.5105,   0.7868,  -3.8123,  ...,  -5.2317,   2.0836,   7.1244],\n",
       "         ...,\n",
       "         [  2.4850,  -5.8462,   2.3783,  ...,   0.4810,   6.2445,   2.8366],\n",
       "         [ -3.0172,  -3.8595,  -0.2488,  ...,   1.4502,   1.9901,   7.5986],\n",
       "         [ -6.4733,  -3.2405,   0.6273,  ...,  -0.0425,   9.1229,   2.0320]],\n",
       "\n",
       "        [[  5.6875,  -3.3613,  -2.2142,  ...,  -0.2712,   3.4600,   8.7570],\n",
       "         [  3.5279,  -5.2556,  -4.7838,  ...,  -3.8649,   3.9463,   8.3395],\n",
       "         [  1.5079, -10.3404,  -3.1070,  ...,   2.6497,   6.3558,  11.2612],\n",
       "         ...,\n",
       "         [  7.7668,  -6.2583,  -0.6006,  ...,   4.4620,   3.9692,   7.2246],\n",
       "         [  1.4855,  -2.9477,  -0.9925,  ...,   3.7489,   3.8718,  11.8489],\n",
       "         [  1.3355,  -3.5755,  -2.9303,  ...,   5.3816,  -0.8442,   7.1049]],\n",
       "\n",
       "        [[ -0.4408,  -2.1708,  -4.6246,  ...,   3.4977,  -2.6082,   4.3060],\n",
       "         [  2.2171,  -4.4941,  -2.2005,  ...,   4.5861,   2.2797,   5.2660],\n",
       "         [  7.9118,  -5.3285,  -3.8168,  ...,   3.3183,   3.8105,   5.3910],\n",
       "         ...,\n",
       "         [  2.1795,  -4.1073,  -5.0869,  ...,   2.2604,   3.9363,   6.0017],\n",
       "         [  2.4689,  -3.3188,  -4.8886,  ...,   0.9196,  -0.9283,   4.8025],\n",
       "         [  3.4621,  -2.7243,  -7.2587,  ...,   0.7954,  -0.6279,   6.0983]],\n",
       "\n",
       "        [[  4.6622,  -2.9030,  -0.7920,  ...,  -1.2700,   3.0735,   2.7459],\n",
       "         [  6.7231, -10.1421,  -8.1300,  ...,   1.1218,   4.3448,   3.3384],\n",
       "         [  6.0699,  -4.8648,  -1.6253,  ...,   2.7423,   1.2202,   5.2494],\n",
       "         ...,\n",
       "         [  3.9364, -11.3248,  -6.5793,  ...,  -2.1180,   4.9223,   1.9446],\n",
       "         [  2.0551,  -9.1130,  -4.8330,  ...,  -1.0857,   2.6203,   7.7561],\n",
       "         [  1.7320, -10.0117,  -5.3964,  ...,  -2.6731,   6.7621,   2.4959]],\n",
       "\n",
       "        [[  2.1469,  -5.7529,  -3.2375,  ...,   3.0948,   4.3935,   6.4392],\n",
       "         [  1.2818,  -1.3955,  -2.9940,  ...,   3.3509,   5.1268,   4.9351],\n",
       "         [  1.7012,  -5.1182,  -4.0078,  ...,   0.3713,   4.4845,   7.8123],\n",
       "         ...,\n",
       "         [  0.1417,  -5.5239,  -5.2037,  ...,   3.6403,   9.9475,   0.8523],\n",
       "         [  1.3011,  -7.2009,  -0.4926,  ...,  -1.1396,   4.2805,   7.0989],\n",
       "         [  1.2099,  -6.3029,  -9.7498,  ...,   4.8843,   8.4585,   4.4588]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
