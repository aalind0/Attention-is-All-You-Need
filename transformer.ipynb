{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=[{cls.__name__}, {name}, {tsr.shape}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "\n",
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1)\n",
    "        assert q.size(-1) == d_k\n",
    "        \n",
    "        # Compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature))\n",
    "        \n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        \n",
    "        attn = torch.exp(attn)\n",
    "        \n",
    "        log_size(attn, \"attention weight\") # Batch, Seq, Seq\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3649, 0.3846, 0.4327, 0.4742, 0.4355, 0.5667, 0.3473, 0.6755,\n",
       "          0.3682, 0.3680, 0.4695, 0.4058, 0.6266, 0.2510, 0.3948, 0.3747,\n",
       "          0.5287, 0.3257, 0.4595, 0.5059],\n",
       "         [0.5351, 0.4985, 0.4842, 0.4417, 0.4807, 0.6019, 0.4135, 0.7394,\n",
       "          0.3959, 0.5468, 0.5392, 0.4992, 0.6482, 0.3249, 0.5005, 0.4512,\n",
       "          0.6011, 0.3977, 0.4671, 0.5979],\n",
       "         [0.5860, 0.4891, 0.5454, 0.5410, 0.5302, 0.7143, 0.4550, 0.8638,\n",
       "          0.5148, 0.5719, 0.5614, 0.5711, 0.7120, 0.4048, 0.5165, 0.4857,\n",
       "          0.7224, 0.4134, 0.5618, 0.6230],\n",
       "         [0.3939, 0.3823, 0.3936, 0.4944, 0.4758, 0.5268, 0.2933, 0.6908,\n",
       "          0.3956, 0.3721, 0.3852, 0.4262, 0.5831, 0.2916, 0.4326, 0.4049,\n",
       "          0.5467, 0.3257, 0.4500, 0.5605],\n",
       "         [0.5742, 0.4893, 0.5407, 0.5308, 0.5406, 0.7119, 0.4438, 0.8632,\n",
       "          0.5032, 0.5491, 0.5844, 0.5421, 0.7282, 0.4040, 0.5288, 0.4995,\n",
       "          0.7226, 0.4289, 0.5676, 0.6335],\n",
       "         [0.5715, 0.5084, 0.5340, 0.5305, 0.5520, 0.7056, 0.4311, 0.8297,\n",
       "          0.4794, 0.5567, 0.5793, 0.5576, 0.7140, 0.4091, 0.5376, 0.5170,\n",
       "          0.7137, 0.4369, 0.5334, 0.6422],\n",
       "         [0.4504, 0.4321, 0.4293, 0.4491, 0.4280, 0.5149, 0.3806, 0.6855,\n",
       "          0.3729, 0.4747, 0.4407, 0.4571, 0.5923, 0.2326, 0.4138, 0.3618,\n",
       "          0.5238, 0.3174, 0.4439, 0.5400],\n",
       "         [0.4899, 0.4405, 0.5051, 0.5091, 0.4775, 0.6477, 0.3927, 0.8159,\n",
       "          0.4745, 0.4889, 0.5072, 0.5177, 0.6808, 0.3126, 0.4642, 0.4139,\n",
       "          0.6357, 0.3607, 0.5369, 0.5981],\n",
       "         [0.4892, 0.4305, 0.5241, 0.5016, 0.4639, 0.6669, 0.4259, 0.7288,\n",
       "          0.4263, 0.5132, 0.4836, 0.5060, 0.6287, 0.3805, 0.4541, 0.4292,\n",
       "          0.6056, 0.3555, 0.4803, 0.5010],\n",
       "         [0.5691, 0.4983, 0.5482, 0.5300, 0.5355, 0.7160, 0.4478, 0.8613,\n",
       "          0.4924, 0.5555, 0.5833, 0.5463, 0.7291, 0.4022, 0.5349, 0.4959,\n",
       "          0.7029, 0.4267, 0.5640, 0.6375]],\n",
       "\n",
       "        [[0.5905, 0.5624, 0.5642, 0.6100, 0.5166, 0.6094, 0.3280, 0.6438,\n",
       "          0.5126, 0.5244, 0.7228, 0.5098, 0.4808, 0.6686, 0.4935, 0.5628,\n",
       "          0.5335, 0.5390, 0.6813, 0.4292],\n",
       "         [0.5278, 0.4437, 0.5426, 0.5326, 0.5015, 0.6140, 0.2982, 0.5713,\n",
       "          0.4419, 0.4321, 0.6116, 0.4030, 0.4147, 0.6393, 0.3868, 0.5156,\n",
       "          0.4395, 0.4786, 0.6815, 0.3440],\n",
       "         [0.5833, 0.5752, 0.5650, 0.6275, 0.5148, 0.6103, 0.3291, 0.6546,\n",
       "          0.4948, 0.5342, 0.7127, 0.5139, 0.4526, 0.6779, 0.4986, 0.5557,\n",
       "          0.5169, 0.5413, 0.6956, 0.4308],\n",
       "         [0.4343, 0.3783, 0.5158, 0.5011, 0.4635, 0.5183, 0.2666, 0.4825,\n",
       "          0.3971, 0.3907, 0.5384, 0.3449, 0.3166, 0.5890, 0.3663, 0.4403,\n",
       "          0.3546, 0.4258, 0.6322, 0.3197],\n",
       "         [0.5871, 0.5832, 0.5524, 0.6354, 0.5038, 0.5980, 0.3325, 0.6711,\n",
       "          0.4912, 0.5471, 0.7205, 0.5270, 0.4603, 0.6702, 0.4987, 0.5473,\n",
       "          0.5234, 0.5422, 0.6886, 0.4349],\n",
       "         [0.5654, 0.5225, 0.4992, 0.5697, 0.4839, 0.6241, 0.2659, 0.5667,\n",
       "          0.4675, 0.4862, 0.6660, 0.4841, 0.3829, 0.6567, 0.4578, 0.4917,\n",
       "          0.4911, 0.4714, 0.6625, 0.3731],\n",
       "         [0.5064, 0.4802, 0.5272, 0.5643, 0.4310, 0.5061, 0.3238, 0.6385,\n",
       "          0.4438, 0.5100, 0.6182, 0.4956, 0.4660, 0.6187, 0.4220, 0.4753,\n",
       "          0.4953, 0.5273, 0.5748, 0.3885],\n",
       "         [0.3822, 0.2871, 0.4370, 0.3904, 0.4090, 0.4773, 0.2409, 0.4174,\n",
       "          0.3612, 0.3166, 0.4645, 0.2364, 0.3387, 0.5020, 0.3285, 0.4740,\n",
       "          0.3343, 0.3527, 0.4766, 0.2543],\n",
       "         [0.5087, 0.5580, 0.4541, 0.5809, 0.4263, 0.5235, 0.3220, 0.6166,\n",
       "          0.3820, 0.4743, 0.6377, 0.4402, 0.3889, 0.5481, 0.4479, 0.5282,\n",
       "          0.4355, 0.4862, 0.5997, 0.4174],\n",
       "         [0.5005, 0.4243, 0.4189, 0.4663, 0.4320, 0.6116, 0.2410, 0.5136,\n",
       "          0.3612, 0.3714, 0.5561, 0.3741, 0.3546, 0.5673, 0.3302, 0.4520,\n",
       "          0.4080, 0.4008, 0.6181, 0.3126]],\n",
       "\n",
       "        [[0.3871, 0.6130, 0.4591, 0.5220, 0.6654, 0.5915, 0.4718, 0.4243,\n",
       "          0.6674, 0.7133, 0.4400, 0.4799, 0.5163, 0.5232, 0.5868, 0.5784,\n",
       "          0.5038, 0.5102, 0.4616, 0.6044],\n",
       "         [0.3208, 0.5115, 0.3350, 0.4775, 0.5862, 0.5119, 0.3427, 0.3915,\n",
       "          0.5637, 0.5796, 0.4056, 0.3960, 0.4586, 0.4595, 0.4838, 0.5532,\n",
       "          0.4631, 0.4336, 0.4184, 0.5470],\n",
       "         [0.3790, 0.6046, 0.4763, 0.5270, 0.6698, 0.5876, 0.4782, 0.4402,\n",
       "          0.6742, 0.7001, 0.4514, 0.4861, 0.5305, 0.5251, 0.5860, 0.5757,\n",
       "          0.5137, 0.5066, 0.4699, 0.5955],\n",
       "         [0.3946, 0.6020, 0.4611, 0.5250, 0.6595, 0.5972, 0.4750, 0.4280,\n",
       "          0.6797, 0.7146, 0.4421, 0.4832, 0.5172, 0.5277, 0.5756, 0.5721,\n",
       "          0.4971, 0.5133, 0.4633, 0.6038],\n",
       "         [0.3616, 0.5953, 0.4733, 0.5251, 0.6695, 0.5939, 0.4725, 0.4458,\n",
       "          0.6580, 0.7093, 0.4489, 0.4938, 0.5294, 0.5387, 0.5954, 0.5884,\n",
       "          0.5203, 0.5210, 0.4761, 0.5956],\n",
       "         [0.1576, 0.3872, 0.2853, 0.3549, 0.4969, 0.3218, 0.2639, 0.3352,\n",
       "          0.3703, 0.4406, 0.3254, 0.3056, 0.4140, 0.3905, 0.4317, 0.4462,\n",
       "          0.4121, 0.3000, 0.3498, 0.5016],\n",
       "         [0.3854, 0.5760, 0.4404, 0.5067, 0.6104, 0.5756, 0.4457, 0.3985,\n",
       "          0.6245, 0.7054, 0.3683, 0.4389, 0.4521, 0.4509, 0.5870, 0.5389,\n",
       "          0.4604, 0.5234, 0.4422, 0.5347],\n",
       "         [0.3712, 0.6122, 0.4862, 0.5223, 0.6683, 0.5893, 0.4878, 0.4318,\n",
       "          0.6720, 0.7157, 0.4471, 0.4924, 0.5347, 0.5343, 0.5992, 0.5800,\n",
       "          0.5116, 0.5205, 0.4675, 0.5917],\n",
       "         [0.2917, 0.5495, 0.4000, 0.4353, 0.6219, 0.5622, 0.4305, 0.3886,\n",
       "          0.5798, 0.6565, 0.4193, 0.4747, 0.5119, 0.5289, 0.5183, 0.5689,\n",
       "          0.4550, 0.5055, 0.4360, 0.5457],\n",
       "         [0.3651, 0.6002, 0.4654, 0.5157, 0.6746, 0.5896, 0.4701, 0.4379,\n",
       "          0.6450, 0.7119, 0.4427, 0.4831, 0.5161, 0.5329, 0.5981, 0.5823,\n",
       "          0.5188, 0.5066, 0.4676, 0.6062]],\n",
       "\n",
       "        [[0.4188, 0.5252, 0.3822, 0.3808, 0.4416, 0.3654, 0.6100, 0.5354,\n",
       "          0.3358, 0.3702, 0.6736, 0.4471, 0.5872, 0.6024, 0.4102, 0.4230,\n",
       "          0.4111, 0.3532, 0.4733, 0.2682],\n",
       "         [0.4821, 0.5622, 0.4700, 0.3909, 0.4500, 0.3815, 0.7734, 0.5991,\n",
       "          0.4214, 0.4107, 0.6699, 0.5460, 0.5648, 0.6177, 0.4553, 0.4221,\n",
       "          0.4200, 0.3944, 0.5346, 0.2792],\n",
       "         [0.5546, 0.6495, 0.5021, 0.4563, 0.5462, 0.4066, 0.7674, 0.6967,\n",
       "          0.4850, 0.4781, 0.7558, 0.5579, 0.6602, 0.6617, 0.5283, 0.4867,\n",
       "          0.4538, 0.4060, 0.5721, 0.3024],\n",
       "         [0.5442, 0.6485, 0.5047, 0.4556, 0.5454, 0.4145, 0.7651, 0.6946,\n",
       "          0.4844, 0.4696, 0.7682, 0.5534, 0.6440, 0.6795, 0.5157, 0.4713,\n",
       "          0.4646, 0.4047, 0.5799, 0.3072],\n",
       "         [0.3838, 0.4485, 0.3994, 0.2993, 0.3790, 0.2097, 0.5958, 0.5282,\n",
       "          0.3728, 0.3554, 0.5070, 0.4157, 0.3789, 0.4927, 0.3658, 0.2969,\n",
       "          0.3635, 0.2457, 0.4825, 0.2510],\n",
       "         [0.5413, 0.6476, 0.5030, 0.4704, 0.5334, 0.4041, 0.7716, 0.6974,\n",
       "          0.4718, 0.4631, 0.7774, 0.5445, 0.6647, 0.6814, 0.5205, 0.4733,\n",
       "          0.4748, 0.4005, 0.5938, 0.3012],\n",
       "         [0.5439, 0.6598, 0.5007, 0.4741, 0.5455, 0.4215, 0.7630, 0.7050,\n",
       "          0.4780, 0.4592, 0.7835, 0.5318, 0.6582, 0.6825, 0.5216, 0.4719,\n",
       "          0.4758, 0.4038, 0.5894, 0.3032],\n",
       "         [0.4598, 0.6059, 0.4463, 0.4371, 0.5141, 0.3288, 0.6425, 0.6764,\n",
       "          0.4362, 0.4523, 0.6801, 0.4216, 0.5468, 0.6088, 0.4494, 0.3920,\n",
       "          0.4575, 0.2881, 0.5466, 0.2999],\n",
       "         [0.3796, 0.5057, 0.3255, 0.3339, 0.4201, 0.3213, 0.4616, 0.5333,\n",
       "          0.3714, 0.2653, 0.5249, 0.2838, 0.3519, 0.4425, 0.3811, 0.2752,\n",
       "          0.3296, 0.2527, 0.4055, 0.2099],\n",
       "         [0.5434, 0.6452, 0.5011, 0.4615, 0.5350, 0.4279, 0.7691, 0.6836,\n",
       "          0.4862, 0.4684, 0.7736, 0.5595, 0.6581, 0.6808, 0.5174, 0.4660,\n",
       "          0.4877, 0.3957, 0.5835, 0.3037]],\n",
       "\n",
       "        [[0.4754, 0.3784, 0.5024, 0.5084, 0.5789, 0.6202, 0.5171, 0.5799,\n",
       "          0.3886, 0.5081, 0.3899, 0.4308, 0.5635, 0.5842, 0.5313, 0.5427,\n",
       "          0.4820, 0.7087, 0.7057, 0.5384],\n",
       "         [0.4237, 0.3516, 0.4822, 0.4950, 0.4866, 0.5696, 0.4924, 0.5046,\n",
       "          0.3829, 0.4190, 0.3627, 0.4200, 0.4652, 0.5021, 0.4775, 0.4956,\n",
       "          0.4066, 0.6060, 0.6527, 0.5220],\n",
       "         [0.4816, 0.3847, 0.5030, 0.5178, 0.5735, 0.6261, 0.5236, 0.5815,\n",
       "          0.4028, 0.5081, 0.3930, 0.4351, 0.5608, 0.5896, 0.5262, 0.5551,\n",
       "          0.4851, 0.7102, 0.6993, 0.5384],\n",
       "         [0.4260, 0.3588, 0.4777, 0.5050, 0.4813, 0.5704, 0.4905, 0.5038,\n",
       "          0.3888, 0.4148, 0.3674, 0.4199, 0.4647, 0.5057, 0.4716, 0.5086,\n",
       "          0.4039, 0.6015, 0.6565, 0.5171],\n",
       "         [0.4156, 0.3921, 0.4726, 0.4668, 0.4653, 0.5473, 0.4731, 0.5159,\n",
       "          0.3280, 0.4143, 0.2819, 0.4125, 0.5567, 0.5128, 0.4437, 0.4702,\n",
       "          0.3900, 0.6439, 0.6105, 0.5213],\n",
       "         [0.3169, 0.3056, 0.3133, 0.3689, 0.4864, 0.4776, 0.4036, 0.5127,\n",
       "          0.3251, 0.4455, 0.3162, 0.3068, 0.4832, 0.3930, 0.4004, 0.3819,\n",
       "          0.3421, 0.5149, 0.4858, 0.4021],\n",
       "         [0.3777, 0.3485, 0.4199, 0.4568, 0.5077, 0.6145, 0.4852, 0.5834,\n",
       "          0.3663, 0.4528, 0.3573, 0.4121, 0.5052, 0.4878, 0.4744, 0.4982,\n",
       "          0.4319, 0.5776, 0.5894, 0.4443],\n",
       "         [0.3922, 0.3522, 0.4237, 0.4648, 0.5063, 0.6124, 0.4820, 0.5789,\n",
       "          0.3710, 0.4581, 0.3591, 0.4160, 0.5059, 0.4903, 0.4666, 0.5138,\n",
       "          0.4393, 0.5899, 0.5830, 0.4637],\n",
       "         [0.4197, 0.3194, 0.4708, 0.4262, 0.4755, 0.5284, 0.5073, 0.4826,\n",
       "          0.3204, 0.4542, 0.3491, 0.3797, 0.5010, 0.5332, 0.5257, 0.4668,\n",
       "          0.4751, 0.6406, 0.5983, 0.4171],\n",
       "         [0.4181, 0.3200, 0.4639, 0.4185, 0.4799, 0.5210, 0.5021, 0.4851,\n",
       "          0.3114, 0.4578, 0.3440, 0.3775, 0.5132, 0.5282, 0.5227, 0.4608,\n",
       "          0.4701, 0.6437, 0.5981, 0.4196]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        # We assume that the queries, keys, features all have the same feature size.\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries)\n",
    "        K = self.key_tfm(keys)\n",
    "        V = self.value_tfm(values)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        \n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5443e-02, -3.1271e-03,  2.5496e-01,  2.7784e-01,  3.4062e-01,\n",
       "          -3.7640e-01, -1.1357e-01,  3.7572e-01,  2.5182e-01,  1.0895e-01,\n",
       "          -1.1351e-02,  5.9561e-02, -3.5149e-01,  1.4703e-01, -1.6218e-01,\n",
       "          -1.1671e-01, -6.2780e-01, -4.7114e-01, -1.9149e-01, -2.4516e-01],\n",
       "         [ 4.4107e-02,  9.5427e-03,  2.0750e-01,  2.3518e-01,  2.3767e-01,\n",
       "          -2.7780e-01, -7.3458e-02,  2.9576e-01,  1.9902e-01,  8.6987e-02,\n",
       "          -3.5164e-02,  5.2046e-02, -2.4403e-01,  9.7605e-02, -7.9549e-02,\n",
       "          -6.6505e-02, -4.8039e-01, -3.8352e-01, -1.3561e-01, -1.8427e-01],\n",
       "         [ 2.4462e-02,  3.5541e-02,  2.3646e-01,  2.6081e-01,  2.8128e-01,\n",
       "          -3.2538e-01, -7.1484e-02,  3.3687e-01,  2.2953e-01,  9.6201e-02,\n",
       "          -2.6813e-02,  6.9793e-02, -2.9232e-01,  1.2726e-01, -1.1854e-01,\n",
       "          -9.2911e-02, -5.6253e-01, -4.5004e-01, -1.4179e-01, -2.1471e-01],\n",
       "         [ 3.9473e-02, -4.8544e-03,  2.1772e-01,  2.8330e-01,  3.2407e-01,\n",
       "          -3.3086e-01, -1.2025e-01,  3.5603e-01,  2.0415e-01,  8.6630e-02,\n",
       "          -5.8936e-04,  6.3280e-02, -3.1300e-01,  1.5240e-01, -1.7113e-01,\n",
       "          -1.0116e-01, -5.6981e-01, -4.2467e-01, -1.7274e-01, -2.3792e-01],\n",
       "         [ 2.0627e-02, -2.9557e-02,  2.2053e-01,  2.5812e-01,  3.0971e-01,\n",
       "          -3.1407e-01, -8.1113e-02,  3.2655e-01,  2.5008e-01,  1.1176e-01,\n",
       "          -9.9376e-03,  3.0606e-02, -3.2590e-01,  1.4029e-01, -1.3166e-01,\n",
       "          -1.0013e-01, -5.7977e-01, -4.2164e-01, -1.6479e-01, -1.8825e-01],\n",
       "         [ 2.7300e-02, -3.7909e-03,  2.5428e-01,  2.7826e-01,  3.4120e-01,\n",
       "          -3.7544e-01, -1.1371e-01,  3.7524e-01,  2.5287e-01,  1.0748e-01,\n",
       "          -1.3645e-02,  6.1549e-02, -3.5101e-01,  1.4666e-01, -1.6261e-01,\n",
       "          -1.1625e-01, -6.2806e-01, -4.7282e-01, -1.9338e-01, -2.4440e-01],\n",
       "         [ 2.9191e-02, -2.7999e-03,  2.5714e-01,  2.8018e-01,  3.3922e-01,\n",
       "          -3.7516e-01, -1.1135e-01,  3.7630e-01,  2.5376e-01,  1.0543e-01,\n",
       "          -1.5707e-02,  6.1288e-02, -3.4730e-01,  1.4494e-01, -1.6016e-01,\n",
       "          -1.1441e-01, -6.2717e-01, -4.7352e-01, -1.9308e-01, -2.4264e-01],\n",
       "         [ 4.8061e-02, -2.2403e-02,  2.2977e-01,  2.2523e-01,  3.2611e-01,\n",
       "          -3.4244e-01, -1.1376e-01,  3.1610e-01,  2.3764e-01,  9.9444e-02,\n",
       "          -3.4246e-02,  7.5663e-02, -3.5772e-01,  1.2466e-01, -1.5102e-01,\n",
       "          -1.1011e-01, -5.7000e-01, -4.3554e-01, -1.9738e-01, -2.3445e-01],\n",
       "         [ 2.4293e-02, -2.5527e-03,  2.5199e-01,  2.7688e-01,  3.4058e-01,\n",
       "          -3.7521e-01, -1.1487e-01,  3.7501e-01,  2.5239e-01,  1.1000e-01,\n",
       "          -1.1569e-02,  6.1949e-02, -3.5314e-01,  1.4650e-01, -1.6185e-01,\n",
       "          -1.1602e-01, -6.2945e-01, -4.7249e-01, -1.9535e-01, -2.4374e-01],\n",
       "         [ 2.5738e-02, -3.2764e-03,  2.5407e-01,  2.7557e-01,  3.4141e-01,\n",
       "          -3.7682e-01, -1.1510e-01,  3.7441e-01,  2.5180e-01,  1.0960e-01,\n",
       "          -1.1973e-02,  6.1550e-02, -3.5444e-01,  1.4634e-01, -1.6257e-01,\n",
       "          -1.1700e-01, -6.2819e-01, -4.7160e-01, -1.9399e-01, -2.4614e-01]],\n",
       "\n",
       "        [[ 3.1204e-02,  6.2702e-02,  2.9995e-01,  2.4697e-01,  1.7851e-01,\n",
       "          -3.0659e-01, -6.9777e-02,  3.3045e-01,  1.5035e-01, -4.3958e-03,\n",
       "          -1.6253e-02,  8.7943e-02, -1.5335e-01, -2.6765e-02, -9.2389e-02,\n",
       "          -4.5350e-02, -4.7437e-01, -3.4421e-01, -2.1683e-01, -2.0226e-01],\n",
       "         [ 4.0547e-02,  3.6164e-02,  3.5199e-01,  3.0161e-01,  2.3162e-01,\n",
       "          -3.9338e-01, -1.1257e-01,  3.8848e-01,  1.6690e-01,  2.3142e-02,\n",
       "          -2.4185e-02,  1.0996e-01, -1.8414e-01, -2.9524e-02, -9.7893e-02,\n",
       "          -4.6605e-02, -5.5645e-01, -4.2247e-01, -2.7556e-01, -2.1857e-01],\n",
       "         [ 5.3072e-02,  1.5491e-02,  3.0738e-01,  2.6317e-01,  2.0145e-01,\n",
       "          -2.8564e-01, -4.8875e-02,  2.7617e-01,  1.4363e-01,  2.9369e-02,\n",
       "          -7.0691e-02,  7.0114e-02, -1.4045e-01, -2.7083e-02, -8.2021e-02,\n",
       "          -5.2714e-02, -4.3993e-01, -3.1588e-01, -1.8984e-01, -1.4946e-01],\n",
       "         [ 4.1992e-02,  3.3503e-02,  3.5558e-01,  2.9857e-01,  2.3383e-01,\n",
       "          -3.9203e-01, -1.1154e-01,  3.9160e-01,  1.6913e-01,  1.8500e-02,\n",
       "          -2.5423e-02,  1.0713e-01, -1.8435e-01, -2.9805e-02, -9.6517e-02,\n",
       "          -4.5565e-02, -5.5665e-01, -4.2467e-01, -2.7737e-01, -2.1957e-01],\n",
       "         [ 3.0666e-02, -1.3360e-02,  2.8592e-01,  2.2587e-01,  2.2561e-01,\n",
       "          -2.9261e-01, -1.1050e-01,  3.3641e-01,  1.4236e-01,  3.0297e-02,\n",
       "           7.1833e-03,  6.0053e-02, -1.8641e-01,  1.5739e-04, -9.2318e-02,\n",
       "          -3.6980e-02, -4.5175e-01, -3.6498e-01, -2.2660e-01, -1.9075e-01],\n",
       "         [ 5.1499e-02, -1.0495e-02,  3.1433e-01,  2.3646e-01,  2.2616e-01,\n",
       "          -3.3846e-01, -1.4032e-01,  3.6706e-01,  1.5103e-01,  2.1763e-02,\n",
       "          -9.7750e-03,  8.9603e-02, -1.9210e-01, -2.4945e-02, -7.7100e-02,\n",
       "          -3.0763e-02, -4.8596e-01, -3.9844e-01, -2.6602e-01, -2.2289e-01],\n",
       "         [ 4.2202e-02,  3.2636e-02,  3.5499e-01,  2.9942e-01,  2.3233e-01,\n",
       "          -3.9183e-01, -1.1193e-01,  3.9041e-01,  1.6920e-01,  2.1151e-02,\n",
       "          -2.7039e-02,  1.0749e-01, -1.8278e-01, -3.1010e-02, -9.5181e-02,\n",
       "          -4.5636e-02, -5.5614e-01, -4.2430e-01, -2.7617e-01, -2.1892e-01],\n",
       "         [ 5.5090e-02,  2.9015e-02,  2.7922e-01,  2.6222e-01,  1.7035e-01,\n",
       "          -2.9908e-01, -9.1150e-02,  2.8695e-01,  1.0722e-01,  3.1180e-02,\n",
       "          -2.3656e-02,  9.9889e-02, -1.3080e-01, -3.8156e-02, -8.4812e-02,\n",
       "          -3.3508e-02, -4.4258e-01, -2.9706e-01, -2.0065e-01, -1.9358e-01],\n",
       "         [ 3.3100e-02,  6.0320e-02,  2.9782e-01,  2.4464e-01,  1.7632e-01,\n",
       "          -3.0434e-01, -7.3179e-02,  3.2888e-01,  1.4955e-01, -3.6434e-03,\n",
       "          -1.5690e-02,  8.8979e-02, -1.5325e-01, -2.8684e-02, -9.1219e-02,\n",
       "          -4.4951e-02, -4.7135e-01, -3.4256e-01, -2.1643e-01, -2.0439e-01],\n",
       "         [ 5.1819e-02, -1.1418e-02,  3.1490e-01,  2.3727e-01,  2.2792e-01,\n",
       "          -3.3830e-01, -1.4019e-01,  3.6725e-01,  1.5106e-01,  2.1405e-02,\n",
       "          -7.7772e-03,  8.8757e-02, -1.9401e-01, -2.3663e-02, -7.9390e-02,\n",
       "          -3.1551e-02, -4.8687e-01, -3.9818e-01, -2.6541e-01, -2.2393e-01]],\n",
       "\n",
       "        [[ 6.4020e-02, -6.5200e-02,  2.5587e-01,  3.7114e-01,  1.4389e-01,\n",
       "          -3.9165e-01, -6.3968e-02,  4.1215e-01,  3.1169e-01,  1.8935e-02,\n",
       "          -6.0783e-02,  6.1638e-02, -1.8942e-01,  7.6680e-02, -1.0826e-01,\n",
       "          -1.0557e-01, -4.9751e-01, -4.2294e-01,  2.4783e-03, -3.2630e-01],\n",
       "         [ 9.2927e-02, -1.2039e-01,  2.4289e-01,  3.2713e-01,  1.1681e-01,\n",
       "          -3.4789e-01, -5.6731e-02,  3.5335e-01,  3.1687e-01, -6.2081e-03,\n",
       "          -4.0828e-02,  2.8240e-02, -1.7449e-01,  6.8802e-02, -1.0996e-01,\n",
       "          -1.2035e-01, -4.4070e-01, -3.8140e-01,  4.7945e-02, -3.2770e-01],\n",
       "         [ 4.9200e-02, -2.4181e-02,  2.2457e-01,  2.5844e-01,  1.0595e-01,\n",
       "          -3.0746e-01, -7.0969e-02,  3.3617e-01,  2.2975e-01,  1.1700e-02,\n",
       "          -8.5041e-02,  8.1092e-02, -1.2469e-01,  4.6547e-02, -5.5435e-02,\n",
       "          -5.5098e-02, -3.8296e-01, -3.2737e-01, -2.0034e-02, -2.8314e-01],\n",
       "         [ 8.4140e-02, -9.3396e-02,  2.2274e-01,  3.1716e-01,  1.4804e-01,\n",
       "          -3.3726e-01, -6.4170e-02,  3.5106e-01,  2.7488e-01, -6.9085e-03,\n",
       "          -3.2586e-02,  6.1964e-02, -1.9602e-01,  6.0119e-02, -1.1146e-01,\n",
       "          -8.7645e-02, -4.5953e-01, -3.7488e-01, -2.7030e-02, -2.7748e-01],\n",
       "         [ 6.4603e-02, -6.5803e-02,  2.5375e-01,  3.6927e-01,  1.4318e-01,\n",
       "          -3.9012e-01, -6.5188e-02,  4.1042e-01,  3.1032e-01,  2.0728e-02,\n",
       "          -5.8869e-02,  6.1787e-02, -1.8901e-01,  7.7576e-02, -1.0895e-01,\n",
       "          -1.0481e-01, -4.9743e-01, -4.2150e-01,  3.2544e-03, -3.2717e-01],\n",
       "         [ 3.6266e-02, -2.9851e-02,  2.4917e-01,  3.3280e-01,  1.1813e-01,\n",
       "          -3.5593e-01, -6.4718e-02,  4.0792e-01,  2.9370e-01,  1.3931e-02,\n",
       "          -6.3988e-02,  5.4983e-02, -1.5965e-01,  4.8392e-02, -8.1894e-02,\n",
       "          -9.0239e-02, -4.5233e-01, -3.9835e-01, -3.0408e-02, -3.0131e-01],\n",
       "         [ 6.2832e-02, -6.6707e-02,  2.5177e-01,  3.6884e-01,  1.4320e-01,\n",
       "          -3.9112e-01, -6.5661e-02,  4.1151e-01,  3.1081e-01,  1.9541e-02,\n",
       "          -5.6701e-02,  6.1175e-02, -1.8951e-01,  7.8375e-02, -1.0945e-01,\n",
       "          -1.0527e-01, -4.9628e-01, -4.2329e-01,  3.0325e-03, -3.2535e-01],\n",
       "         [ 1.8125e-02, -1.1121e-02,  2.2673e-01,  3.2735e-01,  1.0715e-01,\n",
       "          -3.2791e-01, -3.8548e-02,  3.7715e-01,  2.9430e-01,  1.6292e-02,\n",
       "          -7.0888e-02,  3.4493e-02, -1.4160e-01,  4.5564e-02, -7.6098e-02,\n",
       "          -1.0877e-01, -4.1953e-01, -3.7915e-01, -1.4158e-02, -2.6043e-01],\n",
       "         [ 7.2586e-02, -7.9709e-02,  2.2244e-01,  3.2477e-01,  1.5282e-01,\n",
       "          -3.4061e-01, -6.0747e-02,  3.6029e-01,  2.7398e-01, -1.2167e-03,\n",
       "          -3.3214e-02,  6.3904e-02, -1.9814e-01,  6.4433e-02, -1.0914e-01,\n",
       "          -8.5989e-02, -4.6925e-01, -3.8496e-01, -3.2802e-02, -2.7056e-01],\n",
       "         [ 3.3405e-02, -2.8532e-02,  2.4661e-01,  3.3361e-01,  1.1812e-01,\n",
       "          -3.5810e-01, -6.6778e-02,  4.1127e-01,  2.9349e-01,  1.5427e-02,\n",
       "          -6.2184e-02,  5.5173e-02, -1.6016e-01,  4.8435e-02, -8.1650e-02,\n",
       "          -9.0024e-02, -4.5299e-01, -4.0083e-01, -3.3042e-02, -3.0081e-01]],\n",
       "\n",
       "        [[ 1.5564e-01, -3.0212e-02,  3.0349e-01,  3.9587e-01,  2.3326e-01,\n",
       "          -4.0980e-01, -7.6297e-02,  3.7372e-01,  3.2749e-01, -2.3971e-02,\n",
       "          -1.4199e-01,  1.0535e-01, -1.5712e-01, -8.1345e-02, -2.2459e-01,\n",
       "          -1.4303e-01, -4.3229e-01, -4.6834e-01, -2.7717e-01, -2.3794e-01],\n",
       "         [ 1.5715e-01, -3.1242e-02,  3.0378e-01,  3.9453e-01,  2.3398e-01,\n",
       "          -4.0983e-01, -7.5290e-02,  3.7237e-01,  3.2676e-01, -2.4669e-02,\n",
       "          -1.4189e-01,  1.0541e-01, -1.5817e-01, -8.0830e-02, -2.2433e-01,\n",
       "          -1.4259e-01, -4.3212e-01, -4.6792e-01, -2.7557e-01, -2.3799e-01],\n",
       "         [ 1.4773e-01, -2.1692e-02,  2.5095e-01,  3.4873e-01,  2.2066e-01,\n",
       "          -3.6187e-01, -5.6104e-02,  3.0952e-01,  2.7720e-01, -1.7807e-02,\n",
       "          -1.0957e-01,  1.0429e-01, -1.7694e-01, -5.1998e-02, -2.1041e-01,\n",
       "          -1.4675e-01, -4.0837e-01, -4.2017e-01, -1.9278e-01, -2.3707e-01],\n",
       "         [ 1.1393e-01,  1.6952e-03,  3.1207e-01,  3.1839e-01,  2.0042e-01,\n",
       "          -3.4014e-01, -4.9048e-02,  3.3642e-01,  2.6865e-01, -5.0565e-02,\n",
       "          -1.3456e-01,  7.7625e-02, -1.0146e-01, -7.2617e-02, -1.7616e-01,\n",
       "          -9.7473e-02, -3.5566e-01, -3.9921e-01, -2.7742e-01, -1.4870e-01],\n",
       "         [ 1.1061e-01,  6.7487e-03,  3.0410e-01,  3.6419e-01,  2.1033e-01,\n",
       "          -4.0826e-01, -7.2601e-02,  3.7182e-01,  2.8841e-01, -3.3429e-02,\n",
       "          -1.0374e-01,  9.9030e-02, -1.3469e-01, -7.5672e-02, -2.1090e-01,\n",
       "          -1.1689e-01, -3.9554e-01, -4.4026e-01, -2.9721e-01, -1.9375e-01],\n",
       "         [ 1.5100e-01, -2.6566e-02,  2.5384e-01,  3.4742e-01,  2.2381e-01,\n",
       "          -3.6102e-01, -5.6331e-02,  3.1045e-01,  2.7869e-01, -2.1087e-02,\n",
       "          -1.1160e-01,  1.0180e-01, -1.7753e-01, -5.1739e-02, -2.0952e-01,\n",
       "          -1.4657e-01, -4.0731e-01, -4.2070e-01, -1.9267e-01, -2.3851e-01],\n",
       "         [ 1.5544e-01, -4.9345e-02,  2.2326e-01,  3.2110e-01,  1.5974e-01,\n",
       "          -3.7397e-01, -7.3484e-02,  2.8893e-01,  2.8129e-01, -8.4811e-03,\n",
       "          -1.3306e-01,  1.1075e-01, -1.0394e-01, -9.7600e-02, -1.8294e-01,\n",
       "          -9.4620e-02, -3.2273e-01, -3.6948e-01, -2.6745e-01, -1.9384e-01],\n",
       "         [ 1.5591e-01, -3.0501e-02,  3.0304e-01,  3.9567e-01,  2.3407e-01,\n",
       "          -4.0878e-01, -7.5901e-02,  3.7295e-01,  3.2715e-01, -2.4356e-02,\n",
       "          -1.4159e-01,  1.0490e-01, -1.5824e-01, -8.0775e-02, -2.2451e-01,\n",
       "          -1.4394e-01, -4.3239e-01, -4.6830e-01, -2.7512e-01, -2.3911e-01],\n",
       "         [ 1.3975e-01, -2.4271e-02,  2.0867e-01,  2.6712e-01,  1.6987e-01,\n",
       "          -2.1021e-01,  1.2387e-02,  1.7644e-01,  2.1127e-01,  1.6859e-02,\n",
       "          -1.2722e-01,  5.7370e-02, -1.3190e-01, -2.8080e-02, -1.5007e-01,\n",
       "          -9.1831e-02, -3.0283e-01, -2.7050e-01, -1.2938e-01, -1.1579e-01],\n",
       "         [ 8.2196e-02,  6.1986e-03,  2.6664e-01,  3.2984e-01,  1.8223e-01,\n",
       "          -3.5349e-01, -5.6974e-02,  3.3738e-01,  2.5302e-01, -1.2299e-02,\n",
       "          -9.7134e-02,  6.9787e-02, -1.1315e-01, -5.8117e-02, -1.6424e-01,\n",
       "          -9.4693e-02, -3.3006e-01, -3.8748e-01, -2.4525e-01, -1.6536e-01]],\n",
       "\n",
       "        [[ 7.9379e-02, -6.2422e-02,  2.0715e-01,  1.0198e-01,  1.5556e-01,\n",
       "          -4.2021e-01, -2.1937e-01,  3.4184e-01,  1.9008e-01, -9.9543e-02,\n",
       "          -4.5229e-03,  1.9539e-01, -1.2138e-01,  3.8099e-02, -5.9830e-02,\n",
       "          -3.3543e-02, -3.1167e-01, -4.6679e-01, -1.2699e-01, -3.1893e-01],\n",
       "         [ 8.4404e-02, -5.3279e-02,  2.2280e-01,  1.8861e-01,  2.0043e-01,\n",
       "          -4.3934e-01, -1.8177e-01,  3.6823e-01,  1.9614e-01, -5.2494e-02,\n",
       "          -6.2610e-02,  1.7365e-01, -1.7273e-01,  2.0209e-02, -3.5270e-02,\n",
       "          -2.3227e-02, -3.7255e-01, -4.6647e-01, -1.8769e-01, -3.1573e-01],\n",
       "         [ 6.9724e-02, -7.3750e-02,  1.6550e-01,  8.7613e-02,  1.1718e-01,\n",
       "          -3.6412e-01, -1.7280e-01,  2.8400e-01,  1.8012e-01, -9.4916e-02,\n",
       "           2.1230e-02,  1.4275e-01, -9.5109e-02,  2.5415e-02, -4.3418e-02,\n",
       "          -3.8220e-02, -2.8582e-01, -3.9530e-01, -9.0972e-02, -2.7953e-01],\n",
       "         [ 9.5036e-02, -5.4373e-02,  2.6239e-01,  1.9142e-01,  2.0119e-01,\n",
       "          -4.9235e-01, -2.0890e-01,  4.0055e-01,  2.2451e-01, -8.0652e-02,\n",
       "          -6.0624e-02,  2.0271e-01, -1.5918e-01,  1.0379e-02, -4.3615e-02,\n",
       "          -2.7236e-02, -3.8743e-01, -5.1782e-01, -2.0538e-01, -3.4874e-01],\n",
       "         [ 8.4480e-02, -5.1547e-02,  2.2140e-01,  1.8946e-01,  1.9931e-01,\n",
       "          -4.3751e-01, -1.8089e-01,  3.6615e-01,  1.9453e-01, -5.1259e-02,\n",
       "          -6.5665e-02,  1.7506e-01, -1.7133e-01,  1.7878e-02, -3.4507e-02,\n",
       "          -2.1969e-02, -3.6936e-01, -4.6347e-01, -1.8991e-01, -3.1327e-01],\n",
       "         [ 8.6100e-02, -5.9401e-02,  2.4414e-01,  1.6335e-01,  1.8040e-01,\n",
       "          -4.4509e-01, -1.9267e-01,  3.7203e-01,  2.1587e-01, -9.3653e-02,\n",
       "          -5.1841e-02,  1.9464e-01, -1.3502e-01,  2.4814e-02, -5.8228e-02,\n",
       "          -3.3871e-02, -3.5329e-01, -4.9005e-01, -1.5476e-01, -3.1545e-01],\n",
       "         [ 8.5146e-02, -4.9552e-02,  2.2025e-01,  1.3449e-01,  1.7777e-01,\n",
       "          -4.6153e-01, -2.3427e-01,  3.6904e-01,  1.8618e-01, -8.9449e-02,\n",
       "          -1.5115e-02,  2.0601e-01, -1.4334e-01,  2.0955e-02, -4.3111e-02,\n",
       "          -2.5280e-02, -3.4328e-01, -4.8977e-01, -1.8169e-01, -3.4801e-01],\n",
       "         [ 8.7508e-02, -5.6100e-02,  2.1703e-01,  1.3086e-01,  1.7445e-01,\n",
       "          -4.5897e-01, -2.3219e-01,  3.6377e-01,  1.9108e-01, -8.7628e-02,\n",
       "          -1.4085e-02,  2.0277e-01, -1.4338e-01,  2.1063e-02, -4.3076e-02,\n",
       "          -2.5900e-02, -3.3958e-01, -4.8737e-01, -1.7719e-01, -3.4569e-01],\n",
       "         [ 8.5734e-02, -5.8314e-02,  2.4688e-01,  1.6328e-01,  1.7939e-01,\n",
       "          -4.4540e-01, -1.9194e-01,  3.7203e-01,  2.1689e-01, -9.2478e-02,\n",
       "          -5.3049e-02,  1.9429e-01, -1.3394e-01,  2.4231e-02, -5.7158e-02,\n",
       "          -3.3110e-02, -3.5355e-01, -4.8981e-01, -1.5579e-01, -3.1355e-01],\n",
       "         [ 9.6229e-02, -5.3063e-02,  2.6131e-01,  1.9708e-01,  2.0257e-01,\n",
       "          -4.9005e-01, -2.0595e-01,  3.9916e-01,  2.2006e-01, -7.8870e-02,\n",
       "          -6.2519e-02,  2.0024e-01, -1.6136e-01,  7.2534e-03, -3.9877e-02,\n",
       "          -2.5280e-02, -3.8916e-01, -5.1482e-01, -2.1110e-01, -3.4637e-01]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi head attention block applies multiple attention heads as can be seen in the paper \"Attention is all you need\", then concatenates the output and applies single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
