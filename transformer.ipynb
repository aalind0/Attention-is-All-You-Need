{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=[{cls.__name__}, {name}, {tsr.shape}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "\n",
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1)\n",
    "        assert q.size(-1) == d_k\n",
    "        \n",
    "        # Compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature))\n",
    "        \n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        \n",
    "        attn = torch.exp(attn)\n",
    "        \n",
    "        log_size(attn, \"attention weight\") # Batch, Seq, Seq\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5696, 0.5328, 0.5473, 0.5663, 0.6148, 0.5105, 0.3823, 0.5348,\n",
       "          0.7459, 0.5658, 0.5710, 0.4206, 0.4794, 0.5721, 0.5688, 0.5083,\n",
       "          0.4243, 0.5707, 0.6328, 0.5048],\n",
       "         [0.4731, 0.5220, 0.5115, 0.5091, 0.5406, 0.4984, 0.3474, 0.4973,\n",
       "          0.6399, 0.4524, 0.4859, 0.3546, 0.3890, 0.5669, 0.4984, 0.4367,\n",
       "          0.3912, 0.5467, 0.5609, 0.4470],\n",
       "         [0.5654, 0.5371, 0.5451, 0.5681, 0.6154, 0.5152, 0.3812, 0.5335,\n",
       "          0.7432, 0.5596, 0.5680, 0.4191, 0.4731, 0.5773, 0.5658, 0.5069,\n",
       "          0.4304, 0.5720, 0.6331, 0.4998],\n",
       "         [0.4588, 0.5145, 0.5530, 0.5379, 0.5367, 0.4244, 0.3039, 0.4629,\n",
       "          0.6540, 0.5616, 0.4676, 0.4100, 0.4545, 0.5671, 0.4985, 0.3931,\n",
       "          0.3959, 0.5330, 0.5193, 0.4889],\n",
       "         [0.5563, 0.5464, 0.5749, 0.5602, 0.6286, 0.5011, 0.3800, 0.5494,\n",
       "          0.7206, 0.5600, 0.5735, 0.4246, 0.4733, 0.5853, 0.5847, 0.4873,\n",
       "          0.4274, 0.5876, 0.6218, 0.5155],\n",
       "         [0.5564, 0.5539, 0.5706, 0.5649, 0.6224, 0.5084, 0.3812, 0.5501,\n",
       "          0.7260, 0.5533, 0.5731, 0.4201, 0.4708, 0.5930, 0.5797, 0.4937,\n",
       "          0.4306, 0.5898, 0.6283, 0.5120],\n",
       "         [0.4774, 0.4208, 0.5548, 0.4293, 0.5591, 0.4894, 0.3687, 0.5060,\n",
       "          0.6176, 0.5489, 0.4443, 0.3632, 0.4686, 0.4616, 0.5600, 0.3601,\n",
       "          0.2968, 0.5404, 0.5100, 0.4988],\n",
       "         [0.4004, 0.5261, 0.5536, 0.5116, 0.4921, 0.3173, 0.3051, 0.4455,\n",
       "          0.6076, 0.4389, 0.4784, 0.3831, 0.3418, 0.5626, 0.4645, 0.3760,\n",
       "          0.3602, 0.4571, 0.4352, 0.4737],\n",
       "         [0.5687, 0.5631, 0.5746, 0.5598, 0.6289, 0.4939, 0.3845, 0.5494,\n",
       "          0.7224, 0.5405, 0.5979, 0.4255, 0.4452, 0.5985, 0.5882, 0.5081,\n",
       "          0.4299, 0.5878, 0.6279, 0.5095],\n",
       "         [0.4631, 0.3401, 0.3182, 0.5075, 0.4399, 0.3936, 0.2092, 0.3859,\n",
       "          0.6119, 0.4908, 0.4312, 0.3517, 0.4455, 0.4109, 0.3457, 0.4564,\n",
       "          0.3660, 0.4204, 0.5470, 0.3770]],\n",
       "\n",
       "        [[0.6355, 0.4361, 0.5234, 0.5781, 0.4715, 0.4693, 0.5354, 0.4883,\n",
       "          0.4469, 0.6417, 0.7372, 0.4223, 0.2864, 0.4299, 0.4383, 0.4754,\n",
       "          0.4878, 0.5743, 0.4212, 0.5894],\n",
       "         [0.7195, 0.5297, 0.5592, 0.5852, 0.5261, 0.5705, 0.5459, 0.5424,\n",
       "          0.5693, 0.6827, 0.7973, 0.4619, 0.2981, 0.5049, 0.5077, 0.5788,\n",
       "          0.5218, 0.6191, 0.4803, 0.6697],\n",
       "         [0.7055, 0.5425, 0.5576, 0.5822, 0.5321, 0.5617, 0.5569, 0.5299,\n",
       "          0.5766, 0.6753, 0.7795, 0.4569, 0.2862, 0.5149, 0.5075, 0.5685,\n",
       "          0.5192, 0.6248, 0.4892, 0.6657],\n",
       "         [0.7121, 0.5318, 0.5554, 0.5972, 0.5155, 0.5686, 0.5667, 0.5580,\n",
       "          0.5577, 0.6971, 0.7988, 0.4503, 0.3048, 0.5006, 0.4969, 0.5827,\n",
       "          0.5362, 0.6228, 0.4973, 0.6492],\n",
       "         [0.5497, 0.4297, 0.4446, 0.4765, 0.3790, 0.4478, 0.4755, 0.3949,\n",
       "          0.4512, 0.5625, 0.6077, 0.4102, 0.2628, 0.3894, 0.3526, 0.5176,\n",
       "          0.4226, 0.4409, 0.3654, 0.4788],\n",
       "         [0.7083, 0.5358, 0.5571, 0.5966, 0.5192, 0.5628, 0.5781, 0.5498,\n",
       "          0.5595, 0.6907, 0.7943, 0.4504, 0.3051, 0.5003, 0.5008, 0.5687,\n",
       "          0.5332, 0.6280, 0.4960, 0.6483],\n",
       "         [0.7011, 0.5367, 0.5583, 0.5878, 0.5391, 0.5589, 0.5645, 0.5378,\n",
       "          0.5707, 0.6868, 0.7773, 0.4616, 0.2805, 0.5191, 0.4937, 0.5705,\n",
       "          0.5327, 0.6237, 0.4985, 0.6660],\n",
       "         [0.7107, 0.5311, 0.5521, 0.5905, 0.5370, 0.5712, 0.5477, 0.5214,\n",
       "          0.5694, 0.6692, 0.7754, 0.4661, 0.2843, 0.5170, 0.5042, 0.5775,\n",
       "          0.5229, 0.6134, 0.4893, 0.6729],\n",
       "         [0.6580, 0.5109, 0.4739, 0.5090, 0.5150, 0.5314, 0.4725, 0.4640,\n",
       "          0.5228, 0.6138, 0.6715, 0.3767, 0.2254, 0.4964, 0.4008, 0.5297,\n",
       "          0.5183, 0.5278, 0.4848, 0.5867],\n",
       "         [0.7125, 0.5265, 0.5518, 0.5922, 0.5163, 0.5778, 0.5557, 0.5559,\n",
       "          0.5621, 0.6914, 0.7950, 0.4558, 0.3039, 0.5025, 0.4988, 0.5910,\n",
       "          0.5336, 0.6167, 0.4958, 0.6540]],\n",
       "\n",
       "        [[0.3218, 0.3952, 0.4698, 0.3694, 0.5568, 0.3273, 0.5266, 0.3425,\n",
       "          0.2980, 0.3367, 0.2608, 0.2813, 0.2942, 0.4645, 0.3284, 0.4013,\n",
       "          0.3565, 0.4934, 0.4535, 0.5099],\n",
       "         [0.3886, 0.5366, 0.7036, 0.5712, 0.7240, 0.4871, 0.6640, 0.5246,\n",
       "          0.5263, 0.4596, 0.4438, 0.3902, 0.4190, 0.6499, 0.5682, 0.5410,\n",
       "          0.5467, 0.6551, 0.6379, 0.6571],\n",
       "         [0.4041, 0.4818, 0.6322, 0.5173, 0.6686, 0.4413, 0.6096, 0.4692,\n",
       "          0.4833, 0.4459, 0.3899, 0.3854, 0.3586, 0.5950, 0.4639, 0.4954,\n",
       "          0.4569, 0.6584, 0.5569, 0.5757],\n",
       "         [0.3944, 0.5367, 0.7040, 0.5698, 0.7228, 0.4825, 0.6729, 0.5162,\n",
       "          0.5201, 0.4641, 0.4363, 0.3949, 0.4116, 0.6491, 0.5575, 0.5431,\n",
       "          0.5475, 0.6678, 0.6298, 0.6572],\n",
       "         [0.4143, 0.5343, 0.7057, 0.5687, 0.7339, 0.4780, 0.6749, 0.5083,\n",
       "          0.5176, 0.4622, 0.4435, 0.3999, 0.4106, 0.6453, 0.5534, 0.5345,\n",
       "          0.5432, 0.6687, 0.6317, 0.6562],\n",
       "         [0.4108, 0.5311, 0.7089, 0.5746, 0.7340, 0.4787, 0.6714, 0.5064,\n",
       "          0.5222, 0.4644, 0.4536, 0.4020, 0.4044, 0.6509, 0.5557, 0.5290,\n",
       "          0.5408, 0.6651, 0.6288, 0.6462],\n",
       "         [0.3890, 0.5457, 0.7016, 0.5732, 0.7200, 0.4907, 0.6713, 0.5103,\n",
       "          0.5187, 0.4540, 0.4514, 0.3920, 0.4152, 0.6534, 0.5680, 0.5316,\n",
       "          0.5400, 0.6517, 0.6240, 0.6410],\n",
       "         [0.3862, 0.5377, 0.7117, 0.5714, 0.7249, 0.4851, 0.6613, 0.5248,\n",
       "          0.5283, 0.4635, 0.4487, 0.3915, 0.4115, 0.6547, 0.5721, 0.5333,\n",
       "          0.5465, 0.6530, 0.6306, 0.6411],\n",
       "         [0.3929, 0.5208, 0.5813, 0.5225, 0.6016, 0.4670, 0.5905, 0.3910,\n",
       "          0.4389, 0.3880, 0.4108, 0.3713, 0.3640, 0.5989, 0.5274, 0.4462,\n",
       "          0.4118, 0.5195, 0.5381, 0.5019],\n",
       "         [0.2705, 0.3662, 0.5440, 0.5079, 0.5985, 0.3518, 0.4898, 0.4092,\n",
       "          0.4718, 0.3470, 0.4159, 0.2191, 0.3497, 0.5287, 0.4379, 0.4051,\n",
       "          0.4940, 0.5267, 0.5810, 0.5536]],\n",
       "\n",
       "        [[0.4413, 0.3759, 0.6468, 0.5408, 0.4103, 0.6449, 0.3159, 0.4682,\n",
       "          0.4910, 0.6402, 0.5601, 0.5727, 0.4720, 0.4935, 0.5860, 0.4959,\n",
       "          0.5630, 0.6315, 0.5857, 0.6161],\n",
       "         [0.4851, 0.3801, 0.6592, 0.4836, 0.3932, 0.6257, 0.3512, 0.4579,\n",
       "          0.4796, 0.6178, 0.5653, 0.5486, 0.4027, 0.5201, 0.6189, 0.4884,\n",
       "          0.4965, 0.6003, 0.6327, 0.5348],\n",
       "         [0.3481, 0.2654, 0.4705, 0.2162, 0.2349, 0.4490, 0.2726, 0.3079,\n",
       "          0.2412, 0.4650, 0.3733, 0.4169, 0.3616, 0.3719, 0.4201, 0.3913,\n",
       "          0.3834, 0.4412, 0.3700, 0.3832],\n",
       "         [0.4484, 0.4007, 0.5815, 0.4610, 0.4383, 0.6644, 0.3175, 0.4968,\n",
       "          0.4232, 0.6895, 0.5937, 0.6227, 0.4809, 0.5622, 0.6180, 0.5105,\n",
       "          0.5091, 0.6436, 0.5888, 0.6205],\n",
       "         [0.4173, 0.3357, 0.4744, 0.4432, 0.3144, 0.5880, 0.3053, 0.5047,\n",
       "          0.3935, 0.6046, 0.5413, 0.5151, 0.4458, 0.4165, 0.5035, 0.5188,\n",
       "          0.4892, 0.5146, 0.5234, 0.5061],\n",
       "         [0.3851, 0.3650, 0.4856, 0.4237, 0.4257, 0.5706, 0.2584, 0.4244,\n",
       "          0.3824, 0.5911, 0.5336, 0.5249, 0.4068, 0.5330, 0.5785, 0.4305,\n",
       "          0.4183, 0.5633, 0.5403, 0.5420],\n",
       "         [0.3511, 0.2833, 0.3830, 0.3699, 0.3195, 0.5537, 0.2475, 0.4117,\n",
       "          0.3188, 0.6553, 0.4398, 0.5752, 0.4073, 0.4636, 0.4779, 0.4503,\n",
       "          0.3440, 0.4744, 0.4925, 0.5318],\n",
       "         [0.5447, 0.4187, 0.6365, 0.5303, 0.4318, 0.7212, 0.3579, 0.5596,\n",
       "          0.5186, 0.7303, 0.6456, 0.6579, 0.4920, 0.5820, 0.6438, 0.5595,\n",
       "          0.5595, 0.6736, 0.6928, 0.6430],\n",
       "         [0.5321, 0.3323, 0.5200, 0.4255, 0.3436, 0.6300, 0.3241, 0.5103,\n",
       "          0.4604, 0.6670, 0.5499, 0.5927, 0.4150, 0.5361, 0.5409, 0.5119,\n",
       "          0.4770, 0.5719, 0.6673, 0.5612],\n",
       "         [0.4363, 0.3832, 0.5449, 0.3691, 0.3556, 0.6233, 0.3035, 0.4425,\n",
       "          0.3604, 0.6137, 0.5320, 0.5893, 0.4526, 0.4872, 0.5277, 0.4471,\n",
       "          0.4660, 0.6164, 0.5000, 0.5547]],\n",
       "\n",
       "        [[0.5841, 0.5196, 0.5664, 0.6001, 0.5661, 0.3378, 0.6111, 0.6186,\n",
       "          0.4713, 0.6627, 0.6245, 0.3695, 0.7289, 0.4351, 0.4635, 0.6792,\n",
       "          0.5498, 0.6150, 0.4574, 0.5998],\n",
       "         [0.4994, 0.4776, 0.4712, 0.4934, 0.4659, 0.3261, 0.5782, 0.5988,\n",
       "          0.3969, 0.6098, 0.5885, 0.3217, 0.6363, 0.3666, 0.3346, 0.6106,\n",
       "          0.4590, 0.5998, 0.4308, 0.5342],\n",
       "         [0.4289, 0.4437, 0.5207, 0.4358, 0.5294, 0.2264, 0.4668, 0.5168,\n",
       "          0.3913, 0.5362, 0.4298, 0.3255, 0.5871, 0.3483, 0.4157, 0.6197,\n",
       "          0.4562, 0.5059, 0.3914, 0.4586],\n",
       "         [0.6029, 0.5105, 0.5748, 0.6123, 0.5641, 0.3558, 0.5892, 0.6090,\n",
       "          0.4855, 0.6444, 0.6232, 0.3667, 0.7398, 0.4396, 0.4496, 0.6680,\n",
       "          0.5540, 0.6242, 0.4548, 0.6062],\n",
       "         [0.5954, 0.5274, 0.5668, 0.6231, 0.5588, 0.3540, 0.5956, 0.5996,\n",
       "          0.4829, 0.6459, 0.6148, 0.3553, 0.7518, 0.4296, 0.4492, 0.6579,\n",
       "          0.5608, 0.5958, 0.4503, 0.5984],\n",
       "         [0.4216, 0.4437, 0.5174, 0.4329, 0.5266, 0.2248, 0.4723, 0.5191,\n",
       "          0.3911, 0.5343, 0.4348, 0.3230, 0.5760, 0.3509, 0.4244, 0.6163,\n",
       "          0.4487, 0.5063, 0.3768, 0.4553],\n",
       "         [0.5130, 0.4947, 0.5368, 0.5570, 0.4928, 0.3049, 0.4855, 0.5305,\n",
       "          0.4487, 0.5382, 0.5486, 0.2683, 0.6951, 0.4021, 0.3982, 0.5622,\n",
       "          0.5324, 0.5022, 0.3422, 0.5255],\n",
       "         [0.5678, 0.4673, 0.5698, 0.5791, 0.5123, 0.3353, 0.5047, 0.5047,\n",
       "          0.4370, 0.5664, 0.5370, 0.3261, 0.7509, 0.4310, 0.3555, 0.5864,\n",
       "          0.5132, 0.5309, 0.4230, 0.4734],\n",
       "         [0.3953, 0.3735, 0.4237, 0.3994, 0.3266, 0.2595, 0.3908, 0.4639,\n",
       "          0.3191, 0.3997, 0.4729, 0.2054, 0.4864, 0.3205, 0.2758, 0.4074,\n",
       "          0.3727, 0.4831, 0.2286, 0.4288],\n",
       "         [0.5745, 0.4143, 0.4829, 0.4980, 0.4601, 0.3269, 0.4676, 0.4598,\n",
       "          0.4147, 0.5405, 0.4672, 0.3022, 0.6580, 0.3509, 0.3019, 0.5272,\n",
       "          0.4469, 0.4725, 0.4150, 0.4602]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        # We assume that the queries, keys, features all have the same feature size.\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries)\n",
    "        K = self.key_tfm(keys)\n",
    "        V = self.value_tfm(values)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        \n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3415e-01,  4.1547e-01, -1.2958e-01, -1.6530e-01, -1.4366e-02,\n",
       "           1.2347e-01,  2.8432e-01,  5.0719e-02, -4.1539e-01,  3.5242e-01,\n",
       "           1.8646e-01, -2.7570e-01, -1.8071e-01, -2.6601e-01, -2.8802e-02,\n",
       "           1.5071e-01, -1.0538e-01,  3.4945e-01,  2.2730e-01, -2.0590e-02],\n",
       "         [-1.3463e-01,  4.2426e-01, -1.3946e-01, -8.6122e-02, -4.6897e-02,\n",
       "           1.1551e-01,  3.0901e-01,  1.4901e-01, -5.6183e-01,  3.9037e-01,\n",
       "           1.9426e-01, -2.8315e-01, -1.7956e-01, -2.9039e-01, -5.0954e-02,\n",
       "           9.6262e-02, -2.1555e-01,  4.1239e-01,  3.3244e-01, -5.3709e-02],\n",
       "         [-1.5827e-01,  4.7342e-01, -1.1925e-01, -1.6868e-01,  4.4889e-02,\n",
       "           1.7229e-01,  3.0185e-01,  7.8513e-02, -4.6781e-01,  4.1151e-01,\n",
       "           2.4249e-01, -3.0184e-01, -1.3892e-01, -2.4401e-01, -1.5441e-02,\n",
       "           1.3222e-01, -9.2620e-02,  3.5561e-01,  3.1089e-01, -1.1497e-02],\n",
       "         [-1.3133e-01,  4.3470e-01, -1.4057e-01, -1.0172e-01, -2.9635e-02,\n",
       "           1.3707e-01,  3.1836e-01,  1.5173e-01, -5.4371e-01,  4.2051e-01,\n",
       "           1.9566e-01, -2.9996e-01, -1.4122e-01, -2.7667e-01, -4.9943e-02,\n",
       "           9.3426e-02, -1.8474e-01,  3.8430e-01,  3.2941e-01, -3.5292e-02],\n",
       "         [-1.2315e-01,  4.6938e-01, -9.5143e-02, -1.8614e-01,  1.1696e-02,\n",
       "           1.7434e-01,  2.9830e-01,  6.4071e-02, -4.6398e-01,  4.1003e-01,\n",
       "           2.5760e-01, -3.0629e-01, -1.7445e-01, -3.2265e-01, -2.5070e-02,\n",
       "           1.5293e-01, -9.8640e-02,  4.0290e-01,  2.6428e-01, -4.9161e-02],\n",
       "         [-1.7631e-01,  5.7256e-01, -1.6703e-01, -1.5684e-01, -1.9473e-02,\n",
       "           1.8015e-01,  4.0803e-01,  1.3684e-01, -6.3967e-01,  5.0489e-01,\n",
       "           2.5198e-01, -3.6124e-01, -1.9352e-01, -3.3729e-01, -2.2223e-02,\n",
       "           1.4771e-01, -1.9108e-01,  4.6774e-01,  3.8877e-01, -5.7798e-02],\n",
       "         [-1.3011e-01,  4.7892e-01, -1.4254e-01, -1.3962e-01, -5.5818e-02,\n",
       "           1.2292e-01,  3.2950e-01,  1.0994e-01, -5.1874e-01,  3.8406e-01,\n",
       "           2.0205e-01, -2.9801e-01, -1.7750e-01, -2.9375e-01, -5.6552e-02,\n",
       "           1.1813e-01, -1.8343e-01,  3.6189e-01,  2.9367e-01, -5.5202e-02],\n",
       "         [-1.8548e-01,  5.1639e-01, -1.9935e-01, -1.3381e-01, -4.1899e-02,\n",
       "           1.3027e-01,  3.9617e-01,  1.2378e-01, -5.8936e-01,  4.4850e-01,\n",
       "           1.8427e-01, -3.3197e-01, -1.9805e-01, -2.8323e-01, -2.4291e-02,\n",
       "           1.4473e-01, -1.9393e-01,  4.1829e-01,  3.5060e-01, -3.1023e-02],\n",
       "         [-1.6727e-01,  4.6288e-01, -1.4145e-01, -9.7448e-02, -3.6804e-02,\n",
       "           1.4677e-01,  2.9016e-01,  1.4257e-01, -5.6362e-01,  4.0524e-01,\n",
       "           1.7714e-01, -2.5761e-01, -1.5521e-01, -2.3379e-01, -1.3816e-02,\n",
       "           1.0059e-01, -2.0709e-01,  3.5475e-01,  3.6486e-01, -5.4227e-02],\n",
       "         [-1.7597e-01,  5.7442e-01, -1.6354e-01, -1.5909e-01, -1.9398e-02,\n",
       "           1.8363e-01,  4.0276e-01,  1.3780e-01, -6.3843e-01,  5.0643e-01,\n",
       "           2.5620e-01, -3.6063e-01, -1.9311e-01, -3.4045e-01, -2.3164e-02,\n",
       "           1.4739e-01, -1.8993e-01,  4.6731e-01,  3.8811e-01, -6.0213e-02]],\n",
       "\n",
       "        [[-1.4951e-01,  3.2268e-01, -1.3705e-01, -2.1765e-01,  2.1313e-02,\n",
       "           1.7833e-01,  2.3935e-01,  6.3927e-02, -3.1374e-01,  2.7710e-01,\n",
       "           1.3733e-01, -1.8277e-01, -7.9423e-02, -2.4340e-01,  5.7547e-03,\n",
       "           9.6149e-02, -6.8285e-02,  2.8854e-01,  2.2768e-01,  8.1661e-02],\n",
       "         [-1.4801e-01,  5.0305e-01, -1.7240e-01, -2.8940e-01, -1.2562e-02,\n",
       "           2.3496e-01,  3.4723e-01,  9.1232e-02, -4.8737e-01,  4.3200e-01,\n",
       "           2.0821e-01, -3.0142e-01, -1.3576e-01, -3.6382e-01, -4.2454e-02,\n",
       "           1.5978e-01, -1.4565e-01,  4.2096e-01,  3.2194e-01,  7.5160e-02],\n",
       "         [-7.9121e-02,  4.0465e-01, -1.4747e-01, -1.9902e-01, -1.1386e-04,\n",
       "           1.8057e-01,  2.4693e-01,  6.3126e-02, -3.7454e-01,  3.6480e-01,\n",
       "           1.8326e-01, -2.6271e-01, -8.5270e-02, -2.5946e-01, -6.0261e-02,\n",
       "           1.4294e-01, -1.0890e-01,  3.5002e-01,  2.4251e-01,  5.1576e-02],\n",
       "         [-1.4497e-01,  4.9649e-01, -1.9810e-01, -2.4272e-01,  2.3916e-02,\n",
       "           1.6967e-01,  3.1897e-01,  9.5655e-02, -4.2252e-01,  4.0708e-01,\n",
       "           1.6729e-01, -3.2081e-01, -1.2654e-01, -3.1546e-01, -5.2021e-02,\n",
       "           1.5705e-01, -1.3654e-01,  4.1817e-01,  3.1196e-01,  8.1742e-02],\n",
       "         [-1.6270e-01,  5.0370e-01, -1.9087e-01, -2.7543e-01,  2.5694e-02,\n",
       "           2.4028e-01,  3.3379e-01,  1.0836e-01, -5.1106e-01,  4.5573e-01,\n",
       "           1.8291e-01, -3.2197e-01, -1.2861e-01, -3.5877e-01, -2.8589e-02,\n",
       "           1.2468e-01, -1.6434e-01,  4.5074e-01,  3.6100e-01,  1.1939e-01],\n",
       "         [-1.6321e-01,  5.5784e-01, -2.1755e-01, -3.0121e-01,  1.4562e-02,\n",
       "           2.3085e-01,  3.9867e-01,  1.0801e-01, -5.5211e-01,  5.0500e-01,\n",
       "           2.0701e-01, -3.6056e-01, -1.4914e-01, -4.0587e-01, -3.7740e-02,\n",
       "           1.7084e-01, -1.6907e-01,  5.1224e-01,  3.7827e-01,  1.2865e-01],\n",
       "         [-1.7502e-01,  5.6718e-01, -2.2398e-01, -3.0641e-01,  2.1978e-02,\n",
       "           2.4136e-01,  3.9056e-01,  1.1217e-01, -5.5256e-01,  5.1037e-01,\n",
       "           2.0485e-01, -3.6636e-01, -1.4556e-01, -4.0156e-01, -3.5132e-02,\n",
       "           1.6582e-01, -1.6853e-01,  5.1065e-01,  3.8579e-01,  1.3053e-01],\n",
       "         [-1.0445e-01,  4.8513e-01, -1.6503e-01, -2.7668e-01, -1.2778e-02,\n",
       "           1.8197e-01,  3.7398e-01,  1.0418e-01, -4.9993e-01,  4.6264e-01,\n",
       "           1.6769e-01, -3.3551e-01, -1.5303e-01, -4.0697e-01, -3.1700e-02,\n",
       "           1.5067e-01, -1.6816e-01,  4.7993e-01,  3.2360e-01,  1.1045e-01],\n",
       "         [-1.6833e-01,  5.6249e-01, -2.1890e-01, -3.0458e-01,  1.5674e-02,\n",
       "           2.3543e-01,  3.9636e-01,  1.0945e-01, -5.5318e-01,  5.0625e-01,\n",
       "           2.0529e-01, -3.6194e-01, -1.4866e-01, -4.0517e-01, -3.6750e-02,\n",
       "           1.6906e-01, -1.6996e-01,  5.0948e-01,  3.8120e-01,  1.2817e-01],\n",
       "         [-1.2579e-01,  4.6211e-01, -1.6086e-01, -2.4709e-01, -7.8396e-03,\n",
       "           2.1133e-01,  3.1363e-01,  9.3798e-02, -4.2460e-01,  4.0335e-01,\n",
       "           1.9863e-01, -2.9515e-01, -1.1918e-01, -3.2034e-01, -4.9705e-02,\n",
       "           1.4726e-01, -1.2698e-01,  3.9124e-01,  2.8610e-01,  5.5449e-02]],\n",
       "\n",
       "        [[-1.2728e-01,  5.7469e-01, -2.0733e-01, -2.1373e-01, -2.7919e-02,\n",
       "           2.3501e-01,  3.4010e-01,  6.7491e-03, -5.6767e-01,  4.7541e-01,\n",
       "           2.6369e-01, -3.0170e-01, -2.1574e-01, -3.0553e-01, -2.7098e-02,\n",
       "           1.8870e-01, -1.7332e-01,  4.6396e-01,  3.2829e-01,  5.1680e-02],\n",
       "         [-1.2828e-01,  5.7385e-01, -2.0688e-01, -2.1284e-01, -2.2619e-02,\n",
       "           2.3707e-01,  3.4632e-01,  8.3165e-03, -5.6479e-01,  4.7607e-01,\n",
       "           2.6261e-01, -2.9916e-01, -2.0948e-01, -3.0495e-01, -2.7800e-02,\n",
       "           1.9193e-01, -1.7191e-01,  4.6656e-01,  3.3118e-01,  5.8174e-02],\n",
       "         [-1.2923e-01,  5.7684e-01, -2.0533e-01, -2.1341e-01, -3.0291e-02,\n",
       "           2.3651e-01,  3.4125e-01,  8.3444e-03, -5.6461e-01,  4.7354e-01,\n",
       "           2.6666e-01, -2.9980e-01, -2.1686e-01, -3.0294e-01, -2.7974e-02,\n",
       "           1.8857e-01, -1.7100e-01,  4.6050e-01,  3.2966e-01,  4.4987e-02],\n",
       "         [-1.0057e-01,  5.0762e-01, -1.9756e-01, -1.7920e-01, -2.6425e-02,\n",
       "           2.0087e-01,  3.2267e-01,  1.7598e-02, -5.2040e-01,  4.2780e-01,\n",
       "           2.3008e-01, -2.8426e-01, -2.0476e-01, -2.8221e-01, -3.1134e-02,\n",
       "           1.4815e-01, -1.6525e-01,  4.6894e-01,  3.0048e-01,  5.7222e-02],\n",
       "         [-9.4099e-02,  4.4658e-01, -1.3781e-01, -1.8712e-01,  1.7479e-02,\n",
       "           1.9642e-01,  3.3917e-01,  2.6655e-03, -4.3744e-01,  3.7479e-01,\n",
       "           2.0886e-01, -2.6547e-01, -1.5478e-01, -2.6822e-01, -3.8563e-02,\n",
       "           1.7043e-01, -1.2388e-01,  3.9110e-01,  2.6141e-01,  6.6846e-02],\n",
       "         [-1.2684e-01,  5.2392e-01, -1.6995e-01, -1.9890e-01, -3.3898e-02,\n",
       "           2.1867e-01,  2.8491e-01, -6.0184e-03, -5.3050e-01,  4.2046e-01,\n",
       "           2.5614e-01, -2.7786e-01, -2.0106e-01, -2.8041e-01, -3.4628e-02,\n",
       "           1.7148e-01, -1.5817e-01,  3.7256e-01,  2.8312e-01,  1.4218e-02],\n",
       "         [-1.2771e-01,  5.7579e-01, -2.0586e-01, -2.1340e-01, -2.6533e-02,\n",
       "           2.3656e-01,  3.4474e-01,  6.9412e-03, -5.6653e-01,  4.7442e-01,\n",
       "           2.6484e-01, -3.0180e-01, -2.1428e-01, -3.0539e-01, -2.9030e-02,\n",
       "           1.8997e-01, -1.7229e-01,  4.6464e-01,  3.2923e-01,  5.0868e-02],\n",
       "         [-1.3017e-01,  5.7511e-01, -2.0421e-01, -2.1264e-01, -2.2862e-02,\n",
       "           2.4014e-01,  3.4574e-01,  7.3401e-03, -5.6363e-01,  4.7401e-01,\n",
       "           2.6628e-01, -2.9711e-01, -2.0797e-01, -3.0377e-01, -2.9544e-02,\n",
       "           1.9317e-01, -1.7058e-01,  4.6075e-01,  3.3083e-01,  5.4022e-02],\n",
       "         [-1.2208e-01,  4.7411e-01, -1.4295e-01, -2.0434e-01, -5.0156e-02,\n",
       "           1.8530e-01,  2.4947e-01, -4.1756e-03, -4.6908e-01,  3.7529e-01,\n",
       "           2.1694e-01, -2.4168e-01, -1.9899e-01, -2.4760e-01, -1.8569e-02,\n",
       "           1.4997e-01, -1.3255e-01,  3.0033e-01,  2.5555e-01,  1.2035e-03],\n",
       "         [-1.3025e-01,  5.7777e-01, -2.0937e-01, -2.1318e-01, -3.0814e-02,\n",
       "           2.3370e-01,  3.3510e-01,  5.3827e-03, -5.6702e-01,  4.7636e-01,\n",
       "           2.6298e-01, -2.9948e-01, -2.1795e-01, -3.0234e-01, -2.2724e-02,\n",
       "           1.8882e-01, -1.7282e-01,  4.5822e-01,  3.2718e-01,  4.8652e-02]],\n",
       "\n",
       "        [[-2.5695e-01,  5.6378e-01, -2.4319e-01, -1.6588e-01,  6.4646e-02,\n",
       "           2.1587e-01,  3.8480e-01,  1.5877e-01, -5.6580e-01,  5.2648e-01,\n",
       "           2.1702e-01, -4.2622e-01, -1.4540e-01, -3.1775e-01, -6.6494e-02,\n",
       "           6.2856e-02, -1.5639e-01,  4.1443e-01,  3.5458e-01,  6.7513e-02],\n",
       "         [-1.8285e-01,  4.8579e-01, -2.2434e-01, -1.2201e-01, -1.3751e-03,\n",
       "           1.8719e-01,  3.4869e-01,  1.1016e-01, -5.1231e-01,  4.6715e-01,\n",
       "           1.8375e-01, -3.7118e-01, -1.4749e-01, -2.6765e-01, -4.9210e-02,\n",
       "           4.5651e-02, -1.6546e-01,  3.4341e-01,  2.8553e-01,  6.6564e-02],\n",
       "         [-2.6750e-01,  6.2079e-01, -2.7578e-01, -1.7186e-01,  2.7001e-02,\n",
       "           2.5318e-01,  4.2067e-01,  1.6243e-01, -6.1915e-01,  5.8157e-01,\n",
       "           2.3916e-01, -4.4762e-01, -1.7617e-01, -3.3317e-01, -5.5841e-02,\n",
       "           5.6251e-02, -1.8316e-01,  4.3947e-01,  3.8438e-01,  7.3303e-02],\n",
       "         [-2.6686e-01,  6.1999e-01, -2.7632e-01, -1.7062e-01,  2.6429e-02,\n",
       "           2.5184e-01,  4.1791e-01,  1.6423e-01, -6.2125e-01,  5.8019e-01,\n",
       "           2.3881e-01, -4.4971e-01, -1.7647e-01, -3.3479e-01, -5.7131e-02,\n",
       "           5.3701e-02, -1.8649e-01,  4.4054e-01,  3.8428e-01,  7.2044e-02],\n",
       "         [-2.3084e-01,  5.4840e-01, -2.7300e-01, -1.3413e-01,  6.8726e-03,\n",
       "           2.2485e-01,  3.3806e-01,  1.7008e-01, -5.6855e-01,  5.0900e-01,\n",
       "           2.0322e-01, -4.0581e-01, -1.5960e-01, -3.0788e-01, -8.4168e-02,\n",
       "           1.1134e-02, -2.0758e-01,  4.1759e-01,  3.4766e-01,  7.5331e-02],\n",
       "         [-1.8228e-01,  4.8507e-01, -2.2381e-01, -1.2171e-01, -1.4790e-03,\n",
       "           1.8749e-01,  3.4898e-01,  1.0890e-01, -5.1078e-01,  4.6652e-01,\n",
       "           1.8374e-01, -3.6945e-01, -1.4713e-01, -2.6630e-01, -4.9012e-02,\n",
       "           4.6552e-02, -1.6428e-01,  3.4280e-01,  2.8479e-01,  6.6854e-02],\n",
       "         [-2.5568e-01,  5.6192e-01, -2.4177e-01, -1.6449e-01,  6.4530e-02,\n",
       "           2.1589e-01,  3.8818e-01,  1.5655e-01, -5.6491e-01,  5.2796e-01,\n",
       "           2.1561e-01, -4.2274e-01, -1.4417e-01, -3.1552e-01, -6.6212e-02,\n",
       "           6.5538e-02, -1.5367e-01,  4.1345e-01,  3.5390e-01,  6.9125e-02],\n",
       "         [-1.8928e-01,  4.6441e-01, -2.5390e-01, -8.1336e-02,  9.3670e-03,\n",
       "           1.7402e-01,  3.1481e-01,  1.5631e-01, -5.2404e-01,  4.7191e-01,\n",
       "           1.7851e-01, -3.7482e-01, -1.2794e-01, -2.6920e-01, -4.5920e-02,\n",
       "           4.5979e-03, -1.8669e-01,  3.6615e-01,  3.1112e-01,  7.0222e-02],\n",
       "         [-2.6802e-01,  6.1834e-01, -2.7722e-01, -1.6950e-01,  2.8472e-02,\n",
       "           2.5078e-01,  4.1607e-01,  1.6661e-01, -6.2161e-01,  5.7944e-01,\n",
       "           2.4010e-01, -4.5019e-01, -1.7552e-01, -3.3500e-01, -5.6202e-02,\n",
       "           5.2571e-02, -1.8656e-01,  4.4141e-01,  3.8557e-01,  7.1413e-02],\n",
       "         [-2.6649e-01,  5.6986e-01, -2.5248e-01, -1.7037e-01,  5.8292e-02,\n",
       "           2.4394e-01,  3.8069e-01,  1.4584e-01, -5.3455e-01,  5.1922e-01,\n",
       "           2.4354e-01, -3.8841e-01, -1.4858e-01, -2.8853e-01, -4.8865e-02,\n",
       "           7.2567e-02, -1.3245e-01,  4.0814e-01,  3.5917e-01,  7.0471e-02]],\n",
       "\n",
       "        [[-1.7292e-01,  6.0585e-01, -1.6225e-01, -2.8086e-01, -1.2402e-01,\n",
       "           2.4669e-01,  4.3951e-01,  1.5348e-01, -5.4935e-01,  5.0636e-01,\n",
       "           1.1330e-01, -3.0209e-01, -2.2338e-01, -3.9579e-01,  1.1926e-01,\n",
       "           1.2004e-01, -2.0615e-01,  4.4555e-01,  4.0461e-01, -3.3973e-02],\n",
       "         [-1.8143e-01,  5.5251e-01, -1.6253e-01, -2.6771e-01, -1.1066e-01,\n",
       "           2.4902e-01,  4.0070e-01,  1.2331e-01, -4.8165e-01,  4.5727e-01,\n",
       "           9.8748e-02, -2.7564e-01, -1.8486e-01, -3.5520e-01,  8.7235e-02,\n",
       "           1.1965e-01, -1.7059e-01,  3.8584e-01,  3.5444e-01, -1.0308e-02],\n",
       "         [-1.1653e-01,  5.5230e-01, -1.2883e-01, -2.7117e-01, -1.3505e-01,\n",
       "           2.1541e-01,  4.1636e-01,  1.2424e-01, -4.9173e-01,  4.3946e-01,\n",
       "           1.0015e-01, -2.8735e-01, -2.3331e-01, -3.8009e-01,  1.1269e-01,\n",
       "           9.6983e-02, -1.9460e-01,  4.2121e-01,  3.4721e-01, -5.0731e-02],\n",
       "         [-1.7284e-01,  6.0819e-01, -1.6217e-01, -2.8162e-01, -1.2436e-01,\n",
       "           2.4998e-01,  4.3984e-01,  1.5046e-01, -5.4954e-01,  5.0502e-01,\n",
       "           1.1318e-01, -3.0228e-01, -2.2312e-01, -3.9750e-01,  1.2001e-01,\n",
       "           1.2004e-01, -2.0649e-01,  4.4390e-01,  4.0216e-01, -3.6639e-02],\n",
       "         [-1.4215e-01,  5.4547e-01, -1.4127e-01, -2.5411e-01, -1.2629e-01,\n",
       "           2.0489e-01,  3.9391e-01,  1.3245e-01, -5.2460e-01,  4.3605e-01,\n",
       "           1.0260e-01, -2.7844e-01, -2.0883e-01, -3.7105e-01,  8.4230e-02,\n",
       "           1.2678e-01, -2.1851e-01,  4.0142e-01,  3.7293e-01, -3.7960e-02],\n",
       "         [-1.3670e-01,  5.5603e-01, -1.2182e-01, -2.5671e-01, -1.2979e-01,\n",
       "           2.2374e-01,  4.0387e-01,  1.0660e-01, -4.7496e-01,  4.5860e-01,\n",
       "           9.1091e-02, -2.8214e-01, -2.1768e-01, -3.5494e-01,  1.2475e-01,\n",
       "           1.2181e-01, -1.6889e-01,  3.7988e-01,  3.3026e-01, -7.3742e-02],\n",
       "         [-1.3749e-01,  5.5701e-01, -1.2282e-01, -2.5484e-01, -1.3099e-01,\n",
       "           2.2618e-01,  4.0307e-01,  1.0383e-01, -4.7789e-01,  4.5768e-01,\n",
       "           9.0686e-02, -2.8153e-01, -2.1594e-01, -3.5462e-01,  1.2345e-01,\n",
       "           1.2268e-01, -1.7088e-01,  3.7777e-01,  3.2965e-01, -7.4762e-02],\n",
       "         [-1.5068e-01,  4.8825e-01, -1.4193e-01, -2.3905e-01, -1.1197e-01,\n",
       "           2.0348e-01,  3.5357e-01,  1.0547e-01, -4.5546e-01,  3.8777e-01,\n",
       "           8.8126e-02, -2.5001e-01, -1.7035e-01, -3.2720e-01,  5.3138e-02,\n",
       "           1.2462e-01, -1.8134e-01,  3.4339e-01,  3.2471e-01, -1.1686e-02],\n",
       "         [-1.8322e-01,  5.4833e-01, -1.6325e-01, -2.6582e-01, -1.0710e-01,\n",
       "           2.4750e-01,  3.9869e-01,  1.2612e-01, -4.7807e-01,  4.5649e-01,\n",
       "           1.0039e-01, -2.7256e-01, -1.8252e-01, -3.5186e-01,  8.8609e-02,\n",
       "           1.1764e-01, -1.6757e-01,  3.8564e-01,  3.5570e-01, -6.8460e-03],\n",
       "         [-1.1589e-01,  5.5161e-01, -1.2888e-01, -2.6884e-01, -1.3284e-01,\n",
       "           2.1388e-01,  4.1677e-01,  1.2687e-01, -4.8920e-01,  4.4036e-01,\n",
       "           1.0236e-01, -2.8630e-01, -2.3281e-01, -3.7923e-01,  1.1715e-01,\n",
       "           9.5196e-02, -1.9329e-01,  4.2368e-01,  3.4880e-01, -5.0312e-02]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi head attention block applies multiple attention heads as can be seen in the paper \"Attention is all you need\", then concatenates the output and applies single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert d_model == d_feature * n_heads\n",
    "        \n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"Output of single head\")\n",
    "        \n",
    "        #reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Sequence, D_Feature * n_heads)\n",
    "        log_size(x, \"Concatenated output\") \n",
    "        x = self.projection(x) # (Batch, Sequence, D_model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1281, -0.1040, -0.1596,  ..., -0.0451, -0.0963, -0.2033],\n",
       "         [-0.1508, -0.1195, -0.1500,  ..., -0.0101, -0.1618, -0.2477],\n",
       "         [-0.1222, -0.0916, -0.1591,  ..., -0.0591, -0.1288, -0.2281],\n",
       "         ...,\n",
       "         [-0.1259, -0.1078, -0.1386,  ..., -0.0627, -0.1354, -0.1823],\n",
       "         [-0.1296, -0.1241, -0.1671,  ..., -0.0396, -0.1452, -0.2197],\n",
       "         [-0.1626, -0.1016, -0.1604,  ..., -0.0320, -0.1580, -0.2511]],\n",
       "\n",
       "        [[-0.0885, -0.1208, -0.1718,  ...,  0.0211, -0.1654, -0.2570],\n",
       "         [-0.0890, -0.1509, -0.2155,  ...,  0.0176, -0.1603, -0.2447],\n",
       "         [-0.0618, -0.1015, -0.2157,  ...,  0.0111, -0.1502, -0.1866],\n",
       "         ...,\n",
       "         [-0.1170, -0.1258, -0.1726,  ...,  0.0263, -0.1808, -0.2803],\n",
       "         [-0.0922, -0.1131, -0.2260,  ..., -0.0078, -0.1462, -0.2483],\n",
       "         [-0.0864, -0.0788, -0.2351,  ..., -0.0040, -0.1427, -0.1954]],\n",
       "\n",
       "        [[-0.0956, -0.1120, -0.1257,  ..., -0.0546, -0.1430, -0.1628],\n",
       "         [-0.0746, -0.1097, -0.1423,  ..., -0.0247, -0.1520, -0.1750],\n",
       "         [-0.1143, -0.1293, -0.1284,  ..., -0.0135, -0.1472, -0.2456],\n",
       "         ...,\n",
       "         [-0.1204, -0.1373, -0.1577,  ..., -0.0535, -0.1240, -0.2289],\n",
       "         [-0.0816, -0.1142, -0.0970,  ..., -0.0296, -0.1587, -0.1966],\n",
       "         [-0.1292, -0.1087, -0.1584,  ..., -0.0566, -0.1155, -0.2449]],\n",
       "\n",
       "        [[-0.1295, -0.1057, -0.2003,  ..., -0.0140, -0.1045, -0.2978],\n",
       "         [-0.1591, -0.1205, -0.1845,  ..., -0.0192, -0.1428, -0.2438],\n",
       "         [-0.1853, -0.1078, -0.1953,  ..., -0.0081, -0.1198, -0.3278],\n",
       "         ...,\n",
       "         [-0.1174, -0.0963, -0.1699,  ...,  0.0049, -0.1207, -0.2866],\n",
       "         [-0.0957, -0.1009, -0.1650,  ...,  0.0032, -0.1595, -0.2505],\n",
       "         [-0.1388, -0.0974, -0.1703,  ...,  0.0211, -0.1176, -0.3310]],\n",
       "\n",
       "        [[-0.0807, -0.1521, -0.1697,  ..., -0.0776, -0.1522, -0.2032],\n",
       "         [-0.0753, -0.1397, -0.1544,  ..., -0.0425, -0.1427, -0.2285],\n",
       "         [-0.1008, -0.1832, -0.1564,  ..., -0.0656, -0.1895, -0.2117],\n",
       "         ...,\n",
       "         [-0.0866, -0.1875, -0.1108,  ..., -0.0207, -0.1936, -0.2328],\n",
       "         [-0.0480, -0.1679, -0.1206,  ..., -0.0978, -0.1467, -0.1577],\n",
       "         [-0.1126, -0.1270, -0.2202,  ..., -0.0847, -0.1051, -0.1998]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
