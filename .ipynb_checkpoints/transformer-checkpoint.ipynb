{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Transformer from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is based off of the following repos/blog posts:\n",
    "\n",
    "- [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) \n",
    "\n",
    "Thanks so much to their authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=[{cls.__name__}, {name}, {tsr.shape}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "\n",
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1)\n",
    "        assert q.size(-1) == d_k\n",
    "        \n",
    "        # Compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature))\n",
    "        \n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        \n",
    "        attn = torch.exp(attn)\n",
    "        \n",
    "        log_size(attn, \"attention weight\") # Batch, Seq, Seq\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5747, 0.5588, 0.5604, 0.5413, 0.4001, 0.5414, 0.5919, 0.5281,\n",
       "          0.4878, 0.4383, 0.4066, 0.5405, 0.4926, 0.5728, 0.4668, 0.4833,\n",
       "          0.3724, 0.3557, 0.5482, 0.5897],\n",
       "         [0.5776, 0.5657, 0.5311, 0.5910, 0.4681, 0.5854, 0.5741, 0.5346,\n",
       "          0.6200, 0.4737, 0.3521, 0.5186, 0.4846, 0.5690, 0.4792, 0.5204,\n",
       "          0.3871, 0.4188, 0.5351, 0.5517],\n",
       "         [0.5668, 0.5620, 0.5910, 0.5604, 0.4933, 0.5882, 0.5743, 0.5497,\n",
       "          0.5484, 0.5404, 0.4237, 0.5093, 0.5146, 0.6038, 0.4010, 0.5421,\n",
       "          0.3051, 0.4411, 0.5341, 0.5842],\n",
       "         [0.6067, 0.6504, 0.6093, 0.5938, 0.5001, 0.6017, 0.6295, 0.5948,\n",
       "          0.6072, 0.5263, 0.4161, 0.5852, 0.5826, 0.6212, 0.4943, 0.5474,\n",
       "          0.4130, 0.4358, 0.5729, 0.5924],\n",
       "         [0.5021, 0.5467, 0.4901, 0.5166, 0.4331, 0.5713, 0.5534, 0.5780,\n",
       "          0.5203, 0.5151, 0.4063, 0.5148, 0.5387, 0.5561, 0.4459, 0.5483,\n",
       "          0.3647, 0.4241, 0.5175, 0.4893],\n",
       "         [0.5491, 0.4776, 0.5300, 0.5573, 0.4626, 0.5848, 0.5293, 0.4852,\n",
       "          0.5392, 0.4637, 0.3618, 0.4566, 0.4406, 0.5705, 0.3943, 0.4988,\n",
       "          0.3053, 0.3983, 0.5073, 0.5447],\n",
       "         [0.6114, 0.6539, 0.6246, 0.6115, 0.5031, 0.6096, 0.6349, 0.5860,\n",
       "          0.6037, 0.5249, 0.4303, 0.5876, 0.5701, 0.6265, 0.4946, 0.5579,\n",
       "          0.3918, 0.4428, 0.5897, 0.6193],\n",
       "         [0.5050, 0.5688, 0.4753, 0.4629, 0.3687, 0.3808, 0.5182, 0.4870,\n",
       "          0.5390, 0.4527, 0.2829, 0.4600, 0.3973, 0.4672, 0.3937, 0.4274,\n",
       "          0.2852, 0.3723, 0.4225, 0.4938],\n",
       "         [0.5922, 0.5681, 0.6199, 0.5581, 0.4929, 0.5869, 0.5897, 0.5554,\n",
       "          0.5335, 0.5400, 0.4378, 0.5235, 0.5259, 0.6153, 0.4012, 0.5324,\n",
       "          0.3152, 0.4253, 0.5393, 0.6126],\n",
       "         [0.5617, 0.5271, 0.5496, 0.4773, 0.4121, 0.4407, 0.5418, 0.5443,\n",
       "          0.5337, 0.5405, 0.3706, 0.4719, 0.4108, 0.5043, 0.3520, 0.4745,\n",
       "          0.2787, 0.4077, 0.4233, 0.5767]],\n",
       "\n",
       "        [[0.6283, 0.6867, 0.4701, 0.5650, 0.5198, 0.5153, 0.3975, 0.6635,\n",
       "          0.6348, 0.7291, 0.3377, 0.4867, 0.5127, 0.6793, 0.4272, 0.4731,\n",
       "          0.3942, 0.4859, 0.7787, 0.3660],\n",
       "         [0.5610, 0.5718, 0.2899, 0.4535, 0.3972, 0.4233, 0.3085, 0.5494,\n",
       "          0.5132, 0.5954, 0.3146, 0.3597, 0.4003, 0.5060, 0.3598, 0.4376,\n",
       "          0.3145, 0.3772, 0.6777, 0.2534],\n",
       "         [0.6378, 0.6646, 0.4670, 0.5682, 0.5295, 0.5120, 0.4093, 0.6651,\n",
       "          0.6596, 0.7351, 0.3466, 0.4796, 0.5269, 0.6750, 0.4230, 0.4859,\n",
       "          0.4085, 0.4787, 0.7808, 0.3700],\n",
       "         [0.6096, 0.6054, 0.3963, 0.5519, 0.4508, 0.4688, 0.3886, 0.5191,\n",
       "          0.5379, 0.6756, 0.3109, 0.4420, 0.5103, 0.5656, 0.3869, 0.4107,\n",
       "          0.3838, 0.3966, 0.6246, 0.3516],\n",
       "         [0.6056, 0.6543, 0.3827, 0.5667, 0.4197, 0.4825, 0.3505, 0.5542,\n",
       "          0.5655, 0.6895, 0.3396, 0.4337, 0.4705, 0.5818, 0.4100, 0.4859,\n",
       "          0.3905, 0.4215, 0.7340, 0.3023],\n",
       "         [0.6087, 0.6537, 0.4757, 0.5365, 0.5309, 0.4925, 0.3905, 0.6869,\n",
       "          0.6622, 0.7308, 0.3421, 0.4783, 0.5207, 0.6911, 0.3930, 0.4819,\n",
       "          0.3850, 0.4937, 0.7805, 0.3552],\n",
       "         [0.6141, 0.6611, 0.4771, 0.5405, 0.5330, 0.4947, 0.3923, 0.6772,\n",
       "          0.6558, 0.7297, 0.3403, 0.4815, 0.5222, 0.6927, 0.3976, 0.4734,\n",
       "          0.3891, 0.4980, 0.7782, 0.3599],\n",
       "         [0.6154, 0.6623, 0.4778, 0.5547, 0.5294, 0.4936, 0.4017, 0.6661,\n",
       "          0.6639, 0.7263, 0.3380, 0.4820, 0.5221, 0.6804, 0.4075, 0.4820,\n",
       "          0.3996, 0.4899, 0.7859, 0.3597],\n",
       "         [0.6174, 0.6530, 0.4772, 0.5538, 0.5290, 0.4877, 0.3958, 0.6703,\n",
       "          0.6710, 0.7275, 0.3434, 0.4881, 0.5344, 0.6787, 0.3971, 0.4816,\n",
       "          0.3964, 0.4944, 0.7816, 0.3628],\n",
       "         [0.4759, 0.5398, 0.4559, 0.4519, 0.4581, 0.4640, 0.3461, 0.6296,\n",
       "          0.5567, 0.6143, 0.2436, 0.3981, 0.3976, 0.6167, 0.3742, 0.3967,\n",
       "          0.3184, 0.3863, 0.6563, 0.3393]],\n",
       "\n",
       "        [[0.4573, 0.5214, 0.5881, 0.6513, 0.4995, 0.4754, 0.3911, 0.6438,\n",
       "          0.4625, 0.3455, 0.3764, 0.5418, 0.4237, 0.4730, 0.5344, 0.4815,\n",
       "          0.3713, 0.4561, 0.5886, 0.7129],\n",
       "         [0.4744, 0.4592, 0.5868, 0.5850, 0.5348, 0.4711, 0.3795, 0.6246,\n",
       "          0.3826, 0.3547, 0.3646, 0.5347, 0.3746, 0.3615, 0.5116, 0.4227,\n",
       "          0.3671, 0.4262, 0.6465, 0.6471],\n",
       "         [0.5432, 0.5614, 0.7005, 0.6506, 0.5678, 0.5014, 0.4238, 0.6578,\n",
       "          0.4773, 0.4007, 0.3858, 0.6520, 0.4203, 0.4940, 0.5821, 0.5378,\n",
       "          0.3926, 0.5318, 0.6742, 0.7640],\n",
       "         [0.3759, 0.5226, 0.5929, 0.6462, 0.4610, 0.4374, 0.3153, 0.5511,\n",
       "          0.4630, 0.3347, 0.3317, 0.4523, 0.4006, 0.4691, 0.4906, 0.4918,\n",
       "          0.3827, 0.4409, 0.5038, 0.6469],\n",
       "         [0.4928, 0.4135, 0.6280, 0.5529, 0.4461, 0.3675, 0.3669, 0.5900,\n",
       "          0.4625, 0.3378, 0.3570, 0.5951, 0.3561, 0.4025, 0.4995, 0.5073,\n",
       "          0.3472, 0.5013, 0.5335, 0.6365],\n",
       "         [0.4396, 0.5234, 0.5891, 0.6461, 0.4715, 0.4498, 0.3864, 0.6253,\n",
       "          0.4634, 0.3531, 0.3716, 0.5466, 0.4114, 0.4743, 0.5423, 0.4837,\n",
       "          0.3903, 0.4514, 0.5774, 0.7235],\n",
       "         [0.5202, 0.4163, 0.5760, 0.5394, 0.5318, 0.4657, 0.3835, 0.5963,\n",
       "          0.3988, 0.3447, 0.2984, 0.5825, 0.3917, 0.4167, 0.4814, 0.4636,\n",
       "          0.2875, 0.4819, 0.6032, 0.6710],\n",
       "         [0.5393, 0.5604, 0.7103, 0.6456, 0.5534, 0.4842, 0.4167, 0.6509,\n",
       "          0.4748, 0.4040, 0.3806, 0.6586, 0.4125, 0.4891, 0.5932, 0.5369,\n",
       "          0.4069, 0.5361, 0.6741, 0.7717],\n",
       "         [0.5361, 0.5581, 0.7095, 0.6665, 0.5669, 0.4972, 0.4125, 0.6689,\n",
       "          0.4817, 0.4043, 0.3857, 0.6363, 0.4341, 0.4897, 0.5904, 0.5371,\n",
       "          0.4124, 0.5402, 0.6704, 0.7727],\n",
       "         [0.4918, 0.5472, 0.6164, 0.5690, 0.5054, 0.4759, 0.3473, 0.6051,\n",
       "          0.4631, 0.2924, 0.3736, 0.5397, 0.3209, 0.4047, 0.4711, 0.5051,\n",
       "          0.3481, 0.4409, 0.6087, 0.6474]],\n",
       "\n",
       "        [[0.4834, 0.3478, 0.3488, 0.4521, 0.4530, 0.4130, 0.3850, 0.3355,\n",
       "          0.3115, 0.4429, 0.5151, 0.2586, 0.4421, 0.4292, 0.2593, 0.5609,\n",
       "          0.4631, 0.3357, 0.4183, 0.2903],\n",
       "         [0.6435, 0.5644, 0.4297, 0.5117, 0.6879, 0.5611, 0.5267, 0.4889,\n",
       "          0.5002, 0.5882, 0.6515, 0.3263, 0.5817, 0.5465, 0.3387, 0.6789,\n",
       "          0.5575, 0.4070, 0.5732, 0.4194],\n",
       "         [0.5282, 0.4001, 0.2722, 0.3532, 0.6194, 0.4360, 0.4208, 0.4497,\n",
       "          0.3516, 0.4054, 0.4949, 0.2697, 0.5134, 0.4524, 0.2223, 0.5254,\n",
       "          0.4697, 0.3210, 0.4968, 0.4071],\n",
       "         [0.5687, 0.4711, 0.3341, 0.4455, 0.6492, 0.5208, 0.4534, 0.4705,\n",
       "          0.4077, 0.4844, 0.5583, 0.3131, 0.5499, 0.5360, 0.2946, 0.5934,\n",
       "          0.5487, 0.3719, 0.5427, 0.4158],\n",
       "         [0.5713, 0.4776, 0.3379, 0.4372, 0.6629, 0.5023, 0.4552, 0.4817,\n",
       "          0.4312, 0.4908, 0.5397, 0.3248, 0.5568, 0.5305, 0.2933, 0.6022,\n",
       "          0.5458, 0.3751, 0.5388, 0.4292],\n",
       "         [0.6275, 0.5678, 0.4248, 0.5154, 0.6790, 0.5788, 0.5427, 0.4917,\n",
       "          0.5080, 0.5846, 0.6497, 0.3151, 0.5871, 0.5337, 0.3409, 0.6695,\n",
       "          0.5536, 0.3971, 0.5570, 0.4228],\n",
       "         [0.3800, 0.4505, 0.2931, 0.4086, 0.4548, 0.4247, 0.4647, 0.4026,\n",
       "          0.4558, 0.4200, 0.4209, 0.2037, 0.4377, 0.3074, 0.2654, 0.4621,\n",
       "          0.4027, 0.2816, 0.3047, 0.2673],\n",
       "         [0.6048, 0.5131, 0.4208, 0.4062, 0.6313, 0.4611, 0.4248, 0.3909,\n",
       "          0.4918, 0.5646, 0.5470, 0.3213, 0.4948, 0.5004, 0.2904, 0.6231,\n",
       "          0.4458, 0.3647, 0.5415, 0.4091],\n",
       "         [0.6296, 0.5566, 0.4432, 0.5398, 0.6576, 0.5816, 0.5568, 0.4871,\n",
       "          0.4900, 0.5901, 0.6689, 0.3097, 0.5976, 0.5238, 0.3434, 0.6868,\n",
       "          0.5548, 0.3875, 0.5376, 0.4299],\n",
       "         [0.6375, 0.5515, 0.4425, 0.5203, 0.6744, 0.5695, 0.5429, 0.4761,\n",
       "          0.5076, 0.5988, 0.6568, 0.3218, 0.5941, 0.5356, 0.3369, 0.6940,\n",
       "          0.5502, 0.4066, 0.5608, 0.4350]],\n",
       "\n",
       "        [[0.6081, 0.4103, 0.5055, 0.4816, 0.4102, 0.6467, 0.4765, 0.6174,\n",
       "          0.5859, 0.6895, 0.6324, 0.5755, 0.5975, 0.6442, 0.5280, 0.5821,\n",
       "          0.4001, 0.5881, 0.4998, 0.6790],\n",
       "         [0.4895, 0.3610, 0.4847, 0.4292, 0.3691, 0.5155, 0.3986, 0.5069,\n",
       "          0.5381, 0.5389, 0.4697, 0.5154, 0.5238, 0.5572, 0.3950, 0.5239,\n",
       "          0.3241, 0.5370, 0.4226, 0.5746],\n",
       "         [0.6073, 0.3929, 0.5070, 0.4854, 0.4123, 0.6175, 0.4714, 0.6173,\n",
       "          0.5901, 0.6745, 0.6248, 0.5760, 0.6106, 0.6661, 0.5218, 0.5879,\n",
       "          0.4197, 0.5961, 0.4965, 0.6690],\n",
       "         [0.6076, 0.4108, 0.5001, 0.4835, 0.4083, 0.6572, 0.4846, 0.6152,\n",
       "          0.5796, 0.6872, 0.6369, 0.5660, 0.5897, 0.6477, 0.5259, 0.5804,\n",
       "          0.4006, 0.5889, 0.4949, 0.6774],\n",
       "         [0.6060, 0.4092, 0.5241, 0.4911, 0.4183, 0.6663, 0.4961, 0.6219,\n",
       "          0.5891, 0.6762, 0.6220, 0.5559, 0.5833, 0.6484, 0.5317, 0.5692,\n",
       "          0.3947, 0.6060, 0.4864, 0.6758],\n",
       "         [0.5844, 0.4004, 0.4682, 0.4273, 0.3583, 0.5982, 0.4783, 0.5263,\n",
       "          0.5579, 0.6003, 0.5440, 0.5254, 0.5090, 0.6008, 0.4359, 0.5320,\n",
       "          0.3323, 0.5699, 0.4414, 0.5601],\n",
       "         [0.5886, 0.4002, 0.3704, 0.3651, 0.3221, 0.5462, 0.4030, 0.5350,\n",
       "          0.5247, 0.6716, 0.5955, 0.5773, 0.5369, 0.5293, 0.4643, 0.5614,\n",
       "          0.3486, 0.4700, 0.5039, 0.5781],\n",
       "         [0.5363, 0.2929, 0.4630, 0.4050, 0.3210, 0.4996, 0.4016, 0.5257,\n",
       "          0.5087, 0.4977, 0.4586, 0.4526, 0.4362, 0.5115, 0.4276, 0.4326,\n",
       "          0.3178, 0.5604, 0.3885, 0.4584],\n",
       "         [0.5903, 0.4092, 0.5212, 0.4827, 0.4331, 0.6444, 0.4901, 0.6186,\n",
       "          0.5889, 0.6790, 0.6078, 0.5675, 0.6120, 0.6517, 0.5336, 0.5871,\n",
       "          0.3921, 0.5896, 0.4881, 0.6998],\n",
       "         [0.6061, 0.4036, 0.5147, 0.4783, 0.4234, 0.6113, 0.4668, 0.6178,\n",
       "          0.6038, 0.6854, 0.6166, 0.5957, 0.6118, 0.6446, 0.5323, 0.5844,\n",
       "          0.3967, 0.5962, 0.5090, 0.6817]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        # We assume that the queries, keys, features all have the same feature size.\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries)\n",
    "        K = self.key_tfm(keys)\n",
    "        V = self.value_tfm(values)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        \n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3332,  0.1562, -0.2520, -0.5439,  0.7122,  0.5628,  0.1360,\n",
       "          -0.4152,  0.4826, -0.1513,  0.1452, -0.4091, -0.6543, -0.0346,\n",
       "           0.1930, -0.2207,  0.0200,  0.1493, -0.6084, -0.5273],\n",
       "         [ 0.3476,  0.1558, -0.2437, -0.5314,  0.6916,  0.6086,  0.1444,\n",
       "          -0.4102,  0.5066, -0.1382,  0.1524, -0.4225, -0.6639, -0.0427,\n",
       "           0.1914, -0.1967, -0.0146,  0.1179, -0.6035, -0.5093],\n",
       "         [ 0.2885,  0.1691, -0.2391, -0.5583,  0.7181,  0.5551,  0.1639,\n",
       "          -0.4489,  0.4768, -0.1536,  0.1537, -0.4600, -0.6541, -0.0337,\n",
       "           0.1846, -0.2111,  0.0392,  0.1679, -0.6096, -0.5034],\n",
       "         [ 0.3480,  0.1311, -0.2577, -0.5335,  0.6823,  0.5617,  0.1261,\n",
       "          -0.4567,  0.4900, -0.1234,  0.1167, -0.4095, -0.6620, -0.0324,\n",
       "           0.1794, -0.2123,  0.0020,  0.1590, -0.5870, -0.4732],\n",
       "         [ 0.2952,  0.1616, -0.1887, -0.4129,  0.4859,  0.4585,  0.0795,\n",
       "          -0.3534,  0.4185, -0.1273,  0.0311, -0.3608, -0.5353, -0.0040,\n",
       "           0.1157, -0.1877,  0.0204,  0.1591, -0.4944, -0.3624],\n",
       "         [ 0.2854,  0.1661, -0.2369, -0.5575,  0.7187,  0.5533,  0.1641,\n",
       "          -0.4424,  0.4716, -0.1522,  0.1588, -0.4558, -0.6490, -0.0347,\n",
       "           0.1872, -0.2073,  0.0369,  0.1625, -0.6059, -0.5041],\n",
       "         [ 0.3095,  0.1188, -0.2210, -0.4845,  0.6236,  0.5540,  0.1285,\n",
       "          -0.3573,  0.4385, -0.1197,  0.1561, -0.3698, -0.5933, -0.0376,\n",
       "           0.1810, -0.1488, -0.0298,  0.0895, -0.5338, -0.4647],\n",
       "         [ 0.3471,  0.1545, -0.2434, -0.5334,  0.6933,  0.6092,  0.1446,\n",
       "          -0.4100,  0.5066, -0.1387,  0.1549, -0.4225, -0.6642, -0.0431,\n",
       "           0.1930, -0.1951, -0.0157,  0.1172, -0.6036, -0.5095],\n",
       "         [ 0.3304,  0.1568, -0.2471, -0.5476,  0.7173,  0.5603,  0.1338,\n",
       "          -0.4084,  0.4825, -0.1533,  0.1502, -0.4063, -0.6501, -0.0348,\n",
       "           0.1996, -0.2204,  0.0176,  0.1458, -0.6077, -0.5278],\n",
       "         [ 0.3183,  0.1044, -0.2303, -0.4874,  0.5863,  0.4940,  0.1026,\n",
       "          -0.3953,  0.4217, -0.1320,  0.1195, -0.3586, -0.5932, -0.0438,\n",
       "           0.1581, -0.1890,  0.0192,  0.1489, -0.5291, -0.4145]],\n",
       "\n",
       "        [[ 0.3357,  0.0853, -0.2300, -0.4096,  0.5217,  0.4430,  0.1059,\n",
       "          -0.3502,  0.3385, -0.1535,  0.0597, -0.3561, -0.5083, -0.0540,\n",
       "           0.0704, -0.1272,  0.0434,  0.1454, -0.4893, -0.4469],\n",
       "         [ 0.3451,  0.1530, -0.2864, -0.4222,  0.5569,  0.4884,  0.1439,\n",
       "          -0.3557,  0.3741, -0.1697,  0.0798, -0.3698, -0.5436, -0.0534,\n",
       "           0.0879, -0.1725,  0.0489,  0.1450, -0.5663, -0.4675],\n",
       "         [ 0.3763,  0.1461, -0.2982, -0.4903,  0.5928,  0.5609,  0.1473,\n",
       "          -0.4386,  0.4038, -0.1638,  0.0561, -0.4248, -0.6012, -0.0829,\n",
       "           0.0588, -0.1749,  0.0680,  0.2036, -0.5939, -0.4927],\n",
       "         [ 0.3227,  0.1402, -0.2840, -0.4370,  0.5585,  0.4820,  0.1454,\n",
       "          -0.4055,  0.3486, -0.1549,  0.0872, -0.3802, -0.5469, -0.0607,\n",
       "           0.0643, -0.1430,  0.0589,  0.1880, -0.5690, -0.4557],\n",
       "         [ 0.3287,  0.1095, -0.2090, -0.3905,  0.4867,  0.4271,  0.0724,\n",
       "          -0.3423,  0.3750, -0.1601,  0.0259, -0.3320, -0.4957, -0.0498,\n",
       "           0.0885, -0.1476,  0.0492,  0.1592, -0.5028, -0.3817],\n",
       "         [ 0.2992,  0.1308, -0.2374, -0.3573,  0.4991,  0.4230,  0.0286,\n",
       "          -0.3405,  0.3980, -0.1494,  0.0389, -0.3357, -0.4890, -0.0289,\n",
       "           0.0991, -0.1020,  0.0051,  0.1111, -0.5038, -0.3899],\n",
       "         [ 0.2994,  0.1204, -0.2680, -0.3764,  0.4629,  0.4273,  0.0937,\n",
       "          -0.3549,  0.3595, -0.1306,  0.0108, -0.3350, -0.5194, -0.0410,\n",
       "           0.0740, -0.1402,  0.0606,  0.1429, -0.4998, -0.3518],\n",
       "         [ 0.2556,  0.0746, -0.1620, -0.3085,  0.4024,  0.3786,  0.0360,\n",
       "          -0.2723,  0.2876, -0.1595,  0.0503, -0.2979, -0.3826, -0.0838,\n",
       "           0.0446, -0.0721,  0.0329,  0.1120, -0.4094, -0.3814],\n",
       "         [ 0.3748,  0.1227, -0.2985, -0.4805,  0.6127,  0.5350,  0.1293,\n",
       "          -0.4089,  0.4062, -0.1848,  0.0548, -0.4400, -0.6014, -0.0566,\n",
       "           0.0755, -0.1538,  0.0574,  0.1608, -0.5877, -0.5223],\n",
       "         [ 0.3284,  0.0890, -0.2572, -0.3997,  0.4739,  0.4546,  0.1250,\n",
       "          -0.3528,  0.3167, -0.1514,  0.0388, -0.3319, -0.5030, -0.1011,\n",
       "           0.0437, -0.1500,  0.0764,  0.1625, -0.4665, -0.4119]],\n",
       "\n",
       "        [[ 0.2276,  0.0186, -0.2558, -0.4762,  0.6843,  0.4975,  0.1542,\n",
       "          -0.4662,  0.4095, -0.1541,  0.0838, -0.3939, -0.5583, -0.0304,\n",
       "           0.0931, -0.1177,  0.0402,  0.1884, -0.4953, -0.4545],\n",
       "         [ 0.2359,  0.0147, -0.2815, -0.5354,  0.7494,  0.5591,  0.1516,\n",
       "          -0.5167,  0.4700, -0.1721,  0.1018, -0.4379, -0.6231, -0.0544,\n",
       "           0.1182, -0.1447,  0.0481,  0.1956, -0.5652, -0.4990],\n",
       "         [ 0.1995,  0.0137, -0.2507, -0.5018,  0.6823,  0.5075,  0.1309,\n",
       "          -0.4754,  0.4141, -0.1557,  0.0911, -0.3796, -0.5553, -0.0706,\n",
       "           0.0819, -0.1403,  0.0409,  0.2099, -0.5282, -0.4590],\n",
       "         [ 0.2289,  0.0197, -0.2571, -0.4785,  0.6895,  0.5001,  0.1572,\n",
       "          -0.4686,  0.4112, -0.1558,  0.0841, -0.3980, -0.5604, -0.0293,\n",
       "           0.0935, -0.1189,  0.0400,  0.1895, -0.4978, -0.4575],\n",
       "         [ 0.2058,  0.0220, -0.2377, -0.4865,  0.6932,  0.4816,  0.1595,\n",
       "          -0.4595,  0.3993, -0.1838,  0.0976, -0.4321, -0.5339, -0.0228,\n",
       "           0.1123, -0.1340,  0.0467,  0.1875, -0.5022, -0.4484],\n",
       "         [ 0.1380,  0.0558, -0.1347, -0.3764,  0.5280,  0.3672,  0.0931,\n",
       "          -0.3521,  0.3540, -0.1319,  0.0806, -0.3504, -0.4240, -0.0034,\n",
       "           0.1167, -0.1127,  0.0239,  0.1404, -0.4516, -0.3200],\n",
       "         [ 0.2378,  0.0160, -0.2829, -0.5343,  0.7507,  0.5584,  0.1527,\n",
       "          -0.5157,  0.4702, -0.1729,  0.1007, -0.4373, -0.6215, -0.0520,\n",
       "           0.1200, -0.1446,  0.0461,  0.1941, -0.5635, -0.5008],\n",
       "         [ 0.1991,  0.0122, -0.2512, -0.5011,  0.6809,  0.5038,  0.1288,\n",
       "          -0.4742,  0.4112, -0.1574,  0.0901, -0.3771, -0.5519, -0.0697,\n",
       "           0.0829, -0.1388,  0.0405,  0.2099, -0.5254, -0.4591],\n",
       "         [ 0.2367,  0.0138, -0.2827, -0.5358,  0.7491,  0.5609,  0.1498,\n",
       "          -0.5165,  0.4724, -0.1716,  0.1018, -0.4367, -0.6248, -0.0557,\n",
       "           0.1192, -0.1449,  0.0476,  0.1937, -0.5650, -0.4997],\n",
       "         [ 0.2102,  0.0317, -0.2252, -0.4871,  0.6692,  0.4916,  0.0907,\n",
       "          -0.4539,  0.4579, -0.1576,  0.0879, -0.3869, -0.5631, -0.0325,\n",
       "           0.1404, -0.1254,  0.0239,  0.1658, -0.5368, -0.4396]],\n",
       "\n",
       "        [[ 0.2390,  0.1764, -0.2379, -0.5074,  0.7182,  0.5792,  0.2269,\n",
       "          -0.4084,  0.4528, -0.1278,  0.1263, -0.4912, -0.6539,  0.0718,\n",
       "           0.1759, -0.2586,  0.0264,  0.1274, -0.6357, -0.5262],\n",
       "         [ 0.2771,  0.1748, -0.2841, -0.5676,  0.8140,  0.6622,  0.2370,\n",
       "          -0.4426,  0.5251, -0.1538,  0.1086, -0.5716, -0.7236,  0.0911,\n",
       "           0.2034, -0.2794,  0.0274,  0.1093, -0.7017, -0.5941],\n",
       "         [ 0.2769,  0.1741, -0.2856, -0.5672,  0.8128,  0.6634,  0.2370,\n",
       "          -0.4427,  0.5262, -0.1539,  0.1075, -0.5718, -0.7239,  0.0898,\n",
       "           0.2033, -0.2798,  0.0282,  0.1079, -0.7015, -0.5946],\n",
       "         [ 0.2332,  0.1253, -0.2203, -0.4385,  0.6000,  0.4993,  0.1650,\n",
       "          -0.3797,  0.4273, -0.1413,  0.0522, -0.4534, -0.5607,  0.0636,\n",
       "           0.1482, -0.2390,  0.0346,  0.1117, -0.5395, -0.4469],\n",
       "         [ 0.2598,  0.1249, -0.2702, -0.5184,  0.7365,  0.6003,  0.1979,\n",
       "          -0.3938,  0.4847, -0.1405,  0.0529, -0.5453, -0.6405,  0.0894,\n",
       "           0.1891, -0.2497,  0.0485,  0.0720, -0.6084, -0.5399],\n",
       "         [ 0.2558,  0.1504, -0.2684, -0.4923,  0.6895,  0.5988,  0.1996,\n",
       "          -0.4070,  0.4873, -0.1537,  0.0748, -0.5214, -0.6424,  0.0643,\n",
       "           0.1793, -0.2444,  0.0253,  0.0836, -0.6245, -0.5325],\n",
       "         [ 0.2218,  0.1956, -0.2356, -0.5088,  0.7258,  0.5838,  0.2151,\n",
       "          -0.3847,  0.4594, -0.1483,  0.1165, -0.5256, -0.6578,  0.0932,\n",
       "           0.1881, -0.2470,  0.0407,  0.1101, -0.6746, -0.5375],\n",
       "         [ 0.2260,  0.1214, -0.2308, -0.4404,  0.6286,  0.5161,  0.1722,\n",
       "          -0.3708,  0.4185, -0.1443,  0.0573, -0.4636, -0.5408,  0.0726,\n",
       "           0.1413, -0.1954,  0.0039,  0.0974, -0.5773, -0.4902],\n",
       "         [ 0.2561,  0.1386, -0.2659, -0.5195,  0.7592,  0.6137,  0.2247,\n",
       "          -0.3755,  0.4873, -0.1412,  0.0840, -0.5241, -0.6538,  0.0930,\n",
       "           0.1957, -0.2605,  0.0106,  0.0608, -0.5864, -0.5441],\n",
       "         [ 0.2209,  0.1167, -0.2300, -0.3889,  0.5539,  0.4715,  0.1648,\n",
       "          -0.3199,  0.3647, -0.1299,  0.0497, -0.4173, -0.4935,  0.0529,\n",
       "           0.1202, -0.1702,  0.0186,  0.0799, -0.5120, -0.4332]],\n",
       "\n",
       "        [[ 0.1746,  0.1575, -0.1671, -0.3675,  0.5382,  0.4396,  0.0775,\n",
       "          -0.2598,  0.3625, -0.1331,  0.1764, -0.3263, -0.5130, -0.0016,\n",
       "           0.1883, -0.1235, -0.0193,  0.0398, -0.5058, -0.4468],\n",
       "         [ 0.2585,  0.1799, -0.2732, -0.5568,  0.7308,  0.6789,  0.1124,\n",
       "          -0.4331,  0.5276, -0.1858,  0.1939, -0.4724, -0.7338, -0.0744,\n",
       "           0.1829, -0.1830,  0.0248,  0.1069, -0.6965, -0.6704],\n",
       "         [ 0.2307,  0.1392, -0.2694, -0.4988,  0.6601,  0.6318,  0.1162,\n",
       "          -0.4093,  0.4854, -0.1644,  0.1586, -0.4231, -0.6488, -0.1018,\n",
       "           0.1403, -0.1752,  0.0388,  0.1000, -0.5961, -0.6002],\n",
       "         [ 0.2602,  0.1800, -0.2728, -0.5591,  0.7262,  0.6800,  0.1099,\n",
       "          -0.4339,  0.5274, -0.1847,  0.1882, -0.4726, -0.7341, -0.0762,\n",
       "           0.1793, -0.1843,  0.0261,  0.1097, -0.6969, -0.6693],\n",
       "         [ 0.1973,  0.1567, -0.1964, -0.4613,  0.5820,  0.5447,  0.0875,\n",
       "          -0.3001,  0.3963, -0.1499,  0.1686, -0.4034, -0.5881, -0.0138,\n",
       "           0.1774, -0.1537,  0.0297,  0.0650, -0.5918, -0.5307],\n",
       "         [ 0.1926,  0.1340, -0.2464, -0.4112,  0.5200,  0.4805,  0.0583,\n",
       "          -0.3095,  0.3659, -0.1876,  0.1288, -0.3412, -0.4852, -0.0524,\n",
       "           0.1381, -0.1283,  0.0267,  0.0744, -0.5022, -0.5326],\n",
       "         [ 0.2594,  0.1798, -0.2718, -0.5584,  0.7273,  0.6786,  0.1114,\n",
       "          -0.4332,  0.5265, -0.1849,  0.1898, -0.4727, -0.7349, -0.0746,\n",
       "           0.1798, -0.1837,  0.0263,  0.1099, -0.6975, -0.6685],\n",
       "         [ 0.2456,  0.1724, -0.2065, -0.5199,  0.6100,  0.6290,  0.0855,\n",
       "          -0.3890,  0.4864, -0.1244,  0.1435, -0.4344, -0.6777, -0.0756,\n",
       "           0.1609, -0.1877,  0.0290,  0.0980, -0.6265, -0.5748],\n",
       "         [ 0.2253,  0.1676, -0.2431, -0.4451,  0.5937,  0.5149,  0.0833,\n",
       "          -0.3379,  0.4293, -0.1848,  0.1518, -0.3758, -0.5705, -0.0451,\n",
       "           0.1862, -0.1633,  0.0090,  0.0721, -0.5554, -0.5496],\n",
       "         [ 0.2580,  0.1799, -0.2666, -0.5593,  0.7253,  0.6797,  0.1116,\n",
       "          -0.4319,  0.5272, -0.1810,  0.1907, -0.4740, -0.7366, -0.0737,\n",
       "           0.1812, -0.1846,  0.0271,  0.1089, -0.6975, -0.6640]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi head attention block applies multiple attention heads as can be seen in the paper \"Attention is all you need\", then concatenates the output and applies single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert d_model == d_feature * n_heads\n",
    "        \n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"Output of single head\")\n",
    "        \n",
    "        #reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Sequence, D_Feature * n_heads)\n",
    "        log_size(x, \"Concatenated output\") \n",
    "        x = self.projection(x) # (Batch, Sequence, D_model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3098,  0.0632, -0.0779,  ...,  0.1630,  0.1845,  0.0035],\n",
       "         [-0.2915,  0.0624, -0.1241,  ...,  0.1605,  0.2087,  0.0033],\n",
       "         [-0.3313,  0.0462, -0.1258,  ...,  0.1711,  0.2242,  0.0394],\n",
       "         ...,\n",
       "         [-0.3081,  0.0769, -0.1357,  ...,  0.1171,  0.2425,  0.0631],\n",
       "         [-0.3345,  0.0206, -0.1098,  ...,  0.1247,  0.2087,  0.0542],\n",
       "         [-0.3330,  0.0090, -0.1419,  ...,  0.1360,  0.2619,  0.0388]],\n",
       "\n",
       "        [[-0.2791,  0.0551, -0.1730,  ...,  0.1422,  0.2262,  0.0197],\n",
       "         [-0.2610,  0.0868, -0.1833,  ...,  0.1543,  0.1849,  0.0168],\n",
       "         [-0.2864,  0.1029, -0.1754,  ...,  0.1311,  0.2076,  0.0212],\n",
       "         ...,\n",
       "         [-0.2929,  0.1188, -0.1651,  ...,  0.1700,  0.2072,  0.0022],\n",
       "         [-0.2418,  0.0806, -0.1319,  ...,  0.1424,  0.1861, -0.0084],\n",
       "         [-0.2603,  0.0978, -0.1801,  ...,  0.1722,  0.2179,  0.0137]],\n",
       "\n",
       "        [[-0.2872,  0.1403, -0.1051,  ...,  0.1184,  0.1804,  0.0495],\n",
       "         [-0.3351,  0.1149, -0.0621,  ...,  0.1300,  0.1892,  0.1233],\n",
       "         [-0.3550,  0.0993, -0.0768,  ...,  0.1448,  0.2385,  0.1132],\n",
       "         ...,\n",
       "         [-0.3589,  0.1023, -0.0806,  ...,  0.1443,  0.2208,  0.0910],\n",
       "         [-0.3221,  0.1086, -0.0700,  ...,  0.1660,  0.1645,  0.0713],\n",
       "         [-0.3410,  0.1020, -0.0850,  ...,  0.1491,  0.1836,  0.0791]],\n",
       "\n",
       "        [[-0.2318,  0.0563, -0.1643,  ...,  0.1196,  0.2092,  0.0867],\n",
       "         [-0.2801,  0.0654, -0.1245,  ...,  0.1617,  0.2300,  0.0500],\n",
       "         [-0.2920,  0.0177, -0.1380,  ...,  0.1623,  0.2440,  0.0618],\n",
       "         ...,\n",
       "         [-0.2875,  0.0619, -0.1390,  ...,  0.1781,  0.2523,  0.0670],\n",
       "         [-0.2784,  0.0485, -0.1424,  ...,  0.1285,  0.2678,  0.0990],\n",
       "         [-0.2714,  0.0545, -0.1438,  ...,  0.1556,  0.2412,  0.1038]],\n",
       "\n",
       "        [[-0.3481,  0.0719, -0.0885,  ...,  0.1576,  0.2792,  0.0241],\n",
       "         [-0.3328,  0.0796, -0.0885,  ...,  0.0883,  0.2618,  0.0194],\n",
       "         [-0.3407,  0.0617, -0.0759,  ...,  0.1387,  0.2600, -0.0009],\n",
       "         ...,\n",
       "         [-0.3544,  0.0671, -0.0915,  ...,  0.1225,  0.2615,  0.0425],\n",
       "         [-0.3476,  0.0913, -0.1033,  ...,  0.1450,  0.2589,  0.0423],\n",
       "         [-0.3369,  0.0496, -0.1089,  ...,  0.1521,  0.2798,  0.0293]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is made up of the following components:\n",
    "- multi-head attention block\n",
    "- simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps =1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norml = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        attn = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention Output\")\n",
    "        # Applying normalization and residual connection.\n",
    "        x = x + self.dropout(self.layer_norml(attn))\n",
    "        # Applying position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Applying normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention Output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention Output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2019, -0.8521, -1.5490,  ...,  1.8720,  0.6059,  0.6887],\n",
       "         [-0.6209, -1.5672, -2.2606,  ...,  2.2722,  1.7333, -0.0730],\n",
       "         [-0.8003, -1.9977, -2.1881,  ...,  2.0279,  0.5202, -0.3955],\n",
       "         ...,\n",
       "         [-0.8683, -0.7518, -1.7853,  ...,  1.6978,  1.4379,  0.6216],\n",
       "         [ 0.0102, -0.6689, -1.9829,  ...,  1.8591,  0.2776,  0.8346],\n",
       "         [-0.6425, -1.0068, -1.6025,  ...,  1.7708,  1.6560,  0.0441]],\n",
       "\n",
       "        [[-1.3198, -0.1476, -2.2539,  ...,  2.1894,  0.2406, -0.1671],\n",
       "         [-0.9608, -0.5074, -0.6843,  ...,  2.7851,  1.3299, -1.2358],\n",
       "         [-0.3217, -1.9347, -1.2695,  ...,  1.6101,  0.9010, -1.2702],\n",
       "         ...,\n",
       "         [-1.2105, -0.4053, -2.1174,  ...,  1.4413,  0.7410, -0.1657],\n",
       "         [-1.3018, -0.4798, -0.8744,  ...,  2.6181,  0.7699, -0.8672],\n",
       "         [-1.2169, -0.8006,  0.2652,  ...,  2.3291,  0.3244, -0.8648]],\n",
       "\n",
       "        [[ 0.7516, -1.5048, -1.0644,  ...,  1.6249,  0.9817,  0.0616],\n",
       "         [-0.6448, -1.8260,  0.6772,  ...,  1.0483,  1.6555, -1.2635],\n",
       "         [ 0.4442,  0.4019, -2.5057,  ...,  1.9573,  1.6292, -0.4623],\n",
       "         ...,\n",
       "         [ 1.1930, -1.0629, -2.4615,  ...,  2.6265,  0.7995,  0.9011],\n",
       "         [-0.4201, -1.4855, -1.7032,  ...,  1.2585,  1.6112, -0.7297],\n",
       "         [-0.5890,  0.3194, -1.5232,  ...,  2.1044,  0.8188, -1.3610]],\n",
       "\n",
       "        [[-0.3354, -1.2227, -2.5575,  ...,  3.2058,  1.5496, -0.5672],\n",
       "         [-1.0068, -1.7327, -1.8011,  ...,  2.1216,  0.8990,  0.7681],\n",
       "         [-0.6999, -0.8284, -2.2477,  ...,  1.9240, -0.4179, -1.4130],\n",
       "         ...,\n",
       "         [-0.2317, -1.6016, -1.9628,  ...,  1.7057, -0.3395, -1.5001],\n",
       "         [-0.1031, -0.4742, -2.3955,  ...,  2.7973,  0.8770, -1.6959],\n",
       "         [ 0.3699, -1.5310, -2.5309,  ...,  2.0318,  0.3187,  0.1451]],\n",
       "\n",
       "        [[-1.2156, -0.1807, -1.4873,  ...,  2.1735,  1.0619, -1.3946],\n",
       "         [-0.4119, -0.4157, -2.0628,  ...,  1.1032,  1.2518, -0.4779],\n",
       "         [-1.2259, -0.9067,  0.4979,  ...,  1.6251,  1.6875, -0.4540],\n",
       "         ...,\n",
       "         [-0.5912, -0.9693, -1.6261,  ...,  2.6572,  0.9576, -0.1980],\n",
       "         [ 0.0123, -1.8502, -1.5040,  ...,  1.0232,  0.5118,  0.4309],\n",
       "         [-0.8965, -1.9009, -1.9659,  ...,  1.4441,  0.3909, -0.2530]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder is having six consecutive encoder blocks, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks = 6, d_model = 512, n_heads = 8, d_ff = 2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([EncoderBlock(d_model = d_model, d_feature = d_model // n_heads, \n",
    "                                                   d_ff = d_ff, dropout = dropout)\n",
    "                                       for _ in range(n_blocks)\n",
    "                                      ])\n",
    "        \n",
    "        def forward(self, x: torch.FloatTensor, mask = None):\n",
    "            for encoder in self.encoders:\n",
    "                x = encoder(x)\n",
    "            return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is same in structure as the encoder with just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Applying attention to inputs\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Applying attention to the encoder outputs and outputs of the previous layer\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Applying position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention Output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention Output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0071, -1.2843, -4.1290,  ..., -1.8316, -2.1579,  0.5785],\n",
       "         [-0.2845, -0.8257, -1.4583,  ..., -0.8472, -3.2208,  1.6757],\n",
       "         [-0.6898, -1.1150, -2.3871,  ..., -0.0223, -3.1017,  1.3372],\n",
       "         ...,\n",
       "         [-0.5948,  1.2356, -2.6924,  ..., -1.5750, -0.8074,  1.7195],\n",
       "         [-1.3559, -0.3809, -2.2441,  ..., -2.3942, -2.6711,  1.2597],\n",
       "         [-1.9006, -0.5104, -3.2280,  ...,  0.4660, -2.3734,  1.1669]],\n",
       "\n",
       "        [[-0.0665, -0.7638, -2.8435,  ..., -2.0267, -1.3121,  0.7735],\n",
       "         [-1.1719, -0.7845, -0.2363,  ..., -2.7859, -1.4313,  1.4471],\n",
       "         [-0.7660, -0.3501, -2.9673,  ..., -2.6321, -1.2241,  3.3386],\n",
       "         ...,\n",
       "         [-1.9065, -1.7367, -3.0015,  ..., -1.7995, -2.0151,  0.3647],\n",
       "         [-0.6908, -1.4623, -3.2172,  ..., -2.3560, -1.2471,  1.8071],\n",
       "         [-0.4559, -0.2864, -2.9192,  ..., -1.9094, -1.3392,  1.9209]],\n",
       "\n",
       "        [[-1.6635, -1.1010, -2.0872,  ..., -1.6636, -0.5840,  1.8209],\n",
       "         [-1.7915, -1.8656, -2.6164,  ...,  0.8467, -1.9005,  1.3022],\n",
       "         [-1.8428, -0.4214, -3.4611,  ..., -1.9263, -2.0526,  2.5613],\n",
       "         ...,\n",
       "         [-1.5732, -0.6673, -2.4015,  ..., -2.4086, -1.7710,  1.0439],\n",
       "         [-1.2969, -1.5082, -3.5935,  ..., -2.3306, -2.6694,  0.7059],\n",
       "         [-1.8975, -0.4876, -0.0914,  ..., -2.4080, -2.2633,  2.4999]],\n",
       "\n",
       "        [[-1.1427,  0.3312, -2.8553,  ..., -1.9638, -1.4889,  0.0942],\n",
       "         [-1.2392, -1.2629, -0.7349,  ..., -1.8330,  0.6308, -0.4930],\n",
       "         [-1.0625, -0.6064, -3.0088,  ..., -1.7864, -1.7833,  1.2534],\n",
       "         ...,\n",
       "         [-1.8779, -1.4284, -2.9151,  ..., -0.7610, -2.0543,  1.7429],\n",
       "         [-1.8615, -0.3235, -2.1900,  ..., -1.1552, -0.9446, -0.9767],\n",
       "         [-0.8143, -0.7458, -2.4742,  ..., -2.5198,  1.0757,  0.5486]],\n",
       "\n",
       "        [[-1.9176, -1.3109, -2.8816,  ..., -1.3149, -2.3992,  0.3540],\n",
       "         [-1.5454, -0.9983, -2.9032,  ..., -0.6125, -2.8231,  0.1979],\n",
       "         [-0.9162, -0.8672, -2.8366,  ..., -1.9710, -1.5616,  0.8655],\n",
       "         ...,\n",
       "         [-0.9833, -1.0424, -1.2203,  ..., -1.2098, -1.8667,  0.6100],\n",
       "         [-1.8175, -1.8911, -1.5365,  ...,  1.4628, -2.6310,  0.1324],\n",
       "         [-1.5537, -1.2482, -2.1148,  ..., -0.3221, -1.3856,  1.9870]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64,d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, enc_out: torch.FloatTensor, src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks don't have any notion of word order in a sentence. The Transformer explicitly adds the positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
