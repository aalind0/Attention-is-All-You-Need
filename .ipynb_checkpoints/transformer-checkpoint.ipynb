{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        '%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=[{cls.__name__}, {name}, {tsr.shape}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "\n",
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1)\n",
    "        assert q.size(-1) == d_k\n",
    "        \n",
    "        # Compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature))\n",
    "        \n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        \n",
    "        attn = torch.exp(attn)\n",
    "        \n",
    "        log_size(attn, \"attention weight\") # Batch, Seq, Seq\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5911, 0.5458, 0.4365, 0.5171, 0.6749, 0.4671, 0.4404, 0.5136,\n",
       "          0.7278, 0.4326, 0.3734, 0.4543, 0.4597, 0.5284, 0.5157, 0.6071,\n",
       "          0.7912, 0.6423, 0.5359, 0.3856],\n",
       "         [0.5981, 0.5506, 0.4611, 0.5268, 0.6753, 0.5026, 0.4653, 0.4943,\n",
       "          0.7193, 0.4539, 0.3836, 0.4768, 0.4588, 0.5002, 0.5198, 0.5863,\n",
       "          0.7832, 0.6498, 0.5481, 0.3841],\n",
       "         [0.5686, 0.4442, 0.4441, 0.4966, 0.6489, 0.4964, 0.4433, 0.3829,\n",
       "          0.6438, 0.4285, 0.3321, 0.4568, 0.3883, 0.4378, 0.4915, 0.5611,\n",
       "          0.6876, 0.6509, 0.5434, 0.3746],\n",
       "         [0.5874, 0.5380, 0.4460, 0.5247, 0.6819, 0.4935, 0.4663, 0.5041,\n",
       "          0.7309, 0.4440, 0.3720, 0.4575, 0.4571, 0.5184, 0.5227, 0.5911,\n",
       "          0.7923, 0.6545, 0.5287, 0.3792],\n",
       "         [0.5865, 0.5624, 0.4464, 0.5167, 0.6713, 0.4842, 0.4540, 0.5115,\n",
       "          0.7174, 0.4444, 0.3797, 0.4850, 0.4645, 0.5060, 0.5152, 0.5960,\n",
       "          0.7918, 0.6428, 0.5473, 0.3746],\n",
       "         [0.5927, 0.5666, 0.4519, 0.5179, 0.6557, 0.4900, 0.4553, 0.5167,\n",
       "          0.7161, 0.4501, 0.3871, 0.4656, 0.4628, 0.5028, 0.5214, 0.5738,\n",
       "          0.7852, 0.6281, 0.5355, 0.3818],\n",
       "         [0.6032, 0.5548, 0.4540, 0.5100, 0.6644, 0.4918, 0.4600, 0.5145,\n",
       "          0.7122, 0.4615, 0.3806, 0.4739, 0.4663, 0.4949, 0.5211, 0.5846,\n",
       "          0.7900, 0.6317, 0.5423, 0.3851],\n",
       "         [0.5019, 0.5168, 0.3586, 0.3906, 0.4586, 0.3801, 0.3819, 0.4258,\n",
       "          0.5342, 0.4153, 0.3190, 0.3315, 0.3979, 0.3720, 0.4198, 0.3746,\n",
       "          0.6043, 0.4093, 0.4040, 0.3424],\n",
       "         [0.6012, 0.5690, 0.4489, 0.5047, 0.6590, 0.4781, 0.4538, 0.5194,\n",
       "          0.7127, 0.4625, 0.3831, 0.4656, 0.4757, 0.4993, 0.5051, 0.5786,\n",
       "          0.7919, 0.6126, 0.5372, 0.3826],\n",
       "         [0.5950, 0.5528, 0.4448, 0.5207, 0.6632, 0.4751, 0.4552, 0.5095,\n",
       "          0.7229, 0.4488, 0.3862, 0.4482, 0.4666, 0.5166, 0.5165, 0.5813,\n",
       "          0.7844, 0.6268, 0.5282, 0.3924]],\n",
       "\n",
       "        [[0.3537, 0.6249, 0.5834, 0.4448, 0.4959, 0.4784, 0.4106, 0.5284,\n",
       "          0.4495, 0.4176, 0.4064, 0.5270, 0.5975, 0.5803, 0.6006, 0.3221,\n",
       "          0.6191, 0.4729, 0.5170, 0.5604],\n",
       "         [0.3120, 0.5873, 0.4609, 0.4842, 0.4818, 0.4207, 0.4678, 0.4861,\n",
       "          0.5207, 0.3829, 0.4342, 0.4440, 0.5246, 0.6383, 0.5413, 0.2878,\n",
       "          0.6664, 0.4536, 0.4429, 0.5923],\n",
       "         [0.4007, 0.6836, 0.5926, 0.5439, 0.5306, 0.4781, 0.4812, 0.5488,\n",
       "          0.5625, 0.4050, 0.4337, 0.5583, 0.6260, 0.6920, 0.6353, 0.3280,\n",
       "          0.7133, 0.5006, 0.5580, 0.6121],\n",
       "         [0.4032, 0.6918, 0.6026, 0.5451, 0.5290, 0.4943, 0.4723, 0.5507,\n",
       "          0.5549, 0.4097, 0.4440, 0.5714, 0.6164, 0.6793, 0.6504, 0.3342,\n",
       "          0.7047, 0.5090, 0.5485, 0.6310],\n",
       "         [0.2782, 0.5785, 0.3825, 0.4589, 0.4014, 0.3600, 0.3639, 0.3762,\n",
       "          0.4662, 0.2780, 0.4098, 0.3632, 0.4410, 0.5395, 0.5001, 0.2388,\n",
       "          0.5645, 0.4398, 0.3310, 0.5714],\n",
       "         [0.4098, 0.7050, 0.6064, 0.5410, 0.5385, 0.4901, 0.4709, 0.5577,\n",
       "          0.5455, 0.4096, 0.4541, 0.5717, 0.6311, 0.6754, 0.6468, 0.3407,\n",
       "          0.7058, 0.5067, 0.5571, 0.6330],\n",
       "         [0.4161, 0.6896, 0.5959, 0.5474, 0.5350, 0.4848, 0.4635, 0.5626,\n",
       "          0.5500, 0.4072, 0.4463, 0.5682, 0.6338, 0.6799, 0.6549, 0.3354,\n",
       "          0.7187, 0.4996, 0.5479, 0.6300],\n",
       "         [0.4089, 0.6317, 0.5148, 0.4934, 0.5026, 0.4470, 0.4309, 0.5552,\n",
       "          0.4632, 0.3835, 0.4467, 0.5559, 0.5595, 0.6039, 0.5620, 0.3240,\n",
       "          0.6938, 0.4303, 0.5156, 0.6073],\n",
       "         [0.3847, 0.6204, 0.4858, 0.5165, 0.4776, 0.4293, 0.3701, 0.5236,\n",
       "          0.4774, 0.3853, 0.3911, 0.5094, 0.5951, 0.6376, 0.5532, 0.2941,\n",
       "          0.7049, 0.4256, 0.4963, 0.5311],\n",
       "         [0.4150, 0.7126, 0.5837, 0.5522, 0.5416, 0.4802, 0.4692, 0.5598,\n",
       "          0.5478, 0.4004, 0.4671, 0.5599, 0.6334, 0.6815, 0.6426, 0.3367,\n",
       "          0.7234, 0.5095, 0.5491, 0.6464]],\n",
       "\n",
       "        [[0.5543, 0.4964, 0.6978, 0.4231, 0.4453, 0.6311, 0.4278, 0.4750,\n",
       "          0.3115, 0.4844, 0.3847, 0.4029, 0.5454, 0.3805, 0.5405, 0.4218,\n",
       "          0.4236, 0.5939, 0.6730, 0.2989],\n",
       "         [0.4769, 0.4959, 0.6786, 0.4217, 0.4699, 0.7206, 0.4835, 0.4222,\n",
       "          0.2879, 0.4945, 0.4594, 0.4410, 0.6578, 0.4107, 0.4725, 0.4324,\n",
       "          0.5007, 0.6007, 0.7060, 0.2980],\n",
       "         [0.5018, 0.4396, 0.7389, 0.4774, 0.4395, 0.6882, 0.4807, 0.4765,\n",
       "          0.3618, 0.5937, 0.3979, 0.3836, 0.5888, 0.3485, 0.4894, 0.4984,\n",
       "          0.4358, 0.6181, 0.7178, 0.2614],\n",
       "         [0.5236, 0.4706, 0.7361, 0.4407, 0.5020, 0.6861, 0.4905, 0.4871,\n",
       "          0.3296, 0.5647, 0.3830, 0.4449, 0.6050, 0.3720, 0.5286, 0.4706,\n",
       "          0.4629, 0.5914, 0.7233, 0.2880],\n",
       "         [0.3875, 0.3588, 0.5642, 0.2978, 0.2839, 0.5547, 0.3274, 0.3260,\n",
       "          0.1788, 0.3429, 0.3355, 0.3478, 0.4971, 0.3608, 0.4209, 0.2473,\n",
       "          0.3982, 0.4681, 0.5262, 0.2232],\n",
       "         [0.4496, 0.4019, 0.5528, 0.3506, 0.4214, 0.4935, 0.4151, 0.4471,\n",
       "          0.1590, 0.4270, 0.2619, 0.4198, 0.4120, 0.3428, 0.4404, 0.3476,\n",
       "          0.3283, 0.4205, 0.5594, 0.2765],\n",
       "         [0.3764, 0.4023, 0.5027, 0.2822, 0.3430, 0.5233, 0.3385, 0.3237,\n",
       "          0.2279, 0.3750, 0.3690, 0.3915, 0.4506, 0.3054, 0.3990, 0.2832,\n",
       "          0.3628, 0.4197, 0.4889, 0.2705],\n",
       "         [0.5642, 0.5308, 0.7866, 0.4978, 0.5206, 0.7441, 0.5242, 0.5208,\n",
       "          0.3456, 0.6046, 0.4622, 0.4493, 0.6690, 0.4304, 0.5522, 0.5090,\n",
       "          0.4980, 0.6499, 0.7718, 0.3103],\n",
       "         [0.4150, 0.3117, 0.6553, 0.3604, 0.2675, 0.5549, 0.3385, 0.3963,\n",
       "          0.2471, 0.4525, 0.2846, 0.3201, 0.4459, 0.3144, 0.4702, 0.3119,\n",
       "          0.3589, 0.5035, 0.5558, 0.1942],\n",
       "         [0.4842, 0.4646, 0.7597, 0.4588, 0.4559, 0.6439, 0.4567, 0.5018,\n",
       "          0.3181, 0.5793, 0.4112, 0.3820, 0.6035, 0.3913, 0.4993, 0.4780,\n",
       "          0.4706, 0.5911, 0.7143, 0.2537]],\n",
       "\n",
       "        [[0.6431, 0.6586, 0.4676, 0.3743, 0.6070, 0.5760, 0.3719, 0.4918,\n",
       "          0.4669, 0.4347, 0.4356, 0.5324, 0.4643, 0.4608, 0.4353, 0.3668,\n",
       "          0.4884, 0.5803, 0.5041, 0.4944],\n",
       "         [0.6638, 0.6459, 0.5208, 0.4548, 0.5067, 0.4730, 0.4163, 0.3853,\n",
       "          0.4056, 0.3095, 0.4411, 0.5242, 0.3977, 0.3476, 0.3423, 0.2976,\n",
       "          0.4764, 0.4684, 0.4965, 0.4252],\n",
       "         [0.7602, 0.7699, 0.5754, 0.4949, 0.6051, 0.6096, 0.4716, 0.5197,\n",
       "          0.5156, 0.4460, 0.5148, 0.6473, 0.4845, 0.5035, 0.5043, 0.4229,\n",
       "          0.5066, 0.5951, 0.5323, 0.5051],\n",
       "         [0.7659, 0.7766, 0.5859, 0.5017, 0.6163, 0.6079, 0.4770, 0.5139,\n",
       "          0.5174, 0.4393, 0.5212, 0.6448, 0.4795, 0.4949, 0.4944, 0.4168,\n",
       "          0.5232, 0.6065, 0.5379, 0.5100],\n",
       "         [0.7589, 0.7673, 0.5619, 0.5023, 0.6162, 0.6110, 0.4809, 0.5136,\n",
       "          0.5117, 0.4568, 0.5179, 0.6498, 0.4787, 0.5319, 0.5165, 0.4280,\n",
       "          0.4953, 0.6050, 0.5261, 0.5063],\n",
       "         [0.6503, 0.6832, 0.5232, 0.4237, 0.5067, 0.5260, 0.3919, 0.4414,\n",
       "          0.4857, 0.3886, 0.4480, 0.5230, 0.3988, 0.4159, 0.4890, 0.3752,\n",
       "          0.4472, 0.5717, 0.4383, 0.4012],\n",
       "         [0.7575, 0.7589, 0.5533, 0.4843, 0.5991, 0.5967, 0.4561, 0.5128,\n",
       "          0.5179, 0.4642, 0.5045, 0.6357, 0.4767, 0.5079, 0.5126, 0.4291,\n",
       "          0.4929, 0.6088, 0.5282, 0.4899],\n",
       "         [0.6558, 0.7223, 0.5244, 0.4868, 0.5973, 0.5146, 0.4609, 0.4373,\n",
       "          0.4470, 0.4070, 0.5028, 0.6228, 0.4047, 0.4532, 0.4608, 0.4020,\n",
       "          0.4258, 0.5943, 0.4937, 0.4757],\n",
       "         [0.5771, 0.5369, 0.3371, 0.3260, 0.4926, 0.4938, 0.3088, 0.4196,\n",
       "          0.3967, 0.4368, 0.3608, 0.4313, 0.4201, 0.4570, 0.4062, 0.3194,\n",
       "          0.3840, 0.4934, 0.4341, 0.4019],\n",
       "         [0.5779, 0.5469, 0.3467, 0.3329, 0.5051, 0.5021, 0.3173, 0.4166,\n",
       "          0.3963, 0.4310, 0.3641, 0.4310, 0.4255, 0.4617, 0.4164, 0.3190,\n",
       "          0.3955, 0.5044, 0.4457, 0.4090]],\n",
       "\n",
       "        [[0.2184, 0.2629, 0.2472, 0.2763, 0.1422, 0.1657, 0.2161, 0.3088,\n",
       "          0.2673, 0.1456, 0.1822, 0.1386, 0.2881, 0.2705, 0.1757, 0.2221,\n",
       "          0.1797, 0.2150, 0.2647, 0.1363],\n",
       "         [0.5098, 0.3601, 0.4021, 0.5330, 0.4242, 0.3417, 0.5445, 0.5371,\n",
       "          0.5918, 0.3971, 0.4064, 0.2839, 0.6374, 0.4608, 0.3625, 0.2906,\n",
       "          0.3817, 0.3910, 0.4432, 0.4894],\n",
       "         [0.3196, 0.3495, 0.3011, 0.4164, 0.3237, 0.3426, 0.3716, 0.4976,\n",
       "          0.4831, 0.3345, 0.4354, 0.3225, 0.5209, 0.3970, 0.2542, 0.3076,\n",
       "          0.3073, 0.3156, 0.3248, 0.4584],\n",
       "         [0.4970, 0.3477, 0.3814, 0.5014, 0.4458, 0.3463, 0.5691, 0.5097,\n",
       "          0.5810, 0.4435, 0.4553, 0.3549, 0.7161, 0.4386, 0.3502, 0.2781,\n",
       "          0.3550, 0.4006, 0.3663, 0.5391],\n",
       "         [0.4954, 0.3482, 0.3816, 0.4904, 0.4442, 0.3402, 0.5637, 0.5108,\n",
       "          0.5703, 0.4388, 0.4594, 0.3492, 0.7134, 0.4381, 0.3528, 0.2876,\n",
       "          0.3687, 0.3926, 0.3635, 0.5277],\n",
       "         [0.5097, 0.4491, 0.4089, 0.5768, 0.4653, 0.3902, 0.5655, 0.6192,\n",
       "          0.6408, 0.4631, 0.4823, 0.3526, 0.7441, 0.4909, 0.4018, 0.3627,\n",
       "          0.4052, 0.3990, 0.4451, 0.5648],\n",
       "         [0.4660, 0.4287, 0.3504, 0.4527, 0.3487, 0.3435, 0.4982, 0.5396,\n",
       "          0.5239, 0.3736, 0.3474, 0.2765, 0.6455, 0.3867, 0.3907, 0.3422,\n",
       "          0.3804, 0.3201, 0.4279, 0.4432],\n",
       "         [0.4759, 0.4375, 0.3814, 0.4538, 0.3644, 0.3566, 0.4931, 0.5219,\n",
       "          0.5201, 0.3815, 0.3671, 0.2951, 0.6406, 0.4053, 0.3700, 0.3157,\n",
       "          0.3797, 0.3116, 0.4040, 0.4451],\n",
       "         [0.5198, 0.4455, 0.4224, 0.5567, 0.4590, 0.3913, 0.5729, 0.5969,\n",
       "          0.6358, 0.4582, 0.4889, 0.3633, 0.7610, 0.4796, 0.3966, 0.3549,\n",
       "          0.3980, 0.3804, 0.4479, 0.5636],\n",
       "         [0.4039, 0.3501, 0.3408, 0.4936, 0.4086, 0.3521, 0.3579, 0.4964,\n",
       "          0.5172, 0.3595, 0.4656, 0.3332, 0.5707, 0.4699, 0.2675, 0.2825,\n",
       "          0.3817, 0.3299, 0.2677, 0.4117]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        # We assume that the queries, keys, features all have the same feature size.\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries)\n",
    "        K = self.key_tfm(keys)\n",
    "        V = self.value_tfm(values)\n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        \n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0784e-01,  2.2152e-01,  4.5362e-01, -1.5373e-01,  1.2132e-01,\n",
       "           2.8852e-01,  9.0924e-02, -7.1280e-02,  4.8409e-01, -3.6644e-01,\n",
       "           8.2507e-02, -1.2756e-01,  2.3375e-01, -1.1124e-01,  2.2679e-01,\n",
       "          -4.7546e-01, -3.7602e-01,  4.1561e-02,  1.3608e-01, -3.1414e-02],\n",
       "         [-1.9894e-01,  1.9652e-01,  3.8503e-01, -1.2525e-01,  1.0525e-01,\n",
       "           2.5120e-01,  1.0919e-01, -3.3621e-02,  4.4273e-01, -2.8875e-01,\n",
       "           5.2675e-02, -1.1692e-01,  1.9413e-01, -1.1617e-01,  2.1346e-01,\n",
       "          -3.8980e-01, -3.7404e-01,  6.3505e-03,  1.6117e-01, -5.6203e-02],\n",
       "         [-2.0458e-01,  2.2366e-01,  4.5607e-01, -1.5232e-01,  1.2111e-01,\n",
       "           2.9050e-01,  8.6029e-02, -6.8638e-02,  4.8368e-01, -3.7043e-01,\n",
       "           8.1905e-02, -1.2623e-01,  2.2998e-01, -1.1342e-01,  2.2555e-01,\n",
       "          -4.7172e-01, -3.7471e-01,  4.6374e-02,  1.3519e-01, -2.6267e-02],\n",
       "         [-2.0544e-01,  2.2270e-01,  4.5883e-01, -1.5014e-01,  1.1762e-01,\n",
       "           2.8600e-01,  8.8238e-02, -6.5480e-02,  4.8697e-01, -3.7176e-01,\n",
       "           8.0421e-02, -1.2621e-01,  2.3080e-01, -1.1149e-01,  2.2663e-01,\n",
       "          -4.6857e-01, -3.7417e-01,  4.4071e-02,  1.3546e-01, -2.8706e-02],\n",
       "         [-2.0667e-01,  2.2220e-01,  4.5550e-01, -1.5159e-01,  1.2175e-01,\n",
       "           2.9008e-01,  8.9128e-02, -7.0521e-02,  4.8482e-01, -3.7029e-01,\n",
       "           8.2480e-02, -1.2507e-01,  2.3136e-01, -1.1198e-01,  2.2548e-01,\n",
       "          -4.7346e-01, -3.7417e-01,  4.5193e-02,  1.3668e-01, -2.9144e-02],\n",
       "         [-2.0810e-01,  1.9436e-01,  4.0396e-01, -1.3872e-01,  1.0172e-01,\n",
       "           2.2851e-01,  8.4039e-02, -7.5665e-02,  4.1680e-01, -3.2275e-01,\n",
       "           1.0176e-01, -1.2383e-01,  2.3867e-01, -7.9795e-02,  2.0156e-01,\n",
       "          -4.7125e-01, -3.2672e-01,  1.2151e-02,  1.1692e-01, -3.5321e-02],\n",
       "         [-1.7271e-01,  1.6425e-01,  3.3550e-01, -8.5855e-02,  1.0779e-01,\n",
       "           2.2564e-01,  1.1317e-01, -4.0034e-02,  4.0776e-01, -2.8288e-01,\n",
       "           3.5568e-02, -7.2597e-02,  1.3719e-01, -1.1455e-01,  1.9706e-01,\n",
       "          -3.2414e-01, -3.2948e-01,  1.5785e-02,  1.6673e-01, -2.8746e-02],\n",
       "         [-1.6227e-01,  2.1108e-01,  4.4477e-01, -1.1974e-01,  8.7049e-02,\n",
       "           2.3587e-01,  7.4098e-02, -1.9351e-02,  4.6529e-01, -3.5548e-01,\n",
       "           5.5737e-02, -1.3420e-01,  1.9572e-01, -1.0894e-01,  2.0906e-01,\n",
       "          -3.9666e-01, -3.4817e-01,  5.0564e-02,  1.1216e-01, -1.8331e-02],\n",
       "         [-1.6387e-01,  1.2624e-01,  3.3421e-01, -7.2633e-02,  6.9826e-02,\n",
       "           1.6365e-01,  8.7126e-02, -6.3419e-02,  3.4406e-01, -2.8379e-01,\n",
       "           9.0759e-02, -6.3500e-02,  1.9710e-01, -3.4995e-02,  1.4260e-01,\n",
       "          -3.6001e-01, -2.2475e-01,  3.4158e-02,  1.1218e-01, -5.1234e-02],\n",
       "         [-1.3960e-01,  1.9867e-01,  3.7357e-01, -8.1054e-02,  5.2306e-02,\n",
       "           1.6824e-01,  5.4291e-02, -1.6083e-02,  3.6371e-01, -2.7958e-01,\n",
       "           7.0861e-02, -1.1340e-01,  1.8961e-01, -7.6868e-02,  1.2143e-01,\n",
       "          -3.3538e-01, -2.4296e-01,  8.5945e-02,  8.5538e-02, -5.4852e-02]],\n",
       "\n",
       "        [[-2.4678e-01,  1.9175e-01,  3.1953e-01, -4.5059e-02,  9.9997e-02,\n",
       "           1.9817e-01,  1.6534e-01,  7.8913e-02,  2.7819e-01, -2.3323e-01,\n",
       "           1.9229e-01, -5.0553e-02,  1.6940e-01, -3.3515e-02,  8.2495e-02,\n",
       "          -4.3622e-01, -2.8899e-01,  8.2251e-02,  2.5189e-01, -1.0318e-01],\n",
       "         [-3.0736e-01,  1.5747e-01,  4.5038e-01, -5.5493e-02,  2.1537e-01,\n",
       "           2.5856e-01,  1.5853e-01,  2.5409e-02,  3.8485e-01, -3.4030e-01,\n",
       "           2.1513e-01, -7.0898e-02,  2.8758e-01, -4.8669e-02,  1.1235e-01,\n",
       "          -5.5230e-01, -3.9654e-01,  1.0147e-01,  2.9915e-01, -1.9543e-01],\n",
       "         [-2.6192e-01,  1.0789e-01,  4.2037e-01, -5.0534e-02,  1.8866e-01,\n",
       "           2.2490e-01,  1.4144e-01,  4.5461e-02,  2.9560e-01, -3.0369e-01,\n",
       "           2.0674e-01, -9.2471e-02,  2.4769e-01, -5.6132e-02,  7.5779e-02,\n",
       "          -4.9150e-01, -3.7346e-01,  1.0376e-01,  2.6288e-01, -1.7830e-01],\n",
       "         [-2.7903e-01,  1.7736e-01,  4.1165e-01, -3.8968e-02,  2.0049e-01,\n",
       "           2.0281e-01,  1.7261e-01,  1.5476e-02,  3.8230e-01, -3.0610e-01,\n",
       "           1.9462e-01, -6.1160e-02,  2.6331e-01, -6.7213e-02,  1.2316e-01,\n",
       "          -5.2968e-01, -3.6264e-01,  7.2962e-02,  2.7836e-01, -1.6333e-01],\n",
       "         [-2.6652e-01,  1.1024e-01,  3.9320e-01, -6.1148e-02,  2.0254e-01,\n",
       "           2.3086e-01,  1.1472e-01, -4.7058e-03,  2.9925e-01, -2.9392e-01,\n",
       "           1.8613e-01, -5.5278e-02,  2.5064e-01, -5.0338e-02,  9.3285e-02,\n",
       "          -4.6991e-01, -3.5901e-01,  8.4699e-02,  2.5008e-01, -1.6982e-01],\n",
       "         [-2.7623e-01,  1.2869e-01,  4.0363e-01, -1.1371e-02,  1.8881e-01,\n",
       "           2.0733e-01,  1.4695e-01,  2.2418e-02,  3.7706e-01, -3.3029e-01,\n",
       "           1.8643e-01, -6.8042e-02,  2.6122e-01, -3.3852e-02,  1.1259e-01,\n",
       "          -4.8123e-01, -3.5349e-01,  9.5802e-02,  2.9257e-01, -1.8197e-01],\n",
       "         [-2.6484e-01,  1.2468e-01,  4.0930e-01, -5.8739e-02,  2.3297e-01,\n",
       "           2.7167e-01,  9.3636e-02,  1.8576e-02,  3.2333e-01, -3.0397e-01,\n",
       "           2.0912e-01, -5.3064e-02,  2.7065e-01, -5.2003e-02,  7.8250e-02,\n",
       "          -5.1273e-01, -3.5389e-01,  9.5311e-02,  2.5866e-01, -1.7393e-01],\n",
       "         [-2.7598e-01,  1.3148e-01,  4.0335e-01, -1.2639e-02,  1.9407e-01,\n",
       "           2.0840e-01,  1.4511e-01,  1.8877e-02,  3.7891e-01, -3.3056e-01,\n",
       "           1.8681e-01, -6.9182e-02,  2.6348e-01, -3.5946e-02,  1.1223e-01,\n",
       "          -4.8772e-01, -3.5439e-01,  9.7364e-02,  2.9222e-01, -1.8172e-01],\n",
       "         [-2.7634e-01,  1.2897e-01,  4.0530e-01, -1.0610e-02,  1.9022e-01,\n",
       "           2.0785e-01,  1.4740e-01,  2.3303e-02,  3.7856e-01, -3.3120e-01,\n",
       "           1.8787e-01, -6.8081e-02,  2.6274e-01, -3.4311e-02,  1.1278e-01,\n",
       "          -4.8382e-01, -3.5422e-01,  9.5479e-02,  2.9338e-01, -1.8233e-01],\n",
       "         [-1.9924e-01,  9.7667e-02,  2.9630e-01, -6.6045e-02,  1.2341e-01,\n",
       "           1.9040e-01,  1.0218e-01,  2.5663e-02,  1.8735e-01, -2.1682e-01,\n",
       "           1.5339e-01, -8.7855e-02,  1.4924e-01, -4.4566e-02,  5.1175e-02,\n",
       "          -3.5256e-01, -2.9421e-01,  1.0255e-01,  1.8515e-01, -1.2170e-01]],\n",
       "\n",
       "        [[-2.1612e-01,  1.5286e-01,  4.1247e-01, -1.4471e-01,  2.8223e-01,\n",
       "           3.9847e-01, -4.5775e-02, -4.4641e-02,  3.1179e-01, -2.8614e-01,\n",
       "           2.6891e-01, -4.6096e-02,  2.5999e-01, -5.5191e-02,  9.3939e-02,\n",
       "          -7.0505e-01, -3.3277e-01,  1.3783e-03,  1.2116e-01, -1.0143e-01],\n",
       "         [-2.1798e-01,  1.4861e-01,  4.1151e-01, -1.4757e-01,  2.8366e-01,\n",
       "           4.0242e-01, -4.5484e-02, -4.3132e-02,  3.0874e-01, -2.8608e-01,\n",
       "           2.7138e-01, -4.5971e-02,  2.6115e-01, -5.3094e-02,  9.4102e-02,\n",
       "          -7.0716e-01, -3.3504e-01,  8.2847e-04,  1.2538e-01, -1.0367e-01],\n",
       "         [-1.9503e-01,  1.5428e-01,  3.9774e-01, -1.2658e-01,  2.2369e-01,\n",
       "           3.2940e-01, -3.3632e-02, -4.8202e-02,  3.3171e-01, -2.8373e-01,\n",
       "           2.4797e-01, -2.1255e-02,  2.5620e-01, -2.3613e-02,  1.2042e-01,\n",
       "          -6.7451e-01, -2.8862e-01, -3.5749e-02,  9.3139e-02, -7.3964e-02],\n",
       "         [-2.0516e-01,  1.1177e-01,  2.9646e-01, -1.1744e-01,  2.6590e-01,\n",
       "           3.5151e-01, -1.3144e-02,  8.2546e-04,  1.9622e-01, -2.0045e-01,\n",
       "           2.2157e-01, -5.0094e-02,  1.5551e-01, -5.1618e-02,  2.6354e-02,\n",
       "          -5.6834e-01, -2.8267e-01,  2.7224e-02,  1.5125e-01, -1.4044e-01],\n",
       "         [-1.4101e-01,  1.1181e-01,  3.3083e-01, -8.6331e-02,  2.3767e-01,\n",
       "           3.0208e-01, -6.0063e-02, -1.9605e-02,  2.6028e-01, -2.2959e-01,\n",
       "           1.8419e-01, -9.6028e-02,  1.7320e-01, -1.0834e-01,  8.9337e-02,\n",
       "          -5.0653e-01, -2.7893e-01,  5.7714e-03,  7.7820e-02, -7.1059e-02],\n",
       "         [-2.1628e-01,  1.1463e-01,  3.3881e-01, -1.0367e-01,  2.0960e-01,\n",
       "           3.5724e-01, -2.7839e-02,  2.7748e-02,  1.9951e-01, -2.0931e-01,\n",
       "           2.3923e-01, -3.6069e-02,  1.6674e-01, -3.2775e-02,  2.3561e-02,\n",
       "          -5.6410e-01, -2.5646e-01, -1.5276e-03,  1.1565e-01, -1.2849e-01],\n",
       "         [-2.0195e-01,  1.1147e-01,  2.9452e-01, -1.1664e-01,  2.6458e-01,\n",
       "           3.4819e-01, -1.2320e-02, -9.4153e-05,  1.9557e-01, -1.9937e-01,\n",
       "           2.2008e-01, -4.9622e-02,  1.5612e-01, -5.0476e-02,  2.5823e-02,\n",
       "          -5.6453e-01, -2.8248e-01,  2.8098e-02,  1.5132e-01, -1.3893e-01],\n",
       "         [-1.7561e-01,  1.1846e-01,  3.8208e-01, -1.2274e-01,  2.3319e-01,\n",
       "           3.5177e-01, -3.8619e-02, -2.0198e-02,  2.5494e-01, -2.6465e-01,\n",
       "           2.5547e-01, -4.8831e-02,  2.3769e-01, -2.2863e-02,  7.5353e-02,\n",
       "          -6.1656e-01, -3.1978e-01,  2.6831e-03,  1.1358e-01, -8.9644e-02],\n",
       "         [-1.3596e-01,  1.1260e-01,  3.3093e-01, -8.6184e-02,  2.3455e-01,\n",
       "           2.9761e-01, -6.0430e-02, -2.3021e-02,  2.6362e-01, -2.3108e-01,\n",
       "           1.8371e-01, -9.3022e-02,  1.7650e-01, -1.0461e-01,  9.3418e-02,\n",
       "          -5.0612e-01, -2.7744e-01,  4.1093e-03,  7.3640e-02, -6.5527e-02],\n",
       "         [-2.1687e-01,  1.5019e-01,  4.1204e-01, -1.4351e-01,  2.8372e-01,\n",
       "           4.0065e-01, -4.6445e-02, -4.0334e-02,  3.0741e-01, -2.8561e-01,\n",
       "           2.6932e-01, -5.0154e-02,  2.5494e-01, -5.7339e-02,  9.2294e-02,\n",
       "          -7.0272e-01, -3.3465e-01,  2.2195e-03,  1.2220e-01, -1.0425e-01]],\n",
       "\n",
       "        [[-1.4411e-01,  1.6280e-01,  3.8957e-01, -1.1692e-01,  1.8305e-01,\n",
       "           2.2444e-01,  5.8639e-02, -1.7510e-01,  4.0149e-01, -3.5187e-01,\n",
       "           1.5534e-01, -4.5212e-02,  3.3676e-01, -1.2987e-02,  2.1094e-01,\n",
       "          -5.6792e-01, -4.1063e-01, -3.6067e-02,  1.6067e-01,  2.2146e-02],\n",
       "         [-1.6158e-01,  1.7284e-01,  4.3932e-01, -1.4385e-01,  1.7873e-01,\n",
       "           2.4412e-01,  6.7791e-02, -1.3280e-01,  4.2294e-01, -3.6190e-01,\n",
       "           1.8349e-01, -9.0516e-02,  3.6717e-01, -5.5676e-03,  2.2748e-01,\n",
       "          -6.2633e-01, -4.6701e-01, -2.8974e-02,  1.7030e-01, -1.9545e-02],\n",
       "         [-1.3873e-01,  1.7126e-01,  3.7449e-01, -1.2415e-01,  1.5588e-01,\n",
       "           2.0088e-01,  8.2499e-02, -1.6282e-01,  3.9025e-01, -3.1897e-01,\n",
       "           1.5824e-01, -1.0750e-01,  3.2548e-01,  6.5268e-03,  2.0530e-01,\n",
       "          -5.9502e-01, -4.0132e-01, -2.2522e-02,  1.2269e-01, -2.0614e-02],\n",
       "         [-1.6292e-01,  1.7073e-01,  4.3609e-01, -1.4387e-01,  1.8491e-01,\n",
       "           2.4817e-01,  6.4532e-02, -1.3437e-01,  4.2342e-01, -3.6050e-01,\n",
       "           1.8320e-01, -9.0906e-02,  3.6762e-01, -8.1033e-03,  2.2744e-01,\n",
       "          -6.2465e-01, -4.6571e-01, -2.8225e-02,  1.7036e-01, -2.1667e-02],\n",
       "         [-1.6207e-01,  1.7340e-01,  4.4120e-01, -1.4271e-01,  1.7596e-01,\n",
       "           2.4142e-01,  7.1035e-02, -1.3025e-01,  4.2310e-01, -3.6179e-01,\n",
       "           1.8492e-01, -8.9444e-02,  3.6804e-01, -3.2490e-03,  2.2742e-01,\n",
       "          -6.2927e-01, -4.6804e-01, -3.0202e-02,  1.7243e-01, -2.0870e-02],\n",
       "         [-1.6178e-01,  1.7312e-01,  4.3821e-01, -1.4192e-01,  1.7850e-01,\n",
       "           2.4300e-01,  6.9627e-02, -1.3607e-01,  4.2533e-01, -3.6196e-01,\n",
       "           1.8242e-01, -9.1560e-02,  3.6752e-01, -4.4441e-03,  2.2750e-01,\n",
       "          -6.2811e-01, -4.6499e-01, -3.0233e-02,  1.6964e-01, -2.0707e-02],\n",
       "         [-1.6719e-01,  1.3304e-01,  3.6163e-01, -1.3371e-01,  2.0733e-01,\n",
       "           2.3467e-01,  7.3716e-02, -1.0920e-01,  3.7253e-01, -2.8912e-01,\n",
       "           1.7405e-01, -9.7507e-02,  3.3556e-01, -2.5692e-02,  2.1346e-01,\n",
       "          -5.6298e-01, -4.3625e-01, -3.6875e-02,  1.6875e-01, -5.5255e-02],\n",
       "         [-1.6167e-01,  1.7455e-01,  4.3982e-01, -1.4237e-01,  1.7734e-01,\n",
       "           2.4218e-01,  7.1413e-02, -1.3550e-01,  4.2579e-01, -3.6458e-01,\n",
       "           1.8448e-01, -8.7268e-02,  3.6767e-01, -1.1855e-03,  2.2853e-01,\n",
       "          -6.3192e-01, -4.6503e-01, -3.0502e-02,  1.7156e-01, -1.8182e-02],\n",
       "         [-1.2543e-01,  1.3617e-01,  4.0007e-01, -1.1025e-01,  7.1705e-02,\n",
       "           1.9530e-01,  3.3494e-02, -6.5279e-02,  3.2070e-01, -3.1801e-01,\n",
       "           1.4989e-01, -7.2321e-02,  2.7386e-01, -8.5249e-04,  1.5779e-01,\n",
       "          -4.5474e-01, -3.5634e-01, -2.3906e-02,  1.3142e-01,  2.8326e-03],\n",
       "         [-1.6170e-01,  1.7223e-01,  4.3847e-01, -1.4479e-01,  1.8116e-01,\n",
       "           2.4811e-01,  6.3382e-02, -1.3453e-01,  4.2219e-01, -3.6371e-01,\n",
       "           1.8407e-01, -8.9709e-02,  3.6578e-01, -6.7587e-03,  2.2654e-01,\n",
       "          -6.2458e-01, -4.6392e-01, -2.6784e-02,  1.6872e-01, -1.7763e-02]],\n",
       "\n",
       "        [[-2.0547e-01,  1.5903e-01,  4.5064e-01, -4.9773e-02,  1.0826e-01,\n",
       "           1.7155e-01,  1.6481e-01,  2.6947e-02,  4.8421e-01, -3.1966e-01,\n",
       "           1.8586e-01, -1.1393e-01,  2.8181e-01,  2.2585e-02,  2.0513e-01,\n",
       "          -5.1955e-01, -4.0185e-01,  9.9163e-03,  1.1538e-01, -1.9627e-01],\n",
       "         [-1.5416e-01,  1.1588e-01,  3.9660e-01, -5.3372e-02,  6.8407e-02,\n",
       "           1.6907e-01,  1.1957e-01,  1.2458e-03,  4.4314e-01, -2.8546e-01,\n",
       "           1.3559e-01, -8.5400e-02,  2.6221e-01,  5.5766e-02,  2.0410e-01,\n",
       "          -3.9494e-01, -3.3093e-01, -4.9649e-03,  5.7969e-02, -1.6413e-01],\n",
       "         [-2.2611e-01,  1.7092e-01,  5.2117e-01, -5.4958e-02,  1.3489e-01,\n",
       "           1.9076e-01,  1.3228e-01,  6.6948e-04,  5.3465e-01, -3.7150e-01,\n",
       "           2.0759e-01, -1.0441e-01,  3.2611e-01,  1.7789e-02,  2.1455e-01,\n",
       "          -5.7647e-01, -3.9431e-01,  3.1859e-02,  8.1373e-02, -2.2332e-01],\n",
       "         [-2.2649e-01,  1.7305e-01,  5.2246e-01, -5.5689e-02,  1.3343e-01,\n",
       "           1.9297e-01,  1.2816e-01,  2.2158e-03,  5.3577e-01, -3.7190e-01,\n",
       "           2.0924e-01, -1.0399e-01,  3.2683e-01,  1.7199e-02,  2.1451e-01,\n",
       "          -5.7687e-01, -3.9047e-01,  3.5521e-02,  7.9765e-02, -2.2327e-01],\n",
       "         [-2.2888e-01,  1.5469e-01,  4.6366e-01, -5.6813e-02,  1.3375e-01,\n",
       "           1.6221e-01,  8.3643e-02, -7.1611e-03,  4.4843e-01, -3.2367e-01,\n",
       "           2.0720e-01, -7.5048e-02,  3.0835e-01,  1.2388e-02,  1.8586e-01,\n",
       "          -5.3729e-01, -3.3862e-01,  3.8570e-02,  7.0913e-02, -2.1016e-01],\n",
       "         [-1.8301e-01,  1.0387e-01,  4.4156e-01, -6.2375e-02,  1.3753e-01,\n",
       "           1.6687e-01,  1.2583e-01, -5.2946e-02,  4.7961e-01, -3.4552e-01,\n",
       "           1.4308e-01, -1.0119e-01,  2.8561e-01,  2.6188e-02,  1.7840e-01,\n",
       "          -4.6990e-01, -2.9072e-01,  2.3182e-02,  1.5059e-02, -2.0542e-01],\n",
       "         [-1.8348e-01,  1.2273e-01,  4.2710e-01, -5.9948e-02,  9.8305e-02,\n",
       "           1.5832e-01,  1.2365e-01, -1.2267e-02,  4.4328e-01, -3.0670e-01,\n",
       "           1.5274e-01, -1.1986e-01,  2.4968e-01,  8.4502e-03,  1.7842e-01,\n",
       "          -4.4156e-01, -2.8462e-01,  1.0809e-02,  7.6347e-03, -1.9525e-01],\n",
       "         [-2.1025e-01,  1.7707e-01,  3.9529e-01, -4.3813e-02,  1.3247e-01,\n",
       "           1.4693e-01,  7.7606e-02, -7.8593e-04,  4.1853e-01, -2.7963e-01,\n",
       "           1.5513e-01, -5.4560e-02,  2.4995e-01, -7.8994e-03,  1.6461e-01,\n",
       "          -4.6797e-01, -3.1727e-01,  3.9575e-02,  9.4118e-02, -1.8089e-01],\n",
       "         [-2.0705e-01,  1.9650e-01,  4.4949e-01, -3.9750e-02,  1.3247e-01,\n",
       "           1.7557e-01,  1.2785e-01,  9.5681e-03,  5.0551e-01, -3.2553e-01,\n",
       "           1.5390e-01, -8.2490e-02,  2.6423e-01, -3.3202e-03,  1.9403e-01,\n",
       "          -5.0398e-01, -3.7479e-01,  3.1928e-02,  1.0861e-01, -1.9129e-01],\n",
       "         [-2.2483e-01,  1.7103e-01,  5.2100e-01, -5.7002e-02,  1.3204e-01,\n",
       "           1.9458e-01,  1.3111e-01,  2.1288e-03,  5.3688e-01, -3.7206e-01,\n",
       "           2.0712e-01, -1.0538e-01,  3.2640e-01,  2.0225e-02,  2.1664e-01,\n",
       "          -5.7423e-01, -3.9400e-01,  3.3671e-02,  8.0737e-02, -2.2282e-01]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi head attention block applies multiple attention heads as can be seen in the paper \"Attention is all you need\", then concatenates the output and applies single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert d_model == d_feature * n_heads\n",
    "        \n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"Output of single head\")\n",
    "        \n",
    "        #reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Sequence, D_Feature * n_heads)\n",
    "        log_size(x, \"Concatenated output\") \n",
    "        x = self.projection(x) # (Batch, Sequence, D_model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2458e-01, -4.8857e-01,  6.1657e-02,  ...,  4.4601e-01,\n",
       "          -5.2292e-02, -4.9479e-02],\n",
       "         [ 1.1982e-01, -4.7597e-01,  8.6063e-02,  ...,  4.6261e-01,\n",
       "          -9.0486e-02, -3.3123e-02],\n",
       "         [ 1.3382e-01, -4.6789e-01,  8.6461e-02,  ...,  4.5981e-01,\n",
       "          -5.1542e-02, -3.1109e-04],\n",
       "         ...,\n",
       "         [ 1.2216e-01, -4.7918e-01,  8.1664e-02,  ...,  4.2958e-01,\n",
       "          -6.2201e-02, -3.9964e-02],\n",
       "         [ 9.1237e-02, -4.5377e-01,  8.3547e-02,  ...,  4.7651e-01,\n",
       "          -7.7586e-02, -8.5860e-02],\n",
       "         [ 5.9553e-02, -4.2683e-01,  9.0134e-02,  ...,  4.5960e-01,\n",
       "          -8.3119e-02, -9.4775e-02]],\n",
       "\n",
       "        [[ 1.6726e-01, -4.6402e-01,  4.6791e-02,  ...,  4.1466e-01,\n",
       "          -1.0124e-01, -1.1301e-01],\n",
       "         [ 1.9437e-01, -4.9319e-01, -1.1182e-02,  ...,  4.1176e-01,\n",
       "          -1.3245e-01, -1.0238e-01],\n",
       "         [ 1.8208e-01, -4.8433e-01,  8.1616e-03,  ...,  4.2897e-01,\n",
       "          -1.4379e-01, -1.2226e-01],\n",
       "         ...,\n",
       "         [ 1.7532e-01, -4.3649e-01,  7.3908e-03,  ...,  4.2335e-01,\n",
       "          -1.0909e-01, -9.5354e-02],\n",
       "         [ 1.5914e-01, -4.6075e-01,  1.3130e-02,  ...,  4.0930e-01,\n",
       "          -1.3618e-01, -1.2348e-01],\n",
       "         [ 1.4374e-01, -4.2536e-01,  1.2319e-02,  ...,  4.0827e-01,\n",
       "          -1.4326e-01, -9.1459e-02]],\n",
       "\n",
       "        [[ 1.3034e-01, -4.2271e-01,  8.7867e-02,  ...,  4.1319e-01,\n",
       "          -1.5282e-01, -1.4576e-01],\n",
       "         [ 1.1537e-01, -3.8510e-01,  1.0078e-01,  ...,  3.7624e-01,\n",
       "          -1.4724e-01, -1.1417e-01],\n",
       "         [ 1.5002e-01, -4.5291e-01,  7.8571e-02,  ...,  4.0049e-01,\n",
       "          -1.8053e-01, -1.0235e-01],\n",
       "         ...,\n",
       "         [ 1.2489e-01, -4.6079e-01,  9.3492e-02,  ...,  4.2600e-01,\n",
       "          -1.8655e-01, -1.2299e-01],\n",
       "         [ 1.3979e-01, -4.2526e-01,  6.7030e-02,  ...,  3.8991e-01,\n",
       "          -1.6297e-01, -1.2582e-01],\n",
       "         [ 1.3917e-01, -4.6549e-01,  9.0976e-02,  ...,  3.7312e-01,\n",
       "          -1.7080e-01, -1.0625e-01]],\n",
       "\n",
       "        [[ 1.3284e-01, -4.6878e-01, -1.1619e-02,  ...,  4.4214e-01,\n",
       "          -1.2506e-01, -1.3041e-01],\n",
       "         [ 1.1835e-01, -4.6251e-01,  7.1807e-03,  ...,  4.5889e-01,\n",
       "          -1.1204e-01, -1.2358e-01],\n",
       "         [ 1.2361e-01, -4.6830e-01,  7.1961e-03,  ...,  4.5005e-01,\n",
       "          -1.2037e-01, -1.1086e-01],\n",
       "         ...,\n",
       "         [ 1.0318e-01, -4.6819e-01,  7.2835e-03,  ...,  4.5604e-01,\n",
       "          -1.0954e-01, -1.2869e-01],\n",
       "         [ 8.7027e-02, -4.7269e-01,  3.8695e-02,  ...,  4.7223e-01,\n",
       "          -1.4538e-01, -1.4453e-01],\n",
       "         [ 1.2094e-01, -4.1146e-01, -1.4985e-02,  ...,  4.4512e-01,\n",
       "          -8.7263e-02, -1.0609e-01]],\n",
       "\n",
       "        [[ 1.2731e-01, -3.8115e-01,  5.3437e-02,  ...,  3.7403e-01,\n",
       "          -1.0251e-01, -1.0555e-01],\n",
       "         [ 1.0733e-01, -3.7459e-01,  4.5311e-02,  ...,  3.5679e-01,\n",
       "          -1.3123e-01, -1.0456e-01],\n",
       "         [ 1.1500e-01, -4.4451e-01,  6.6600e-02,  ...,  3.7855e-01,\n",
       "          -1.5504e-01, -9.7174e-02],\n",
       "         ...,\n",
       "         [ 1.1268e-01, -4.2692e-01,  7.7673e-02,  ...,  3.6004e-01,\n",
       "          -1.5098e-01, -6.6174e-02],\n",
       "         [ 1.2586e-01, -4.1318e-01,  5.8572e-02,  ...,  3.8369e-01,\n",
       "          -1.5356e-01, -1.0828e-01],\n",
       "         [ 9.5371e-02, -3.8831e-01,  5.9763e-02,  ...,  3.2556e-01,\n",
       "          -1.8960e-01, -1.1986e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is made up of the following components:\n",
    "- multi-head attention block\n",
    "- simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps =1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norml = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        attn = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention Output\")\n",
    "        # Applying normalization and residual connection.\n",
    "        x = x + self.dropout(self.layer_norml(attn))\n",
    "        # Applying position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Applying normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention Output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5833,  3.1703,  2.7410,  ...,  0.8939, -1.1184, -1.0422],\n",
       "         [ 1.5682,  3.0529,  1.9326,  ..., -0.7414, -0.8055, -1.8851],\n",
       "         [ 1.4794,  1.9777,  2.1705,  ...,  0.3330, -1.0011, -0.2723],\n",
       "         ...,\n",
       "         [ 1.0954,  2.7196,  2.5938,  ..., -0.9390, -1.0696,  0.1681],\n",
       "         [ 2.5727,  2.4877,  3.5313,  ..., -0.8972, -1.5018, -0.5330],\n",
       "         [-0.7734,  2.0177,  2.0921,  ...,  0.1652, -1.1879, -0.7408]],\n",
       "\n",
       "        [[ 1.1904,  2.4746,  2.8082,  ..., -1.8894, -2.0657,  0.5893],\n",
       "         [ 3.3139,  2.7497,  3.2283,  ..., -0.9785, -1.8251, -0.8296],\n",
       "         [ 1.7955,  1.5634,  3.3610,  ..., -1.5514, -1.3444, -0.5784],\n",
       "         ...,\n",
       "         [ 2.3839,  1.9991,  2.3468,  ..., -1.4291, -1.9721, -0.6121],\n",
       "         [ 2.2030,  2.3667,  1.1972,  ..., -1.9345, -1.0124, -1.1355],\n",
       "         [ 1.3757,  2.1950,  2.9918,  ...,  1.1616, -0.6135, -1.0362]],\n",
       "\n",
       "        [[ 2.9574,  0.8282,  2.0989,  ..., -0.9618, -0.1570,  1.7864],\n",
       "         [ 2.0363,  2.1637,  2.5635,  ...,  1.4634, -0.2655, -0.5081],\n",
       "         [ 1.8068,  2.3578,  3.1065,  ...,  0.8333,  0.3452, -0.3033],\n",
       "         ...,\n",
       "         [ 2.0068,  1.7819,  1.3895,  ..., -1.0015, -1.0421, -0.2535],\n",
       "         [ 2.1575,  1.6365,  1.4483,  ..., -1.4676, -1.9847,  0.6146],\n",
       "         [ 2.7666,  2.0091,  1.8983,  ..., -1.5855, -2.1264, -0.0104]],\n",
       "\n",
       "        [[ 2.8595,  2.2707,  2.7314,  ...,  0.6400, -0.1836, -0.0976],\n",
       "         [ 1.7700,  2.3977,  2.7780,  ..., -1.1639, -2.0624, -1.0269],\n",
       "         [ 2.2764,  1.5781,  2.7080,  ..., -1.1635, -0.2407,  0.9552],\n",
       "         ...,\n",
       "         [ 1.8379,  2.9995,  1.7725,  ..., -1.2953, -0.9942, -1.0119],\n",
       "         [ 2.3332,  2.7262,  2.4253,  ..., -1.1045, -1.1469, -0.3429],\n",
       "         [-0.6546,  2.4438,  3.3590,  ..., -1.5814, -1.1159,  1.0383]],\n",
       "\n",
       "        [[ 1.7813,  2.5251,  2.8924,  ..., -0.8037, -1.2384, -0.9444],\n",
       "         [ 1.8858,  1.4595,  2.1702,  ..., -1.6412, -1.2282, -0.1167],\n",
       "         [ 2.0547,  1.9084,  3.2943,  ..., -1.0347, -0.3421, -1.1679],\n",
       "         ...,\n",
       "         [ 3.1325,  1.8256,  1.7638,  ..., -1.0272, -1.6079, -0.8695],\n",
       "         [-0.1063,  0.0354,  3.3744,  ..., -0.7720, -1.3139, -1.1946],\n",
       "         [ 1.7655,  1.5720,  2.8215,  ..., -1.6463, -1.1094, -1.0594]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder is having six consecutive encoder blocks, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks = 6, d_model = 512, n_heads = 8, d_ff = 2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([EncoderBlock(d_model = d_model, d_feature = d_model // n_heads, \n",
    "                                                   d_ff = f_ff, dropout = dropout)\n",
    "                                       for _ in range(n_blocks)\n",
    "                                      ])\n",
    "        \n",
    "        def forward(self, x: torch.FloatTensor, mask = None):\n",
    "            for encoder in self.encoders:\n",
    "                x = encoder(x)\n",
    "            return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is same in structure as the encoder with just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Applying attention to inputs\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Applying attention to the encoder outputs and outputs of the previous layer\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Applying position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention Output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'Concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4330,  1.2729,  0.6011,  ...,  1.9798, -0.1023,  3.0183],\n",
       "         [-1.9039,  0.9775,  1.4220,  ...,  1.6646, -0.1382,  3.1742],\n",
       "         [ 0.1228,  1.1531,  0.2262,  ...,  0.5965, -0.5110,  3.5058],\n",
       "         ...,\n",
       "         [-0.5174,  2.7846,  2.5088,  ...,  1.5095, -0.7713,  2.4147],\n",
       "         [-0.2419,  1.3183,  1.0722,  ...,  1.6039,  0.8515,  2.8190],\n",
       "         [-0.9381,  1.7539,  0.3402,  ...,  0.2018,  0.3287,  3.3271]],\n",
       "\n",
       "        [[ 0.4451,  0.6623,  1.3989,  ...,  2.1048, -0.0888,  2.2787],\n",
       "         [ 0.2795,  0.4871,  1.4640,  ...,  1.6827, -0.3780,  3.4613],\n",
       "         [-0.7752,  0.3807,  1.8392,  ...,  1.5982, -1.5318,  2.7350],\n",
       "         ...,\n",
       "         [ 0.6048,  1.2919,  1.8496,  ...,  1.8464, -0.2870,  1.0968],\n",
       "         [ 0.3051, -0.2096,  1.5842,  ...,  0.4973, -0.5270,  3.5191],\n",
       "         [-0.1666,  0.8657,  1.3226,  ...,  0.4381, -0.4392,  3.1127]],\n",
       "\n",
       "        [[ 0.7625,  2.0394,  1.7812,  ...,  0.6427, -1.0682,  1.4749],\n",
       "         [-0.2428,  1.9672,  1.7076,  ...,  1.9312, -0.2330,  3.1720],\n",
       "         [ 0.1336,  1.7676,  1.2759,  ...,  1.1187, -0.0861,  2.8430],\n",
       "         ...,\n",
       "         [ 0.0636,  2.2068,  0.3801,  ...,  1.4785, -0.3409,  2.8182],\n",
       "         [ 0.4636,  2.2414,  1.1558,  ...,  0.8277, -0.7832,  2.1638],\n",
       "         [-0.3167,  0.8675,  1.6002,  ...,  2.8610, -1.2798,  2.9102]],\n",
       "\n",
       "        [[-1.2961,  1.3894,  0.4527,  ...,  3.7856, -0.8304,  3.1925],\n",
       "         [ 0.3633,  1.8485,  1.7601,  ...,  1.6604,  0.2049,  2.5492],\n",
       "         [-0.2929,  0.8014,  0.9881,  ...,  1.7887,  0.5545,  0.2584],\n",
       "         ...,\n",
       "         [-1.7241,  0.8527,  2.5683,  ...,  2.0273,  0.2387,  2.5411],\n",
       "         [ 1.2584,  1.7515,  2.0134,  ...,  1.9006,  1.2447,  2.0996],\n",
       "         [-1.3321,  2.5036,  0.7162,  ...,  1.7403, -0.5304,  2.4276]],\n",
       "\n",
       "        [[ 0.8325,  1.3716,  1.6781,  ...,  1.3001, -0.2049,  3.0352],\n",
       "         [-1.7864,  0.8751,  1.6515,  ...,  1.0441, -0.2164,  2.3365],\n",
       "         [-1.5243,  1.4488,  2.1462,  ...,  2.1663, -1.3177,  1.7762],\n",
       "         ...,\n",
       "         [-0.3836,  1.2400,  1.0201,  ...,  0.4852, -0.0938,  3.3677],\n",
       "         [ 0.2230,  2.1103,  0.5649,  ..., -0.4674, -0.1679,  0.8534],\n",
       "         [-0.7628,  0.5945,  1.8598,  ...,  1.4586, -0.9941,  3.1091]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64,d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, enc_out: torch.FloatTensor, src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks don't have any notion of word order in a sentence. The Transformer explicitly adda the positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
