{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Transformer from Scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is based off of the following repos/blog posts:\n",
    "\n",
    "- [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch)\n",
    "- [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) \n",
    "\n",
    "Thanks so much to their authors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"tensor_shapes\")\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.setLevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "def getclass():\n",
    "    stack = inspect.stack()\n",
    "    return stack[3][0].f_locals[\"self\"].__class__\n",
    "\n",
    "# A helper function to check how tensor sizes change\n",
    "def log_size(tsr: torch.Tensor, name: str):\n",
    "    cls = getclass()\n",
    "    logger.log(level=cls.level, msg=[{cls.__name__}, {name}, {tsr.shape}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "# Control how much debugging output we want\n",
    "class TensorLoggingLevels(IntEnum):\n",
    "    attention = 1\n",
    "    attention_head = 2\n",
    "    multihead_attention_block = 3\n",
    "    enc_dec_block = 4\n",
    "    enc_dec = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components\n",
    "\n",
    "\n",
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textrm{Attention}(Q, K, V) = \\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.attention # Logging level: \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # Compute the dot product between queries and keys for each batch and position in the sequence\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
    "\n",
    "        attn = attn / math.sqrt(d_k)\n",
    "\n",
    "        attn = torch.exp(attn)\n",
    "        log_size(attn, \"attention weight\") # (Batch, Seq, Seq)\n",
    "        \n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        log_size(output, \"attention output size\") # (Batch, Seq, Seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5487, 0.6859, 0.4235, 0.5959, 0.6148, 0.6001, 0.5805, 0.4103,\n",
       "          0.7313, 0.7234, 0.7655, 0.5142, 0.3910, 0.8098, 0.6246, 0.5217,\n",
       "          0.7084, 0.4585, 0.5233, 0.4251],\n",
       "         [0.3373, 0.5487, 0.3416, 0.4417, 0.4118, 0.3639, 0.4414, 0.3636,\n",
       "          0.5160, 0.4745, 0.5662, 0.3672, 0.2377, 0.6190, 0.5036, 0.4159,\n",
       "          0.5534, 0.3568, 0.4758, 0.3571],\n",
       "         [0.4081, 0.5068, 0.2381, 0.4257, 0.3871, 0.4319, 0.4775, 0.1905,\n",
       "          0.4603, 0.4435, 0.4720, 0.3849, 0.2394, 0.5561, 0.4252, 0.2508,\n",
       "          0.4453, 0.3278, 0.2998, 0.3501],\n",
       "         [0.5352, 0.6768, 0.4247, 0.5926, 0.5892, 0.5686, 0.5846, 0.4257,\n",
       "          0.7153, 0.7050, 0.7539, 0.5124, 0.3841, 0.7924, 0.6262, 0.5240,\n",
       "          0.7170, 0.4837, 0.5364, 0.4157],\n",
       "         [0.4520, 0.6554, 0.3821, 0.5466, 0.5309, 0.4867, 0.5075, 0.4208,\n",
       "          0.6461, 0.6102, 0.6723, 0.3967, 0.3367, 0.7423, 0.5300, 0.4870,\n",
       "          0.6903, 0.3904, 0.5220, 0.3977],\n",
       "         [0.4389, 0.6298, 0.4233, 0.5146, 0.5761, 0.5236, 0.5081, 0.4168,\n",
       "          0.6618, 0.6439, 0.6959, 0.4348, 0.3781, 0.7767, 0.5889, 0.5133,\n",
       "          0.6194, 0.4062, 0.4515, 0.3517],\n",
       "         [0.4386, 0.5982, 0.4055, 0.5159, 0.4879, 0.4559, 0.5208, 0.3844,\n",
       "          0.5936, 0.5687, 0.6482, 0.4945, 0.2931, 0.7045, 0.6150, 0.4616,\n",
       "          0.5886, 0.4396, 0.4787, 0.3815],\n",
       "         [0.5518, 0.6791, 0.4363, 0.5840, 0.6181, 0.5868, 0.5676, 0.4117,\n",
       "          0.7249, 0.7066, 0.7704, 0.5118, 0.3816, 0.8128, 0.6292, 0.5216,\n",
       "          0.7007, 0.4523, 0.5374, 0.4428],\n",
       "         [0.4413, 0.6698, 0.3829, 0.5422, 0.5447, 0.5026, 0.5247, 0.4074,\n",
       "          0.6395, 0.6140, 0.6788, 0.4228, 0.3311, 0.7515, 0.5294, 0.4770,\n",
       "          0.6971, 0.3818, 0.5267, 0.4129],\n",
       "         [0.3821, 0.5235, 0.3740, 0.4304, 0.5309, 0.4599, 0.4752, 0.3844,\n",
       "          0.5866, 0.6174, 0.6249, 0.4193, 0.3689, 0.6613, 0.4916, 0.4686,\n",
       "          0.6017, 0.4092, 0.4143, 0.2803]],\n",
       "\n",
       "        [[0.5850, 0.2508, 0.6267, 0.5456, 0.3882, 0.4156, 0.7537, 0.4212,\n",
       "          0.5269, 0.5460, 0.3453, 0.4371, 0.6743, 0.4529, 0.5658, 0.6691,\n",
       "          0.4778, 0.6317, 0.5286, 0.6043],\n",
       "         [0.4615, 0.1865, 0.4154, 0.4713, 0.2654, 0.3690, 0.5592, 0.3669,\n",
       "          0.3482, 0.4519, 0.3033, 0.4114, 0.5012, 0.3713, 0.4647, 0.4774,\n",
       "          0.2712, 0.5752, 0.3586, 0.4078],\n",
       "         [0.5428, 0.2510, 0.5864, 0.5140, 0.3537, 0.3389, 0.6598, 0.3606,\n",
       "          0.4514, 0.5191, 0.2953, 0.3636, 0.6467, 0.4481, 0.4956, 0.6209,\n",
       "          0.4167, 0.5588, 0.4319, 0.5242],\n",
       "         [0.4380, 0.2297, 0.5838, 0.4826, 0.2829, 0.3950, 0.6641, 0.3949,\n",
       "          0.4927, 0.4517, 0.2916, 0.4255, 0.5768, 0.4361, 0.5372, 0.6295,\n",
       "          0.4392, 0.5701, 0.5063, 0.5329],\n",
       "         [0.4493, 0.2200, 0.5738, 0.4807, 0.2764, 0.3930, 0.6627, 0.4085,\n",
       "          0.4966, 0.4544, 0.3019, 0.4305, 0.5684, 0.4223, 0.5290, 0.6181,\n",
       "          0.4357, 0.5782, 0.5126, 0.5284],\n",
       "         [0.5762, 0.2893, 0.6110, 0.5682, 0.4048, 0.4258, 0.7642, 0.4470,\n",
       "          0.4949, 0.5491, 0.3644, 0.4694, 0.6847, 0.4583, 0.5403, 0.6507,\n",
       "          0.4564, 0.6485, 0.5283, 0.6116],\n",
       "         [0.5258, 0.1748, 0.5310, 0.4127, 0.3423, 0.2720, 0.5693, 0.2609,\n",
       "          0.4306, 0.4610, 0.2324, 0.2573, 0.5454, 0.3387, 0.4357, 0.5312,\n",
       "          0.4060, 0.4393, 0.3798, 0.4915],\n",
       "         [0.5678, 0.2696, 0.5312, 0.4823, 0.3786, 0.3523, 0.6797, 0.4316,\n",
       "          0.4411, 0.5413, 0.3108, 0.4048, 0.5969, 0.3555, 0.4643, 0.5551,\n",
       "          0.4397, 0.5858, 0.4928, 0.5687],\n",
       "         [0.4941, 0.2462, 0.5125, 0.4588, 0.3886, 0.3631, 0.7030, 0.3936,\n",
       "          0.4719, 0.4545, 0.3430, 0.3564, 0.6418, 0.4206, 0.4817, 0.6131,\n",
       "          0.4430, 0.5386, 0.4739, 0.5053],\n",
       "         [0.5242, 0.2721, 0.5596, 0.4900, 0.3855, 0.3678, 0.7073, 0.3571,\n",
       "          0.3947, 0.4835, 0.2832, 0.4126, 0.6196, 0.3760, 0.4618, 0.5736,\n",
       "          0.4076, 0.5666, 0.4649, 0.5915]],\n",
       "\n",
       "        [[0.6933, 0.6719, 0.6522, 0.4793, 0.7021, 0.3597, 0.6321, 0.6101,\n",
       "          0.6574, 0.3872, 0.4437, 0.6327, 0.5455, 0.6280, 0.7050, 0.5690,\n",
       "          0.5062, 0.5063, 0.5327, 0.4952],\n",
       "         [0.6923, 0.6742, 0.6523, 0.4805, 0.7088, 0.3565, 0.6347, 0.6025,\n",
       "          0.6590, 0.3898, 0.4397, 0.6187, 0.5262, 0.6534, 0.7118, 0.5590,\n",
       "          0.5133, 0.5063, 0.5251, 0.5111],\n",
       "         [0.5641, 0.6234, 0.6202, 0.4128, 0.6184, 0.3398, 0.5568, 0.5539,\n",
       "          0.6115, 0.3026, 0.4312, 0.6377, 0.5024, 0.5486, 0.6887, 0.5620,\n",
       "          0.3959, 0.4711, 0.5225, 0.3852],\n",
       "         [0.6845, 0.6670, 0.6401, 0.4828, 0.7157, 0.3617, 0.6314, 0.6042,\n",
       "          0.6531, 0.3790, 0.4297, 0.6322, 0.5253, 0.6440, 0.7138, 0.5729,\n",
       "          0.5125, 0.5185, 0.5414, 0.4994],\n",
       "         [0.3938, 0.4611, 0.3862, 0.4239, 0.4580, 0.2700, 0.4062, 0.3522,\n",
       "          0.3955, 0.2636, 0.2553, 0.4383, 0.3366, 0.4772, 0.4241, 0.4150,\n",
       "          0.4054, 0.3691, 0.3610, 0.3680],\n",
       "         [0.5987, 0.6353, 0.6423, 0.4132, 0.6358, 0.2992, 0.5647, 0.5850,\n",
       "          0.5702, 0.3761, 0.3916, 0.5627, 0.4695, 0.6439, 0.6346, 0.4679,\n",
       "          0.4960, 0.4570, 0.4524, 0.4742],\n",
       "         [0.5998, 0.5759, 0.5130, 0.4458, 0.6518, 0.3451, 0.5784, 0.4768,\n",
       "          0.5834, 0.3268, 0.3318, 0.5202, 0.4005, 0.6206, 0.6518, 0.5171,\n",
       "          0.4611, 0.4745, 0.4790, 0.4739],\n",
       "         [0.6578, 0.6536, 0.5903, 0.3935, 0.6931, 0.3034, 0.6002, 0.5359,\n",
       "          0.6549, 0.3240, 0.3682, 0.5322, 0.4418, 0.6067, 0.7106, 0.5003,\n",
       "          0.4838, 0.4343, 0.4799, 0.5026],\n",
       "         [0.5180, 0.5662, 0.4660, 0.4590, 0.5269, 0.3375, 0.4925, 0.4054,\n",
       "          0.4984, 0.3152, 0.2958, 0.4874, 0.4093, 0.5184, 0.5480, 0.5061,\n",
       "          0.4434, 0.4150, 0.4154, 0.4145],\n",
       "         [0.5471, 0.6341, 0.5784, 0.3375, 0.6040, 0.2835, 0.5342, 0.4833,\n",
       "          0.6337, 0.2403, 0.3756, 0.5484, 0.4301, 0.5022, 0.7076, 0.5103,\n",
       "          0.3762, 0.3856, 0.4734, 0.3929]],\n",
       "\n",
       "        [[0.3533, 0.6385, 0.7337, 0.6517, 0.5842, 0.5614, 0.4273, 0.6206,\n",
       "          0.5750, 0.3614, 0.5731, 0.5633, 0.7893, 0.5237, 0.5914, 0.4168,\n",
       "          0.4428, 0.5409, 0.4973, 0.6228],\n",
       "         [0.3267, 0.5663, 0.6484, 0.5876, 0.5180, 0.5016, 0.3549, 0.5950,\n",
       "          0.5537, 0.2778, 0.5624, 0.5253, 0.7291, 0.4975, 0.5793, 0.4167,\n",
       "          0.4077, 0.5318, 0.4256, 0.5703],\n",
       "         [0.3600, 0.6272, 0.7186, 0.6298, 0.5621, 0.5533, 0.4403, 0.6003,\n",
       "          0.5486, 0.3571, 0.5614, 0.5540, 0.8036, 0.5104, 0.5779, 0.4222,\n",
       "          0.4634, 0.5456, 0.4997, 0.6427],\n",
       "         [0.3209, 0.6010, 0.6300, 0.5706, 0.4732, 0.4422, 0.3847, 0.5193,\n",
       "          0.4937, 0.3098, 0.4889, 0.4551, 0.6927, 0.4299, 0.5652, 0.3629,\n",
       "          0.3554, 0.4802, 0.4302, 0.5250],\n",
       "         [0.2622, 0.4835, 0.5189, 0.5245, 0.4672, 0.3417, 0.3267, 0.5107,\n",
       "          0.4220, 0.3103, 0.4849, 0.4267, 0.6220, 0.4446, 0.4208, 0.3142,\n",
       "          0.3846, 0.4621, 0.4045, 0.4507],\n",
       "         [0.3055, 0.5133, 0.6430, 0.5932, 0.5132, 0.5457, 0.4165, 0.5356,\n",
       "          0.5052, 0.3054, 0.4949, 0.5109, 0.7393, 0.4401, 0.4927, 0.4084,\n",
       "          0.3855, 0.5031, 0.4609, 0.5403],\n",
       "         [0.3365, 0.5431, 0.5753, 0.5439, 0.4577, 0.4338, 0.3356, 0.5820,\n",
       "          0.5167, 0.3189, 0.4862, 0.5147, 0.7069, 0.4590, 0.4618, 0.3843,\n",
       "          0.4463, 0.4563, 0.4229, 0.5211],\n",
       "         [0.3572, 0.6290, 0.7135, 0.6502, 0.5886, 0.5496, 0.4369, 0.6290,\n",
       "          0.5700, 0.3520, 0.5909, 0.5645, 0.7901, 0.5277, 0.5981, 0.4060,\n",
       "          0.4537, 0.5585, 0.4868, 0.6374],\n",
       "         [0.3061, 0.4738, 0.5104, 0.5148, 0.4310, 0.3974, 0.2749, 0.5681,\n",
       "          0.5086, 0.2393, 0.4793, 0.4927, 0.6476, 0.4365, 0.4616, 0.3853,\n",
       "          0.3963, 0.4541, 0.3737, 0.4506],\n",
       "         [0.3592, 0.6266, 0.7198, 0.6461, 0.5811, 0.5561, 0.4423, 0.6145,\n",
       "          0.5591, 0.3581, 0.5706, 0.5626, 0.8000, 0.5169, 0.5839, 0.4181,\n",
       "          0.4590, 0.5502, 0.5042, 0.6325]],\n",
       "\n",
       "        [[0.6005, 0.3839, 0.6740, 0.5800, 0.5191, 0.6401, 0.5243, 0.4928,\n",
       "          0.4874, 0.6787, 0.4985, 0.5867, 0.4862, 0.6320, 0.5966, 0.3939,\n",
       "          0.6223, 0.5945, 0.5409, 0.4320],\n",
       "         [0.6036, 0.4930, 0.7146, 0.6418, 0.5216, 0.6853, 0.5461, 0.5477,\n",
       "          0.5255, 0.7122, 0.4861, 0.6365, 0.5069, 0.6309, 0.6170, 0.4094,\n",
       "          0.7209, 0.6295, 0.6018, 0.4726],\n",
       "         [0.6037, 0.4923, 0.7243, 0.6480, 0.5334, 0.6837, 0.5604, 0.5285,\n",
       "          0.5359, 0.6998, 0.4849, 0.6439, 0.4890, 0.6347, 0.6116, 0.3912,\n",
       "          0.7054, 0.6251, 0.6136, 0.4822],\n",
       "         [0.5534, 0.4166, 0.6656, 0.5874, 0.5129, 0.6242, 0.5428, 0.4779,\n",
       "          0.5161, 0.6376, 0.4671, 0.5913, 0.4557, 0.5985, 0.5821, 0.3743,\n",
       "          0.6247, 0.5822, 0.5661, 0.4165],\n",
       "         [0.5303, 0.3637, 0.6728, 0.5909, 0.3912, 0.6112, 0.4664, 0.4691,\n",
       "          0.3792, 0.5891, 0.4688, 0.5034, 0.4321, 0.5268, 0.4808, 0.3151,\n",
       "          0.5876, 0.5405, 0.4862, 0.4517],\n",
       "         [0.5642, 0.4186, 0.6649, 0.5813, 0.5231, 0.6307, 0.5470, 0.4779,\n",
       "          0.5178, 0.6529, 0.4725, 0.5912, 0.4499, 0.6184, 0.5824, 0.3794,\n",
       "          0.6300, 0.5733, 0.5645, 0.4299],\n",
       "         [0.6059, 0.4953, 0.7166, 0.6623, 0.5266, 0.6829, 0.5437, 0.5402,\n",
       "          0.5319, 0.7086, 0.4839, 0.6483, 0.4852, 0.6358, 0.5945, 0.4002,\n",
       "          0.7176, 0.6059, 0.6243, 0.4906],\n",
       "         [0.6160, 0.4770, 0.7199, 0.6446, 0.5241, 0.6999, 0.5463, 0.5504,\n",
       "          0.5201, 0.7028, 0.4985, 0.6343, 0.4983, 0.6328, 0.6071, 0.3944,\n",
       "          0.7262, 0.6214, 0.6016, 0.4847],\n",
       "         [0.6015, 0.4103, 0.6752, 0.5759, 0.5423, 0.6202, 0.5363, 0.4524,\n",
       "          0.4667, 0.6463, 0.4324, 0.5942, 0.4717, 0.5966, 0.5630, 0.3212,\n",
       "          0.6310, 0.5934, 0.5304, 0.4255],\n",
       "         [0.4185, 0.3989, 0.6142, 0.6142, 0.3668, 0.5162, 0.4718, 0.4125,\n",
       "          0.4001, 0.5193, 0.3716, 0.4929, 0.3606, 0.4471, 0.3933, 0.2887,\n",
       "          0.5653, 0.4564, 0.5309, 0.3757]]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    level = TensorLoggingLevels.attention_head\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) \n",
    "        V = self.value_tfm(values) \n",
    "        log_size(Q, \"queries, keys, vals\")\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention weight'}, {torch.Size([5, 10, 10])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n",
      "[{'ScaledDotProductAttention'}, {'attention output size'}, {torch.Size([5, 10, 20])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0839, -0.1160, -0.3149, -0.2495, -0.1862, -0.0053,  0.0912,\n",
       "           0.0918,  0.4122,  0.2330,  0.1884,  0.4006, -0.3230,  0.0929,\n",
       "           0.1710,  0.4789, -0.5986, -0.4741, -0.1592, -0.1236],\n",
       "         [-0.1058, -0.0880, -0.3448, -0.2976, -0.1884, -0.0016,  0.1065,\n",
       "           0.1135,  0.5054,  0.3075,  0.2410,  0.4434, -0.3087,  0.1432,\n",
       "           0.1693,  0.5710, -0.6204, -0.5448, -0.2274, -0.1145],\n",
       "         [-0.1096, -0.1153, -0.4004, -0.3189, -0.2159,  0.0050,  0.1237,\n",
       "           0.1141,  0.5421,  0.3275,  0.2803,  0.5214, -0.3736,  0.1355,\n",
       "           0.1965,  0.6032, -0.6990, -0.5953, -0.2246, -0.1616],\n",
       "         [-0.1117, -0.1138, -0.4006, -0.3179, -0.2142,  0.0077,  0.1246,\n",
       "           0.1156,  0.5438,  0.3253,  0.2700,  0.5120, -0.3707,  0.1390,\n",
       "           0.1967,  0.6073, -0.7029, -0.5964, -0.2262, -0.1530],\n",
       "         [-0.1105, -0.1132, -0.3989, -0.3193, -0.2136,  0.0042,  0.1234,\n",
       "           0.1157,  0.5451,  0.3276,  0.2751,  0.5163, -0.3712,  0.1357,\n",
       "           0.1970,  0.6083, -0.7020, -0.5947, -0.2276, -0.1562],\n",
       "         [-0.1092, -0.1136, -0.3981, -0.3182, -0.2143,  0.0025,  0.1241,\n",
       "           0.1153,  0.5453,  0.3293,  0.2803,  0.5213, -0.3731,  0.1330,\n",
       "           0.1968,  0.6071, -0.7012, -0.5943, -0.2277, -0.1604],\n",
       "         [-0.1128, -0.1036, -0.3850, -0.2790, -0.1469,  0.0254,  0.1058,\n",
       "           0.0804,  0.4170,  0.2611,  0.2159,  0.4238, -0.3402,  0.0939,\n",
       "           0.1628,  0.4633, -0.5593, -0.4797, -0.1737, -0.1801],\n",
       "         [-0.0982, -0.1018, -0.3545, -0.2802, -0.2000,  0.0071,  0.1141,\n",
       "           0.1001,  0.4804,  0.3142,  0.2821,  0.4820, -0.3373,  0.1232,\n",
       "           0.1635,  0.5272, -0.5943, -0.5426, -0.2133, -0.1697],\n",
       "         [-0.1054, -0.0882, -0.3431, -0.2978, -0.1890, -0.0022,  0.1059,\n",
       "           0.1128,  0.5031,  0.3061,  0.2411,  0.4429, -0.3068,  0.1443,\n",
       "           0.1687,  0.5687, -0.6179, -0.5433, -0.2252, -0.1135],\n",
       "         [-0.1129, -0.1084, -0.4036, -0.3187, -0.2083,  0.0052,  0.1271,\n",
       "           0.1154,  0.5455,  0.3296,  0.2770,  0.5155, -0.3715,  0.1348,\n",
       "           0.1955,  0.6086, -0.6979, -0.5974, -0.2323, -0.1595]],\n",
       "\n",
       "        [[-0.0127,  0.1487, -0.5568, -0.2216, -0.0318,  0.1078,  0.3154,\n",
       "           0.2047,  0.3494,  0.2703,  0.3322,  0.3697, -0.3656,  0.3001,\n",
       "          -0.0501,  0.5605, -0.5356, -0.7242, -0.0992, -0.2086],\n",
       "         [-0.0133,  0.1487, -0.5538, -0.2223, -0.0317,  0.1072,  0.3148,\n",
       "           0.2044,  0.3514,  0.2715,  0.3351,  0.3720, -0.3646,  0.2998,\n",
       "          -0.0495,  0.5613, -0.5361, -0.7208, -0.0982, -0.2102],\n",
       "         [-0.0153,  0.1483, -0.5563, -0.2227, -0.0343,  0.1098,  0.3125,\n",
       "           0.2033,  0.3475,  0.2701,  0.3369,  0.3677, -0.3624,  0.3001,\n",
       "          -0.0498,  0.5624, -0.5345, -0.7214, -0.0969, -0.2122],\n",
       "         [-0.0071,  0.1258, -0.4798, -0.1782, -0.0212,  0.0996,  0.2687,\n",
       "           0.1663,  0.2936,  0.2226,  0.3118,  0.3317, -0.3087,  0.2546,\n",
       "          -0.0255,  0.4597, -0.4635, -0.6191, -0.0878, -0.2026],\n",
       "         [-0.0103,  0.1486, -0.5538, -0.2219, -0.0337,  0.1066,  0.3150,\n",
       "           0.2054,  0.3508,  0.2704,  0.3343,  0.3706, -0.3636,  0.3014,\n",
       "          -0.0510,  0.5614, -0.5365, -0.7217, -0.0981, -0.2066],\n",
       "         [-0.0107,  0.1477, -0.5060, -0.2091, -0.0198,  0.1078,  0.2852,\n",
       "           0.1784,  0.2913,  0.2555,  0.3197,  0.3179, -0.3153,  0.2887,\n",
       "          -0.0673,  0.5018, -0.4606, -0.6305, -0.0642, -0.1951],\n",
       "         [-0.0317,  0.1514, -0.5035, -0.2059, -0.0010,  0.0987,  0.2921,\n",
       "           0.1812,  0.3379,  0.2661,  0.3109,  0.3494, -0.3424,  0.2663,\n",
       "          -0.0462,  0.5105, -0.4895, -0.6477, -0.0929, -0.2117],\n",
       "         [-0.0169,  0.0908, -0.4263, -0.1713, -0.0312,  0.0479,  0.2761,\n",
       "           0.1774,  0.2455,  0.1833,  0.2136,  0.2568, -0.2595,  0.2519,\n",
       "          -0.0367,  0.4646, -0.4207, -0.5818, -0.0764, -0.0991],\n",
       "         [-0.0160,  0.1491, -0.5562, -0.2223, -0.0331,  0.1091,  0.3129,\n",
       "           0.2033,  0.3521,  0.2708,  0.3359,  0.3709, -0.3656,  0.2984,\n",
       "          -0.0481,  0.5623, -0.5381, -0.7236, -0.0996, -0.2127],\n",
       "         [-0.0106,  0.1473, -0.5550, -0.2208, -0.0359,  0.1072,  0.3137,\n",
       "           0.2050,  0.3499,  0.2685,  0.3341,  0.3667, -0.3615,  0.3022,\n",
       "          -0.0512,  0.5620, -0.5377, -0.7226, -0.0986, -0.2048]],\n",
       "\n",
       "        [[-0.1828,  0.0649, -0.6418, -0.3347, -0.0917,  0.0939,  0.1439,\n",
       "           0.0929,  0.3283,  0.1905,  0.2086,  0.3518, -0.4356,  0.1250,\n",
       "           0.0789,  0.5989, -0.6366, -0.7221, -0.0768, -0.2193],\n",
       "         [-0.1326,  0.0326, -0.5232, -0.2919, -0.0975,  0.1045,  0.0799,\n",
       "           0.0634,  0.2979,  0.1613,  0.2064,  0.2904, -0.3146,  0.1345,\n",
       "           0.0776,  0.4693, -0.5056, -0.5792, -0.0785, -0.1537],\n",
       "         [-0.1617,  0.0522, -0.5835, -0.2963, -0.0996,  0.1116,  0.1189,\n",
       "           0.0770,  0.3143,  0.1718,  0.1876,  0.3030, -0.3703,  0.1230,\n",
       "           0.0830,  0.5484, -0.5750, -0.6400, -0.0813, -0.1855],\n",
       "         [-0.1826,  0.0690, -0.6430, -0.3361, -0.0926,  0.0910,  0.1430,\n",
       "           0.0918,  0.3240,  0.1899,  0.2110,  0.3514, -0.4352,  0.1214,\n",
       "           0.0757,  0.6025, -0.6354, -0.7233, -0.0735, -0.2177],\n",
       "         [-0.1843,  0.0690, -0.6427, -0.3372, -0.0917,  0.0901,  0.1458,\n",
       "           0.0926,  0.3228,  0.1882,  0.2066,  0.3519, -0.4376,  0.1232,\n",
       "           0.0776,  0.5994, -0.6354, -0.7254, -0.0719, -0.2189],\n",
       "         [-0.1816,  0.0681, -0.6417, -0.3336, -0.0931,  0.0902,  0.1448,\n",
       "           0.0938,  0.3246,  0.1904,  0.2102,  0.3504, -0.4349,  0.1212,\n",
       "           0.0734,  0.6043, -0.6356, -0.7216, -0.0741, -0.2178],\n",
       "         [-0.1349,  0.0023, -0.5099, -0.3000, -0.0632,  0.0891,  0.0757,\n",
       "           0.0695,  0.3016,  0.1625,  0.2038,  0.3207, -0.3427,  0.1336,\n",
       "           0.0814,  0.4227, -0.5063, -0.5667, -0.0837, -0.1800],\n",
       "         [-0.1826,  0.0699, -0.6426, -0.3366, -0.0933,  0.0925,  0.1444,\n",
       "           0.0921,  0.3257,  0.1908,  0.2097,  0.3507, -0.4343,  0.1250,\n",
       "           0.0768,  0.6021, -0.6348, -0.7247, -0.0746, -0.2161],\n",
       "         [-0.1831,  0.0660, -0.6415, -0.3360, -0.0916,  0.0901,  0.1462,\n",
       "           0.0945,  0.3249,  0.1891,  0.2078,  0.3529, -0.4372,  0.1247,\n",
       "           0.0756,  0.5986, -0.6358, -0.7240, -0.0731, -0.2195],\n",
       "         [-0.1170,  0.0209, -0.4469, -0.2128, -0.0732,  0.0796,  0.0624,\n",
       "           0.0470,  0.2338,  0.1211,  0.1533,  0.2349, -0.2915,  0.0455,\n",
       "           0.0770,  0.4168, -0.4584, -0.4588, -0.0696, -0.1632]],\n",
       "\n",
       "        [[-0.0727,  0.0562, -0.5281, -0.1872, -0.1904,  0.1933,  0.1822,\n",
       "           0.1169,  0.3428,  0.2592,  0.2069,  0.2787, -0.4406,  0.2153,\n",
       "          -0.0255,  0.6372, -0.6510, -0.6620, -0.0472, -0.2111],\n",
       "         [-0.0989,  0.0681, -0.5913, -0.2293, -0.1959,  0.1982,  0.1951,\n",
       "           0.1137,  0.3841,  0.2542,  0.2048,  0.3133, -0.4880,  0.2194,\n",
       "          -0.0039,  0.6968, -0.7192, -0.7055, -0.0634, -0.2351],\n",
       "         [-0.0944,  0.0890, -0.5444, -0.2007, -0.1426,  0.1708,  0.2133,\n",
       "           0.0990,  0.3381,  0.2193,  0.1968,  0.2609, -0.4168,  0.2123,\n",
       "          -0.0402,  0.6179, -0.6211, -0.6003, -0.0637, -0.2120],\n",
       "         [-0.0987,  0.0656, -0.5904, -0.2283, -0.1983,  0.1987,  0.1944,\n",
       "           0.1141,  0.3855,  0.2541,  0.2043,  0.3139, -0.4880,  0.2195,\n",
       "          -0.0028,  0.6980, -0.7212, -0.7063, -0.0627, -0.2347],\n",
       "         [-0.1120,  0.0307, -0.5090, -0.2094, -0.1891,  0.1656,  0.1255,\n",
       "           0.0743,  0.3334,  0.1715,  0.1677,  0.2785, -0.4217,  0.1418,\n",
       "           0.0509,  0.6029, -0.6260, -0.5950, -0.0578, -0.2123],\n",
       "         [-0.1062,  0.0275, -0.4699, -0.2105, -0.1828,  0.1702,  0.1535,\n",
       "           0.0871,  0.3511,  0.2316,  0.1677,  0.2784, -0.4126,  0.1930,\n",
       "           0.0551,  0.5789, -0.6129, -0.5899, -0.0577, -0.2106],\n",
       "         [-0.0827,  0.0688, -0.4226, -0.1868, -0.1344,  0.1600,  0.1436,\n",
       "           0.0688,  0.2660,  0.2724,  0.2130,  0.2297, -0.3735,  0.1835,\n",
       "           0.0016,  0.4962, -0.4953, -0.5376, -0.0350, -0.2094],\n",
       "         [-0.0997,  0.0699, -0.5917, -0.2334, -0.1956,  0.1991,  0.1949,\n",
       "           0.1121,  0.3840,  0.2591,  0.2073,  0.3159, -0.4925,  0.2211,\n",
       "          -0.0032,  0.6955, -0.7190, -0.7061, -0.0608, -0.2395],\n",
       "         [-0.0942,  0.0830, -0.5426, -0.1985, -0.1471,  0.1713,  0.2101,\n",
       "           0.1003,  0.3415,  0.2103,  0.1905,  0.2618, -0.4152,  0.2092,\n",
       "          -0.0364,  0.6183, -0.6247, -0.5981, -0.0645, -0.2091],\n",
       "         [-0.0014,  0.0533, -0.3763, -0.1211, -0.1322,  0.1198,  0.1347,\n",
       "           0.0992,  0.2123,  0.2001,  0.1357,  0.1973, -0.3331,  0.1592,\n",
       "          -0.0681,  0.4516, -0.4730, -0.4870, -0.0070, -0.1332]],\n",
       "\n",
       "        [[-0.0336,  0.0679, -0.4734, -0.2162, -0.1948,  0.2330,  0.1869,\n",
       "           0.1111,  0.5519,  0.4488,  0.3168,  0.4759, -0.4669,  0.2629,\n",
       "           0.0921,  0.5378, -0.6890, -0.7321, -0.2053, -0.2393],\n",
       "         [-0.0311,  0.0656, -0.4734, -0.2163, -0.1946,  0.2330,  0.1836,\n",
       "           0.1133,  0.5548,  0.4498,  0.3189,  0.4755, -0.4645,  0.2636,\n",
       "           0.0910,  0.5384, -0.6889, -0.7304, -0.2074, -0.2391],\n",
       "         [-0.0289,  0.0589, -0.3059, -0.1151, -0.1354,  0.1759,  0.1442,\n",
       "           0.0530,  0.2944,  0.3061,  0.1766,  0.2641, -0.2712,  0.1836,\n",
       "           0.0340,  0.3612, -0.3750, -0.4711, -0.0863, -0.1226],\n",
       "         [-0.0332,  0.0694, -0.4736, -0.2153, -0.1937,  0.2322,  0.1879,\n",
       "           0.1113,  0.5521,  0.4478,  0.3175,  0.4746, -0.4665,  0.2631,\n",
       "           0.0905,  0.5378, -0.6901, -0.7314, -0.2053, -0.2389],\n",
       "         [-0.0328,  0.0674, -0.4742, -0.2171, -0.1974,  0.2323,  0.1858,\n",
       "           0.1123,  0.5517,  0.4498,  0.3177,  0.4751, -0.4638,  0.2633,\n",
       "           0.0905,  0.5431, -0.6877, -0.7334, -0.2034, -0.2362],\n",
       "         [-0.0318,  0.0697, -0.4713, -0.2163, -0.1957,  0.2324,  0.1851,\n",
       "           0.1124,  0.5538,  0.4497,  0.3174,  0.4756, -0.4645,  0.2632,\n",
       "           0.0892,  0.5406, -0.6884, -0.7310, -0.2058, -0.2369],\n",
       "         [-0.0304,  0.0670, -0.4714, -0.2151, -0.1937,  0.2340,  0.1832,\n",
       "           0.1139,  0.5567,  0.4492,  0.3184,  0.4760, -0.4657,  0.2635,\n",
       "           0.0902,  0.5371, -0.6898, -0.7293, -0.2097, -0.2403],\n",
       "         [-0.0247,  0.0982, -0.3723, -0.1404, -0.1492,  0.1843,  0.1873,\n",
       "           0.0888,  0.4695,  0.3633,  0.2627,  0.3808, -0.3864,  0.2165,\n",
       "           0.0652,  0.4631, -0.6088, -0.6124, -0.1923, -0.1759],\n",
       "         [-0.0336,  0.0664, -0.4780, -0.2213, -0.1986,  0.2328,  0.1857,\n",
       "           0.1117,  0.5505,  0.4522,  0.3190,  0.4777, -0.4669,  0.2637,\n",
       "           0.0911,  0.5428, -0.6863, -0.7359, -0.2008, -0.2400],\n",
       "         [-0.0365,  0.1396, -0.2872, -0.1054, -0.1109,  0.1541,  0.1970,\n",
       "           0.0641,  0.3713,  0.2691,  0.1936,  0.3025, -0.3483,  0.1821,\n",
       "           0.0259,  0.3606, -0.5060, -0.5028, -0.1442, -0.1547]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi head attention block applies multiple attention heads as can be seen in the paper \"Attention is all you need\", then concatenates the output and applies single linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    level = TensorLoggingLevels.multihead_attention_block\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Sequence, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        log_size(x[0], \"output of single head\")\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Sequence, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Sequence, D_Model)\n",
    "        log_size(x, \"projected output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 160])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'AttentionHead'}, {'queries, keys, vals'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 20])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 160])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8625e-02,  4.4277e-03, -5.3866e-01,  ..., -1.1546e-01,\n",
       "          -1.3225e-01,  1.2852e-02],\n",
       "         [ 3.1109e-02,  4.0733e-02, -4.9640e-01,  ..., -6.7372e-02,\n",
       "          -1.1141e-01, -7.8243e-03],\n",
       "         [ 4.2289e-02,  1.4381e-02, -5.2694e-01,  ..., -9.5735e-02,\n",
       "          -1.0509e-01,  2.2255e-03],\n",
       "         ...,\n",
       "         [ 3.0347e-02,  4.4175e-03, -5.3604e-01,  ..., -1.4354e-01,\n",
       "          -1.4847e-01, -7.5184e-03],\n",
       "         [ 1.3951e-02, -3.7347e-02, -4.5489e-01,  ..., -7.9561e-02,\n",
       "          -1.8650e-01, -1.7714e-02],\n",
       "         [ 5.5675e-03,  9.2399e-03, -4.9065e-01,  ..., -1.2078e-01,\n",
       "          -1.0069e-01, -4.1701e-02]],\n",
       "\n",
       "        [[ 8.4204e-03,  2.5438e-02, -4.5736e-01,  ..., -5.8907e-02,\n",
       "          -1.9848e-01, -1.0257e-01],\n",
       "         [-3.0457e-02,  6.0310e-02, -3.9213e-01,  ..., -2.5930e-02,\n",
       "          -2.1253e-01, -1.2257e-01],\n",
       "         [-1.2325e-02,  2.3600e-02, -4.4613e-01,  ..., -1.3368e-02,\n",
       "          -1.8879e-01, -9.9528e-02],\n",
       "         ...,\n",
       "         [-1.6378e-02, -6.0273e-03, -4.5461e-01,  ..., -4.2598e-02,\n",
       "          -2.0866e-01, -1.0729e-01],\n",
       "         [-1.2614e-02,  3.3040e-02, -4.5416e-01,  ..., -1.2636e-02,\n",
       "          -1.7141e-01, -1.0719e-01],\n",
       "         [-1.6738e-02,  1.7257e-02, -4.8317e-01,  ..., -3.8393e-02,\n",
       "          -1.9975e-01, -9.0658e-02]],\n",
       "\n",
       "        [[-2.3226e-02,  1.8136e-02, -5.0159e-01,  ..., -3.9997e-02,\n",
       "          -1.7441e-01, -3.1152e-03],\n",
       "         [-3.6459e-02,  4.8247e-02, -4.2389e-01,  ..., -7.9887e-02,\n",
       "          -1.1695e-01, -2.3171e-02],\n",
       "         [-3.2429e-02,  1.5653e-02, -4.8056e-01,  ..., -1.4186e-02,\n",
       "          -1.8810e-01,  1.7092e-03],\n",
       "         ...,\n",
       "         [-2.2395e-03,  3.6843e-02, -4.3096e-01,  ..., -4.1843e-02,\n",
       "          -1.2460e-01, -5.0994e-03],\n",
       "         [-1.9590e-03,  6.6264e-02, -4.5024e-01,  ..., -2.9554e-02,\n",
       "          -1.3321e-01,  9.0675e-03],\n",
       "         [-1.6137e-02,  3.4757e-02, -4.8123e-01,  ..., -3.4346e-02,\n",
       "          -1.9525e-01, -3.2060e-03]],\n",
       "\n",
       "        [[-1.6704e-04,  7.0218e-03, -3.8017e-01,  ..., -5.8420e-02,\n",
       "          -2.2554e-01, -5.1658e-02],\n",
       "         [ 7.6096e-03, -1.1184e-03, -3.9574e-01,  ..., -6.2142e-02,\n",
       "          -2.0607e-01, -4.6866e-02],\n",
       "         [ 1.4815e-02, -2.2603e-02, -3.6845e-01,  ..., -3.8893e-02,\n",
       "          -1.8041e-01, -2.9803e-03],\n",
       "         ...,\n",
       "         [ 8.6788e-03,  8.8885e-04, -3.9834e-01,  ..., -6.1170e-02,\n",
       "          -1.7572e-01, -6.8129e-02],\n",
       "         [-1.6638e-02, -2.6480e-02, -3.8211e-01,  ..., -9.2165e-02,\n",
       "          -2.5891e-01, -6.5742e-02],\n",
       "         [-5.7350e-03,  2.4469e-02, -3.1497e-01,  ..., -8.5294e-02,\n",
       "          -1.5344e-01, -4.2994e-02]],\n",
       "\n",
       "        [[ 6.8209e-03,  2.7075e-03, -5.3112e-01,  ..., -9.2929e-02,\n",
       "          -1.6788e-01, -7.2062e-02],\n",
       "         [ 2.5012e-02, -2.8651e-02, -5.6401e-01,  ..., -8.8923e-02,\n",
       "          -2.2949e-01, -5.8529e-02],\n",
       "         [ 1.7888e-02, -7.0252e-04, -5.3063e-01,  ..., -5.6860e-02,\n",
       "          -1.7654e-01, -5.9501e-02],\n",
       "         ...,\n",
       "         [ 9.1022e-03, -8.9027e-03, -5.7159e-01,  ..., -5.1010e-02,\n",
       "          -1.7821e-01, -5.7577e-02],\n",
       "         [ 5.1162e-03, -5.9692e-03, -4.9544e-01,  ..., -1.1224e-01,\n",
       "          -1.7110e-01, -5.2374e-02],\n",
       "         [ 3.4272e-02, -5.8180e-03, -5.1624e-01,  ..., -1.2219e-01,\n",
       "          -2.1116e-01, -8.4020e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is made up of the following components:\n",
    "- multi-head attention block\n",
    "- simple feedforward neural network\n",
    "\n",
    "These components are connected using residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "    logger.setLevel(TensorLoggingLevels.multihead_attention_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        log_size(x, \"Encoder block input\")\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        log_size(x, \"Attention output\")\n",
    "        # Applying normalization and residual connection.\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Applying position-wise feedforward network.\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        log_size(x, \"Feedforward output\")\n",
    "        # Applying normalization and residual connection.\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        log_size(x, \"Encoder size output\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7751,  1.1467,  3.4461,  ..., -0.6820, -2.4045,  1.2170],\n",
       "         [ 1.3583, -0.1930,  4.0995,  ..., -0.1053, -1.5256,  0.3254],\n",
       "         [ 0.2713,  0.1199,  2.4488,  ..., -0.2137,  0.1164, -0.1484],\n",
       "         ...,\n",
       "         [ 2.9047,  0.0258,  2.6899,  ..., -0.4092, -2.7584, -0.8172],\n",
       "         [ 1.4165,  0.1880,  3.6077,  ...,  0.7653, -2.6604, -1.3268],\n",
       "         [ 0.7241,  1.0821,  2.5580,  ...,  0.2299, -2.2741, -0.8416]],\n",
       "\n",
       "        [[ 2.2110,  0.6463,  4.1705,  ...,  0.0997, -3.2632, -1.5028],\n",
       "         [ 2.2632,  1.8673,  4.1719,  ...,  1.0696, -2.9554,  0.5570],\n",
       "         [ 1.5239,  0.3931,  3.8011,  ...,  0.7221, -3.5631, -1.3783],\n",
       "         ...,\n",
       "         [ 2.2188,  1.4470,  3.1167,  ...,  0.6588, -0.7751, -0.6084],\n",
       "         [ 1.3707,  1.2954,  3.4004,  ..., -0.4372, -1.9897, -0.2909],\n",
       "         [ 2.7263,  1.1859,  4.0976,  ...,  0.8435, -2.3570, -1.1413]],\n",
       "\n",
       "        [[ 1.1442,  0.4413,  2.9840,  ..., -0.4572, -2.6007, -1.4273],\n",
       "         [ 0.9324,  0.4598,  1.0547,  ..., -0.0736, -2.4643,  0.1089],\n",
       "         [ 2.6321,  0.8672,  3.0326,  ..., -1.0379, -2.1265, -1.3153],\n",
       "         ...,\n",
       "         [ 2.9966,  0.4380,  2.7616,  ..., -0.4957, -3.1018, -0.8214],\n",
       "         [ 1.2691,  0.8857,  2.1585,  ..., -1.3189, -2.9689, -1.2769],\n",
       "         [-0.0401, -0.1790,  3.4996,  ..., -1.2490, -1.1030, -0.2237]],\n",
       "\n",
       "        [[ 2.1738,  0.8232,  2.4704,  ..., -0.7140, -2.7207, -0.0399],\n",
       "         [ 1.6686,  0.4235,  3.1379,  ...,  0.1055, -1.1408, -0.3043],\n",
       "         [ 1.9368,  0.5784,  1.7554,  ..., -0.0134, -3.2627, -1.0226],\n",
       "         ...,\n",
       "         [-1.0520, -0.1778,  3.7057,  ..., -0.6341, -1.7427, -0.6091],\n",
       "         [ 0.0391,  0.5555,  4.2861,  ..., -0.2641, -1.7470, -0.8621],\n",
       "         [ 1.5163,  0.9061,  3.4677,  ...,  0.0200, -1.2779, -0.1209]],\n",
       "\n",
       "        [[ 2.2095,  1.2795,  3.0132,  ..., -0.3025, -2.7129, -0.1330],\n",
       "         [ 2.0098,  0.8193,  3.9326,  ..., -0.3395, -3.4477,  0.9663],\n",
       "         [ 1.8781,  0.7672,  3.7495,  ..., -1.3286, -3.6890, -0.1471],\n",
       "         ...,\n",
       "         [ 1.9989,  1.2143,  3.3306,  ...,  0.3886, -2.3637, -0.4598],\n",
       "         [ 1.0742,  0.5836,  3.7614,  ..., -0.3517, -2.5173,  0.1287],\n",
       "         [ 0.8460,  0.6237,  4.2896,  ..., -0.2734, -3.0485, -0.6261]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder is having six consecutive encoder blocks, thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder is same in structure as the encoder with just one additional multi-head attention block that takes the target sentence as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64,\n",
    "                 d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        # Applying attention to inputs.\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        # Applying attention to the encoder outputs and outputs of the previous layer.\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "        # Applying position-wise feedforward network.\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 10, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 10, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5575, -0.9342, -0.8271,  ...,  2.1886,  3.3905, -1.1279],\n",
       "         [ 0.2831,  0.1582,  0.2548,  ...,  0.0379,  1.5856, -1.4237],\n",
       "         [ 0.4611, -2.0253, -0.7102,  ..., -0.5144,  2.2126,  0.2841],\n",
       "         ...,\n",
       "         [ 0.1243, -1.4682, -1.0445,  ...,  1.5168,  2.4904, -1.3807],\n",
       "         [ 0.4120, -1.2903, -0.1611,  ...,  0.8600,  2.2050, -0.8814],\n",
       "         [ 0.0323, -1.7075, -2.1069,  ..., -0.2950,  1.1387, -0.4732]],\n",
       "\n",
       "        [[ 0.4988, -2.6431, -1.9691,  ..., -0.2050,  1.2303, -0.8485],\n",
       "         [ 0.7427, -0.0800, -0.6465,  ...,  0.7916,  1.7076, -1.6753],\n",
       "         [ 0.6754, -0.8299, -0.7837,  ...,  0.8535,  1.5801, -1.5188],\n",
       "         ...,\n",
       "         [-0.4863, -1.2051, -0.0130,  ...,  1.7027,  1.5050, -1.0456],\n",
       "         [ 1.0873, -1.3567, -0.4417,  ..., -0.3055,  1.3601, -0.3979],\n",
       "         [-0.5913, -1.0328, -0.2476,  ...,  0.0326,  1.9341, -1.4867]],\n",
       "\n",
       "        [[ 1.1301, -1.1240, -1.7672,  ...,  0.4280,  1.6007, -0.0960],\n",
       "         [ 1.5915, -1.8536, -2.0984,  ...,  0.0669,  2.0707, -0.2220],\n",
       "         [-0.3552, -1.6466, -1.0963,  ...,  1.7951,  2.5390, -1.3588],\n",
       "         ...,\n",
       "         [ 1.4937, -0.3796, -1.8249,  ...,  1.1183,  2.2341, -1.1231],\n",
       "         [ 0.9471, -1.6403, -0.5375,  ..., -0.0188,  2.3846, -0.7059],\n",
       "         [ 0.0636, -1.8153, -1.3263,  ...,  0.1910,  2.5446, -0.5019]],\n",
       "\n",
       "        [[-0.5217, -2.3884, -0.8856,  ..., -0.0800,  1.8525, -0.1297],\n",
       "         [ 0.2192, -2.2303, -0.6622,  ...,  0.4690,  1.3955, -0.1446],\n",
       "         [ 1.6726, -2.3311, -1.1976,  ...,  1.1003,  1.5944, -0.5263],\n",
       "         ...,\n",
       "         [ 1.2476, -1.7586,  0.9610,  ..., -0.5959,  0.9389, -1.3821],\n",
       "         [ 0.8625,  0.1893, -0.7860,  ...,  2.0001,  1.5886,  0.4693],\n",
       "         [ 1.9599, -2.3965, -0.9515,  ...,  1.5001,  1.0531, -0.9290]],\n",
       "\n",
       "        [[-0.0769, -1.3273, -1.5308,  ...,  0.5860,  2.5291, -1.9869],\n",
       "         [ 0.9921, -1.2501, -1.5798,  ...,  0.4224,  0.6439, -1.0473],\n",
       "         [-0.5225, -1.8787, -0.4614,  ...,  2.3977,  0.8845, -1.1711],\n",
       "         ...,\n",
       "         [ 0.3358, -1.8103, -0.1499,  ...,  1.0073,  1.2667, -0.7743],\n",
       "         [-1.3649, -1.3227, -0.1506,  ..., -0.0967,  1.6656, -1.0372],\n",
       "         [ 0.7127, -1.6137, -0.8110,  ...,  0.0672,  1.3367, -2.4221]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    level = TensorLoggingLevels.enc_dec\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention blocks don't have any notion of word order in a sentence. The Transformer explicitly adds the positional information via the positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'Input queries'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'output of single head'}, {torch.Size([5, 30, 64])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'concatenated output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'MultiHeadAttention'}, {'projected output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2577e+00, -1.2576e+00,  2.1603e+00,  ...,  1.8294e+00,\n",
       "           8.6026e+00, -3.7816e-01],\n",
       "         [ 3.6246e-01,  6.4817e-01,  4.4497e+00,  ...,  7.0527e-01,\n",
       "           6.6239e+00,  7.6782e-01],\n",
       "         [ 7.2338e-03,  5.6890e-01,  5.5462e+00,  ..., -1.7403e+00,\n",
       "           3.6862e+00, -1.6055e+00],\n",
       "         ...,\n",
       "         [ 1.1985e+00,  2.9628e+00,  4.9665e+00,  ...,  4.1127e-01,\n",
       "           4.2465e+00, -2.2261e+00],\n",
       "         [ 1.6161e+00, -5.7866e-01,  6.3342e+00,  ..., -2.7053e+00,\n",
       "           3.3395e+00, -1.5021e+00],\n",
       "         [-1.1713e-01, -2.8804e+00,  4.6234e+00,  ..., -1.1865e+00,\n",
       "           6.3678e+00, -6.4391e-01]],\n",
       "\n",
       "        [[ 3.3451e+00,  2.2801e+00,  2.4258e+00,  ..., -4.1175e-01,\n",
       "           4.5857e+00, -2.9214e+00],\n",
       "         [ 4.0797e+00,  2.9298e+00,  3.6687e+00,  ...,  2.3159e-01,\n",
       "           6.1309e+00, -4.4661e+00],\n",
       "         [ 5.0622e+00,  1.0185e+00,  3.8508e+00,  ...,  8.4012e-01,\n",
       "           1.7289e+00, -7.4246e-01],\n",
       "         ...,\n",
       "         [ 1.7492e+00, -5.5354e-01, -1.5298e+00,  ..., -1.8694e+00,\n",
       "           7.6817e+00, -3.4116e+00],\n",
       "         [ 9.0319e-01, -1.2842e+00,  2.0419e+00,  ..., -8.5342e-01,\n",
       "           2.5592e+00, -2.2958e+00],\n",
       "         [-2.5013e+00,  2.7754e+00, -1.6849e+00,  ...,  6.7074e-01,\n",
       "          -1.4151e-01,  1.3144e+00]],\n",
       "\n",
       "        [[ 1.4114e+00,  3.3385e+00,  4.2683e+00,  ...,  2.1449e+00,\n",
       "           3.8847e+00, -1.1476e+00],\n",
       "         [-1.5288e+00,  1.0632e+00, -9.6082e-01,  ...,  2.6351e+00,\n",
       "           3.1074e+00, -1.1413e+00],\n",
       "         [-2.1810e+00,  8.5573e-01,  1.7971e-02,  ...,  6.9051e-01,\n",
       "           6.4426e+00, -1.6835e+00],\n",
       "         ...,\n",
       "         [ 1.8762e+00, -6.5344e-01,  3.1647e+00,  ..., -4.3536e+00,\n",
       "           4.5714e+00, -4.1849e-01],\n",
       "         [-1.0603e+00, -1.8096e+00,  1.5866e+00,  ..., -2.6132e+00,\n",
       "           4.7683e+00,  4.3564e-02],\n",
       "         [-1.7111e-01, -2.0459e+00,  3.4155e+00,  ...,  2.7668e+00,\n",
       "           4.8235e+00, -1.2973e+00]],\n",
       "\n",
       "        [[ 3.4122e+00, -1.1614e+00,  3.8092e+00,  ..., -3.8083e+00,\n",
       "           1.7709e+00,  5.3119e-01],\n",
       "         [-1.4934e+00, -2.5727e+00,  3.8370e+00,  ..., -2.0098e+00,\n",
       "           1.0768e+00,  2.2598e+00],\n",
       "         [ 4.7215e+00, -7.0309e-01, -9.9717e-01,  ..., -5.0662e+00,\n",
       "           4.2894e+00, -6.5770e+00],\n",
       "         ...,\n",
       "         [-1.0831e+00, -5.4539e-01,  5.8338e-02,  ..., -1.5692e+00,\n",
       "           4.1212e+00,  1.3386e+00],\n",
       "         [-5.2202e-02, -1.2376e+00,  1.3942e+00,  ..., -4.5269e+00,\n",
       "           5.6647e+00,  2.5381e-01],\n",
       "         [-2.5934e+00,  8.4764e-01, -1.3223e+00,  ..., -3.1670e+00,\n",
       "           7.3507e+00,  4.9371e+00]],\n",
       "\n",
       "        [[-1.5418e+00,  7.6856e-01,  2.0481e+00,  ...,  2.2037e+00,\n",
       "           6.1853e-01, -3.6643e+00],\n",
       "         [ 2.0960e+00,  3.5601e+00,  5.4485e+00,  ...,  3.1334e+00,\n",
       "          -2.2106e+00,  1.9413e+00],\n",
       "         [ 1.2951e+00, -2.0999e+00,  6.8599e+00,  ...,  1.5014e+00,\n",
       "           4.5703e-01, -3.6088e+00],\n",
       "         ...,\n",
       "         [ 3.2362e+00,  1.5134e+00,  3.8647e+00,  ...,  1.0864e-02,\n",
       "           6.4564e+00, -2.5391e+00],\n",
       "         [ 3.4820e+00, -1.2863e+00,  1.7890e+00,  ..., -1.0813e+00,\n",
       "           1.8997e+00,  1.0549e+00],\n",
       "         [-3.0036e-01, -3.2584e-01,  5.4908e+00,  ..., -2.8331e+00,\n",
       "           1.9572e+00, -5.7902e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(TensorLoggingLevels.enc_dec_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder block input'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Attention output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Feedforward output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n",
      "[{'EncoderBlock'}, {'Encoder size output'}, {torch.Size([5, 30, 512])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.2814,  3.7747,  3.0296,  ..., -4.6321,  3.3187,  0.8122],\n",
       "         [-1.3944, -1.7222,  1.2845,  ..., -1.1253, -1.0714,  3.9675],\n",
       "         [-4.3500,  5.4735,  3.5669,  ...,  0.8623,  1.9125, -1.6193],\n",
       "         ...,\n",
       "         [-5.7946,  5.7984, -3.0367,  ..., -4.0312, -0.2769, -2.2626],\n",
       "         [-3.0773,  2.6791,  8.0121,  ..., -1.0238, -1.2385,  3.3627],\n",
       "         [-5.2695,  2.3082,  0.7036,  ..., -7.9986, -0.1983,  3.4443]],\n",
       "\n",
       "        [[-3.9637,  4.1581,  6.5100,  ..., -5.7876,  0.3916,  6.7167],\n",
       "         [ 0.8292,  4.9841,  7.2288,  ..., -4.0899,  7.0819,  5.8788],\n",
       "         [ 0.4265,  2.3783,  6.0820,  ..., -4.7353,  4.3201,  2.1114],\n",
       "         ...,\n",
       "         [ 2.0476,  4.7992,  2.8144,  ..., -0.2555,  1.9226,  0.1272],\n",
       "         [-3.6189,  2.0032,  3.4841,  ..., -4.3685,  2.4501,  6.3162],\n",
       "         [-2.0968,  3.8993,  6.6248,  ..., -5.6812,  3.8303,  3.1802]],\n",
       "\n",
       "        [[-0.8310,  9.9016,  6.0963,  ...,  1.9776,  5.9374,  2.3406],\n",
       "         [-5.4111,  4.5498,  6.7915,  ..., -0.3748,  5.3866,  3.3883],\n",
       "         [-5.4286,  5.5573,  7.0706,  ..., -5.1792,  2.6524,  4.6143],\n",
       "         ...,\n",
       "         [-5.5692,  5.8766,  7.7736,  ..., -2.6454,  2.1679,  0.1493],\n",
       "         [-8.1527, -0.4287,  9.9226,  ...,  2.4383,  4.1892,  4.1427],\n",
       "         [-2.8386,  5.4580,  7.6478,  ...,  0.7763, -1.2491,  6.4933]],\n",
       "\n",
       "        [[-8.9911,  2.8750,  3.6100,  ..., -5.4332,  5.0037,  8.9365],\n",
       "         [-4.9821,  4.1243,  4.0547,  ..., -4.6887,  6.7867,  6.8804],\n",
       "         [-2.1806, -2.5662,  0.0637,  ..., -5.0375,  3.1668,  5.1377],\n",
       "         ...,\n",
       "         [ 2.5675,  3.5609,  1.9387,  ..., -5.4650,  6.7706, -0.0989],\n",
       "         [-2.5474,  0.6664,  2.2939,  ..., -1.1118,  6.8813,  5.3664],\n",
       "         [-6.1792,  0.0706,  4.0747,  ..., -5.1122,  8.9748,  5.5923]],\n",
       "\n",
       "        [[-3.2368,  0.3042,  2.4811,  ..., -8.3740, -4.4856,  5.9927],\n",
       "         [-9.9256,  4.2046,  3.2180,  ..., -6.0612,  4.2407,  2.8098],\n",
       "         [-1.4266,  4.6520,  0.7162,  ..., -7.9721,  6.9341,  4.3785],\n",
       "         ...,\n",
       "         [-4.6369,  3.2013,  5.2663,  ..., -1.0331,  5.9942,  4.2552],\n",
       "         [-5.3672, -2.1569,  3.9178,  ..., -7.7008,  4.4120,  1.7128],\n",
       "         [-6.5804, -1.6997,  4.9337,  ..., -8.0939,  3.5275,  6.4086]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
